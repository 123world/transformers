# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/vit.rst:14
msgid "Vision Transformer (ViT)"
msgstr ""

#: ../../source/model_doc/vit.rst:18
msgid ""
"This is a recently introduced model so the API hasn't been tested "
"extensively. There may be some bugs or slight breaking changes to fix it "
"in the future. If you see something strange, file a `Github Issue "
"<https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template"
"=bug-report.md&title>`__."
msgstr ""

#: ../../source/model_doc/vit.rst:24
msgid "Overview"
msgstr ""

#: ../../source/model_doc/vit.rst:26
msgid ""
"The Vision Transformer (ViT) model was proposed in `An Image is Worth "
"16x16 Words: Transformers for Image Recognition at Scale "
"<https://arxiv.org/abs/2010.11929>`__ by Alexey Dosovitskiy, Lucas Beyer,"
" Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas "
"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain "
"Gelly, Jakob Uszkoreit, Neil Houlsby. It's the first paper that "
"successfully trains a Transformer encoder on ImageNet, attaining very "
"good results compared to familiar convolutional architectures."
msgstr ""

#: ../../source/model_doc/vit.rst:33
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/vit.rst:35
msgid ""
"*While the Transformer architecture has become the de-facto standard for "
"natural language processing tasks, its applications to computer vision "
"remain limited. In vision, attention is either applied in conjunction "
"with convolutional networks, or used to replace certain components of "
"convolutional networks while keeping their overall structure in place. We"
" show that this reliance on CNNs is not necessary and a pure transformer "
"applied directly to sequences of image patches can perform very well on "
"image classification tasks. When pre-trained on large amounts of data and"
" transferred to multiple mid-sized or small image recognition benchmarks "
"(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains "
"excellent results compared to state-of-the-art convolutional networks "
"while requiring substantially fewer computational resources to train.*"
msgstr ""

#: ../../source/model_doc/vit.rst:44
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/vit.rst:46
msgid ""
"To feed images to the Transformer encoder, each image is split into a "
"sequence of fixed-size non-overlapping patches, which are then linearly "
"embedded. A [CLS] token is added to serve as representation of an entire "
"image, which can be used for classification. The authors also add "
"absolute position embeddings, and feed the resulting sequence of vectors "
"to a standard Transformer encoder."
msgstr ""

#: ../../source/model_doc/vit.rst:50
msgid ""
"As the Vision Transformer expects each image to be of the same size "
"(resolution), one can use :class:`~transformers.ViTFeatureExtractor` to "
"resize (or rescale) and normalize images for the model."
msgstr ""

#: ../../source/model_doc/vit.rst:52
msgid ""
"Both the patch resolution and image resolution used during pre-training "
"or fine-tuning are reflected in the name of each checkpoint. For example,"
" :obj:`google/vit-base-patch16-224` refers to a base-sized architecture "
"with patch resolution of 16x16 and fine-tuning resolution of 224x224. All"
" checkpoints can be found on the `hub "
"<https://huggingface.co/models?search=vit>`__."
msgstr ""

#: ../../source/model_doc/vit.rst:56
msgid ""
"The available checkpoints are either (1) pre-trained on `ImageNet-21k "
"<http://www.image-net.org/>`__ (a collection of 14 million images and 21k"
" classes) only, or (2) also fine-tuned on `ImageNet <http://www.image-"
"net.org/challenges/LSVRC/2012/>`__ (also referred to as ILSVRC 2012, a "
"collection of 1.3 million images and 1,000 classes)."
msgstr ""

#: ../../source/model_doc/vit.rst:60
msgid ""
"The Vision Transformer was pre-trained using a resolution of 224x224. "
"During fine-tuning, it is often beneficial to use a higher resolution "
"than pre-training `(Touvron et al., 2019) "
"<https://arxiv.org/abs/1906.06423>`__, `(Kolesnikov et al., 2020) "
"<https://arxiv.org/abs/1912.11370>`__. In order to fine-tune at higher "
"resolution, the authors perform 2D interpolation of the pre-trained "
"position embeddings, according to their location in the original image."
msgstr ""

#: ../../source/model_doc/vit.rst:64
msgid ""
"The best results are obtained with supervised pre-training, which is not "
"the case in NLP. The authors also performed an experiment with a self-"
"supervised pre-training objective, namely masked patched prediction "
"(inspired by masked language modeling). With this approach, the smaller "
"ViT-B/16 model achieves 79.9% accuracy on ImageNet, a significant "
"improvement of 2% to training from scratch, but still 4% behind "
"supervised pre-training."
msgstr ""

#: ../../source/model_doc/vit.rst:70
msgid ""
"This model was contributed by `nielsr <https://huggingface.co/nielsr>`__."
" The original code (written in JAX) can be found `here "
"<https://github.com/google-research/vision_transformer>`__."
msgstr ""

#: ../../source/model_doc/vit.rst:73
msgid ""
"Note that we converted the weights from Ross Wightman's `timm library "
"<https://github.com/rwightman/pytorch-image-models>`__, who already "
"converted the weights from JAX to PyTorch. Credits go to him!"
msgstr ""

#: ../../source/model_doc/vit.rst:79
msgid "ViTConfig"
msgstr ""

#: of transformers.ViTConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.ViTModel`. It is used to instantiate an ViT model "
"according to the specified arguments, defining the model architecture. "
"Instantiating a configuration with the defaults will yield a similar "
"configuration to that of the ViT `google/vit-base-patch16-224 "
"<https://huggingface.co/google/vit-base-patch16-224>`__ architecture."
msgstr ""

#: of transformers.ViTConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.FlaxViTForImageClassification transformers.FlaxViTModel
#: transformers.ViTConfig transformers.ViTFeatureExtractor
#: transformers.ViTFeatureExtractor.__call__
#: transformers.ViTForImageClassification
#: transformers.ViTForImageClassification.forward transformers.ViTModel
#: transformers.ViTModel.forward
msgid "Parameters"
msgstr ""

#: of transformers.ViTConfig:10
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.ViTConfig:12
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.ViTConfig:14
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.ViTConfig:16
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.ViTConfig:18
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.ViTConfig:21
msgid ""
"The dropout probabilitiy for all fully connected layers in the "
"embeddings, encoder, and pooler."
msgstr ""

#: of transformers.ViTConfig:23
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.ViTConfig:25
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.ViTConfig:27
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.ViTConfig:29
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.ViTConfig:31
msgid "The size (resolution) of each image."
msgstr ""

#: of transformers.ViTConfig:33
msgid "The size (resolution) of each patch."
msgstr ""

#: of transformers.ViTConfig:35
msgid "The number of input channels."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:25
#: transformers.ViTConfig:38
msgid "Example::"
msgstr ""

#: ../../source/model_doc/vit.rst:86
msgid "ViTFeatureExtractor"
msgstr ""

#: of transformers.ViTFeatureExtractor:1
msgid "Constructs a ViT feature extractor."
msgstr ""

#: of transformers.ViTFeatureExtractor:3
msgid ""
"This feature extractor inherits from "
":class:`~transformers.FeatureExtractionMixin` which contains most of the "
"main methods. Users should refer to this superclass for more information "
"regarding those methods."
msgstr ""

#: of transformers.ViTFeatureExtractor:6
msgid "Whether to resize the input to a certain :obj:`size`."
msgstr ""

#: of transformers.ViTFeatureExtractor:8
msgid ""
"Resize the input to the given size. If a tuple is provided, it should be "
"(width, height). If only an integer is provided, then the input will be "
"resized to (size, size). Only has an effect if :obj:`do_resize` is set to"
" :obj:`True`."
msgstr ""

#: of transformers.ViTFeatureExtractor:12
msgid ""
"An optional resampling filter. This can be one of "
":obj:`PIL.Image.NEAREST`, :obj:`PIL.Image.BOX`, "
":obj:`PIL.Image.BILINEAR`, :obj:`PIL.Image.HAMMING`, "
":obj:`PIL.Image.BICUBIC` or :obj:`PIL.Image.LANCZOS`. Only has an effect "
"if :obj:`do_resize` is set to :obj:`True`."
msgstr ""

#: of transformers.ViTFeatureExtractor:16
msgid "Whether or not to normalize the input with mean and standard deviation."
msgstr ""

#: of transformers.ViTFeatureExtractor:18
msgid ""
"The sequence of means for each channel, to be used when normalizing "
"images."
msgstr ""

#: of transformers.ViTFeatureExtractor:20
msgid ""
"The sequence of standard deviations for each channel, to be used when "
"normalizing images."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:1
msgid "Main method to prepare for the model one or several image(s)."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:5
msgid ""
"NumPy arrays and PyTorch tensors are converted to PIL images when "
"resizing, so the most efficient is to pass PIL images."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:8
msgid ""
"The image or batch of images to be prepared. Each image can be a PIL "
"image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch "
"tensor, each image should be of shape (C, H, W), where C is a number of "
"channels, H and W are image height and width."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:12
msgid ""
"If set, will return tensors of a particular framework. Acceptable values "
"are:  * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects. * "
":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects. * :obj:`'np'`: "
"Return NumPy :obj:`np.ndarray` objects. * :obj:`'jax'`: Return JAX "
":obj:`jnp.ndarray` objects."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:12
msgid ""
"If set, will return tensors of a particular framework. Acceptable values "
"are:"
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:14
msgid ":obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:15
msgid ":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:16
msgid ":obj:`'np'`: Return NumPy :obj:`np.ndarray` objects."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:17
msgid ":obj:`'jax'`: Return JAX :obj:`jnp.ndarray` objects."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__
#: transformers.FlaxViTModel.__call__ transformers.ViTFeatureExtractor.__call__
#: transformers.ViTForImageClassification.forward transformers.ViTModel.forward
msgid "Returns"
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:20
msgid ""
"A :class:`~transformers.BatchFeature` with the following fields:  - "
"**pixel_values** -- Pixel values to be fed to a model, of shape "
"(batch_size, num_channels, height,   width)."
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:20
msgid "A :class:`~transformers.BatchFeature` with the following fields:"
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:22
msgid ""
"**pixel_values** -- Pixel values to be fed to a model, of shape "
"(batch_size, num_channels, height, width)."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__
#: transformers.FlaxViTModel.__call__ transformers.ViTFeatureExtractor.__call__
#: transformers.ViTForImageClassification.forward transformers.ViTModel.forward
msgid "Return type"
msgstr ""

#: of transformers.ViTFeatureExtractor.__call__:24
msgid ":class:`~transformers.BatchFeature`"
msgstr ""

#: ../../source/model_doc/vit.rst:93
msgid "ViTModel"
msgstr ""

#: of transformers.ViTModel:1
msgid ""
"The bare ViT Model transformer outputting raw hidden-states without any "
"specific head on top. This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use"
" it as a regular PyTorch Module and refer to the PyTorch documentation "
"for all matter related to general usage and behavior."
msgstr ""

#: of transformers.ViTForImageClassification:8 transformers.ViTModel:6
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.ViTModel.forward:1
msgid ""
"The :class:`~transformers.ViTModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:4
#: transformers.FlaxViTModel.__call__:4
#: transformers.ViTForImageClassification.forward:4
#: transformers.ViTModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.ViTForImageClassification.forward:8
#: transformers.ViTModel.forward:8
msgid ""
"Pixel values. Pixel values can be obtained using "
":class:`~transformers.ViTFeatureExtractor`. See "
":meth:`transformers.ViTFeatureExtractor.__call__` for details."
msgstr ""

#: of transformers.ViTForImageClassification.forward:11
#: transformers.ViTModel.forward:11
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.ViTForImageClassification.forward:11
#: transformers.ViTModel.forward:11
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.ViTForImageClassification.forward:13
#: transformers.ViTModel.forward:13
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.ViTForImageClassification.forward:14
#: transformers.ViTModel.forward:14
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.ViTForImageClassification.forward:16
#: transformers.ViTModel.forward:16
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.ViTForImageClassification.forward:19
#: transformers.ViTModel.forward:19
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.ViTForImageClassification.forward:22
#: transformers.ViTModel.forward:22
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.ViTModel.forward:25
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.ViTConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **pooler_output** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`) -- "
"Last layer hidden-state of the first token of the sequence "
"(classification token) further processed by a   Linear layer and a Tanh "
"activation function. The Linear layer weights are trained from the next "
"sentence   prediction (classification) objective during pretraining. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"ViTFeatureExtractor, ViTModel     >>> from PIL import Image     >>> "
"import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = ViTFeatureExtractor.from_pretrained('google/vit-base-"
"patch16-224-in21k')     >>> model = ViTModel.from_pretrained('google/vit-"
"base-patch16-224-in21k')      >>> inputs = "
"feature_extractor(images=image, return_tensors=\"pt\")     >>> outputs = "
"model(**inputs)     >>> last_hidden_states = outputs.last_hidden_state"
msgstr ""

#: of transformers.ViTModel.forward:25
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.ViTConfig`) and inputs."
msgstr ""

#: of transformers.ViTModel.forward:29
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.ViTModel.forward:30
msgid ""
"**pooler_output** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"hidden_size)`) -- Last layer hidden-state of the first token of the "
"sequence (classification token) further processed by a Linear layer and a"
" Tanh activation function. The Linear layer weights are trained from the "
"next sentence prediction (classification) objective during pretraining."
msgstr ""

#: of transformers.ViTForImageClassification.forward:35
#: transformers.ViTModel.forward:33
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:17
#: transformers.FlaxViTModel.__call__:20
#: transformers.ViTForImageClassification.forward:38
#: transformers.ViTModel.forward:36
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.ViTForImageClassification.forward:39
#: transformers.ViTModel.forward:37
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:21
#: transformers.FlaxViTModel.__call__:24
#: transformers.ViTForImageClassification.forward:42
#: transformers.ViTModel.forward:40
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.FlaxViTModel.__call__:28
#: transformers.ViTForImageClassification.forward:46
#: transformers.ViTModel.forward:44
msgid "Examples::"
msgstr ""

#: of transformers.ViTModel.forward:59
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/vit.rst:100
msgid "ViTForImageClassification"
msgstr ""

#: of transformers.FlaxViTForImageClassification:1
#: transformers.ViTForImageClassification:1
msgid ""
"ViT Model transformer with an image classification head on top (a linear "
"layer on top of the final hidden state of the [CLS] token) e.g. for "
"ImageNet."
msgstr ""

#: of transformers.ViTForImageClassification:4
msgid ""
"This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use"
" it as a regular PyTorch Module and refer to the PyTorch documentation "
"for all matter related to general usage and behavior."
msgstr ""

#: of transformers.ViTForImageClassification.forward:1
msgid ""
"The :class:`~transformers.ViTForImageClassification` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.ViTForImageClassification.forward:24
msgid ""
"Labels for computing the image classification/regression loss. Indices "
"should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy)."
msgstr ""

#: of transformers.ViTForImageClassification.forward:29
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.ViTConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss. - **logits** (:obj:`torch.FloatTensor` of"
" shape :obj:`(batch_size, config.num_labels)`) -- Classification (or "
"regression if config.num_labels==1) scores (before SoftMax). - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"ViTFeatureExtractor, ViTForImageClassification     >>> from PIL import "
"Image     >>> import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')"
"     >>> model = ViTForImageClassification.from_pretrained('google/vit-"
"base-patch16-224')      >>> inputs = feature_extractor(images=image, "
"return_tensors=\"pt\")     >>> outputs = model(**inputs)     >>> logits ="
" outputs.logits     >>> # model predicts one of the 1000 ImageNet classes"
"     >>> predicted_class_idx = logits.argmax(-1).item()     >>> "
"print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
msgstr ""

#: of transformers.ViTForImageClassification.forward:29
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.ViTConfig`) and inputs."
msgstr ""

#: of transformers.ViTForImageClassification.forward:33
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.ViTForImageClassification.forward:34
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.ViTForImageClassification.forward:64
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/vit.rst:107
msgid "FlaxVitModel"
msgstr ""

#: of transformers.FlaxViTModel:1
msgid ""
"The bare ViT Model transformer outputting raw hidden-states without any "
"specific head on top."
msgstr ""

#: of transformers.FlaxViTForImageClassification:5 transformers.FlaxViTModel:3
msgid ""
"This model inherits from :class:`~transformers.FlaxPreTrainedModel`. "
"Check the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading, saving and converting "
"weights from PyTorch models)"
msgstr ""

#: of transformers.FlaxViTForImageClassification:9 transformers.FlaxViTModel:7
msgid ""
"This model is also a Flax Linen `flax.linen.Module "
"<https://flax.readthedocs.io/en/latest/flax.linen.html#module>`__ "
"subclass. Use it as a regular Flax linen Module and refer to the Flax "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.FlaxViTForImageClassification:13
#: transformers.FlaxViTModel:11
msgid "Finally, this model supports inherent JAX features such as:"
msgstr ""

#: of transformers.FlaxViTForImageClassification:15
#: transformers.FlaxViTModel:13
msgid ""
"`Just-In-Time (JIT) compilation "
"<https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-"
"jit>`__"
msgstr ""

#: of transformers.FlaxViTForImageClassification:16
#: transformers.FlaxViTModel:14
msgid ""
"`Automatic Differentiation <https://jax.readthedocs.io/en/latest/jax.html"
"#automatic-differentiation>`__"
msgstr ""

#: of transformers.FlaxViTForImageClassification:17
#: transformers.FlaxViTModel:15
msgid ""
"`Vectorization <https://jax.readthedocs.io/en/latest/jax.html"
"#vectorization-vmap>`__"
msgstr ""

#: of transformers.FlaxViTForImageClassification:18
#: transformers.FlaxViTModel:16
msgid ""
"`Parallelization <https://jax.readthedocs.io/en/latest/jax.html"
"#parallelization-pmap>`__"
msgstr ""

#: of transformers.FlaxViTForImageClassification:20
#: transformers.FlaxViTModel:18
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.FlaxPreTrainedModel.from_pretrained` method to load "
"the model weights."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:1
#: transformers.FlaxViTModel.__call__:1
msgid ""
"The :class:`~transformers.FlaxViTPreTrainedModel` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.FlaxViTModel.__call__:9
msgid ""
"A "
":class:`~transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.vit.configuration_vit.ViTConfig'>`) and inputs.  - "
"**last_hidden_state** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model. - **pooler_output** "
"(:obj:`jnp.ndarray` of shape :obj:`(batch_size, hidden_size)`) -- Last "
"layer hidden-state of the first token of the sequence (classification "
"token) further processed by a   Linear layer and a Tanh activation "
"function. The Linear layer weights are trained from the next sentence   "
"prediction (classification) objective during pretraining. - "
"**hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxViTModel.__call__:9
msgid ""
"A "
":class:`~transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.vit.configuration_vit.ViTConfig'>`) and inputs."
msgstr ""

#: of transformers.FlaxViTModel.__call__:13
msgid ""
"**last_hidden_state** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model."
msgstr ""

#: of transformers.FlaxViTModel.__call__:14
msgid ""
"**pooler_output** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"hidden_size)`) -- Last layer hidden-state of the first token of the "
"sequence (classification token) further processed by a Linear layer and a"
" Tanh activation function. The Linear layer weights are trained from the "
"next sentence prediction (classification) objective during pretraining."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:14
#: transformers.FlaxViTModel.__call__:17
msgid ""
"**hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:18
#: transformers.FlaxViTModel.__call__:21
msgid ""
"**attentions** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.FlaxViTModel.__call__:26
msgid ""
":class:`~transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/vit.rst:114
msgid "FlaxViTForImageClassification"
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:9
msgid ""
"A "
":class:`~transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.vit.configuration_vit.ViTConfig'>`) and inputs.  - "
"**logits** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **hidden_states** "
"(:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:9
msgid ""
"A "
":class:`~transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.vit.configuration_vit.ViTConfig'>`) and inputs."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:13
msgid ""
"**logits** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.FlaxViTForImageClassification.__call__:23
msgid ""
":class:`~transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/detr.rst:14
msgid "DETR"
msgstr ""

#: ../../source/model_doc/detr.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/detr.rst:19
msgid ""
"The DETR model was proposed in `End-to-End Object Detection with "
"Transformers <https://arxiv.org/abs/2005.12872>`__ by Nicolas Carion, "
"Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov "
"and Sergey Zagoruyko. DETR consists of a convolutional backbone followed "
"by an encoder-decoder Transformer which can be trained end-to-end for "
"object detection. It greatly simplifies a lot of the complexity of models"
" like Faster-R-CNN and Mask-R-CNN, which use things like region "
"proposals, non-maximum suppression procedure and anchor generation. "
"Moreover, DETR can also be naturally extended to perform panoptic "
"segmentation, by simply adding a mask head on top of the decoder outputs."
msgstr ""

#: ../../source/model_doc/detr.rst:26
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/detr.rst:28
msgid ""
"*We present a new method that views object detection as a direct set "
"prediction problem. Our approach streamlines the detection pipeline, "
"effectively removing the need for many hand-designed components like a "
"non-maximum suppression procedure or anchor generation that explicitly "
"encode our prior knowledge about the task. The main ingredients of the "
"new framework, called DEtection TRansformer or DETR, are a set-based "
"global loss that forces unique predictions via bipartite matching, and a "
"transformer encoder-decoder architecture. Given a fixed small set of "
"learned object queries, DETR reasons about the relations of the objects "
"and the global image context to directly output the final set of "
"predictions in parallel. The new model is conceptually simple and does "
"not require a specialized library, unlike many other modern detectors. "
"DETR demonstrates accuracy and run-time performance on par with the well-"
"established and highly-optimized Faster RCNN baseline on the challenging "
"COCO object detection dataset. Moreover, DETR can be easily generalized "
"to produce panoptic segmentation in a unified manner. We show that it "
"significantly outperforms competitive baselines.*"
msgstr ""

#: ../../source/model_doc/detr.rst:40
msgid ""
"This model was contributed by `nielsr <https://huggingface.co/nielsr>`__."
" The original code can be found `here "
"<https://github.com/facebookresearch/detr>`__."
msgstr ""

#: ../../source/model_doc/detr.rst:43
msgid ""
"The quickest way to get started with DETR is by checking the `example "
"notebooks <https://github.com/NielsRogge/Transformers-"
"Tutorials/tree/master/DETR>`__ (which showcase both inference and fine-"
"tuning on custom data)."
msgstr ""

#: ../../source/model_doc/detr.rst:47
msgid ""
"Here's a TLDR explaining how "
":class:`~transformers.DetrForObjectDetection` works:"
msgstr ""

#: ../../source/model_doc/detr.rst:49
msgid ""
"First, an image is sent through a pre-trained convolutional backbone (in "
"the paper, the authors use ResNet-50/ResNet-101). Let's assume we also "
"add a batch dimension. This means that the input to the backbone is a "
"tensor of shape :obj:`(batch_size, 3, height, width)`, assuming the image"
" has 3 color channels (RGB). The CNN backbone outputs a new lower-"
"resolution feature map, typically of shape :obj:`(batch_size, 2048, "
"height/32, width/32)`. This is then projected to match the hidden "
"dimension of the Transformer of DETR, which is :obj:`256` by default, "
"using a :obj:`nn.Conv2D` layer. So now, we have a tensor of shape "
":obj:`(batch_size, 256, height/32, width/32).` Next, the feature map is "
"flattened and transposed to obtain a tensor of shape :obj:`(batch_size, "
"seq_len, d_model)` = :obj:`(batch_size, width/32*height/32, 256)`. So a "
"difference with NLP models is that the sequence length is actually longer"
" than usual, but with a smaller :obj:`d_model` (which in NLP is typically"
" 768 or higher)."
msgstr ""

#: ../../source/model_doc/detr.rst:59
msgid ""
"Next, this is sent through the encoder, outputting "
":obj:`encoder_hidden_states` of the same shape (you can consider these as"
" image features). Next, so-called **object queries** are sent through the"
" decoder. This is a tensor of shape :obj:`(batch_size, num_queries, "
"d_model)`, with :obj:`num_queries` typically set to 100 and initialized "
"with zeros. These input embeddings are learnt positional encodings that "
"the authors refer to as object queries, and similarly to the encoder, "
"they are added to the input of each attention layer. Each object query "
"will look for a particular object in the image. The decoder updates these"
" embeddings through multiple self-attention and encoder-decoder attention"
" layers to output :obj:`decoder_hidden_states` of the same shape: "
":obj:`(batch_size, num_queries, d_model)`. Next, two heads are added on "
"top for object detection: a linear layer for classifying each object "
"query into one of the objects or \"no object\", and a MLP to predict "
"bounding boxes for each query."
msgstr ""

#: ../../source/model_doc/detr.rst:69
msgid ""
"The model is trained using a **bipartite matching loss**: so what we "
"actually do is compare the predicted classes + bounding boxes of each of "
"the N = 100 object queries to the ground truth annotations, padded up to "
"the same length N (so if an image only contains 4 objects, 96 annotations"
" will just have a \"no object\" as class and \"no bounding box\" as "
"bounding box). The `Hungarian matching algorithm "
"<https://en.wikipedia.org/wiki/Hungarian_algorithm>`__ is used to find an"
" optimal one-to-one mapping of each of the N queries to each of the N "
"annotations. Next, standard cross-entropy (for the classes) and a linear "
"combination of the L1 and `generalized IoU loss "
"<https://giou.stanford.edu/>`__ (for the bounding boxes) are used to "
"optimize the parameters of the model."
msgstr ""

#: ../../source/model_doc/detr.rst:77
msgid ""
"DETR can be naturally extended to perform panoptic segmentation (which "
"unifies semantic segmentation and instance segmentation). "
":class:`~transformers.DetrForSegmentation` adds a segmentation mask head "
"on top of :class:`~transformers.DetrForObjectDetection`. The mask head "
"can be trained either jointly, or in a two steps process, where one first"
" trains a :class:`~transformers.DetrForObjectDetection` model to detect "
"bounding boxes around both \"things\" (instances) and \"stuff\" "
"(background things like trees, roads, sky), then freeze all the weights "
"and train only the mask head for 25 epochs. Experimentally, these two "
"approaches give similar results. Note that predicting boxes is required "
"for the training to be possible, since the Hungarian matching is computed"
" using distances between boxes."
msgstr ""

#: ../../source/model_doc/detr.rst:85
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/detr.rst:87
msgid ""
"DETR uses so-called **object queries** to detect objects in an image. The"
" number of queries determines the maximum number of objects that can be "
"detected in a single image, and is set to 100 by default (see parameter "
":obj:`num_queries` of :class:`~transformers.DetrConfig`). Note that it's "
"good to have some slack (in COCO, the authors used 100, while the maximum"
" number of objects in a COCO image is ~70)."
msgstr ""

#: ../../source/model_doc/detr.rst:91
msgid ""
"The decoder of DETR updates the query embeddings in parallel. This is "
"different from language models like GPT-2, which use autoregressive "
"decoding instead of parallel. Hence, no causal attention mask is used."
msgstr ""

#: ../../source/model_doc/detr.rst:93
msgid ""
"DETR adds position embeddings to the hidden states at each self-attention"
" and cross-attention layer before projecting to queries and keys. For the"
" position embeddings of the image, one can choose between fixed "
"sinusoidal or learned absolute position embeddings. By default, the "
"parameter :obj:`position_embedding_type` of "
":class:`~transformers.DetrConfig` is set to :obj:`\"sine\"`."
msgstr ""

#: ../../source/model_doc/detr.rst:97
msgid ""
"During training, the authors of DETR did find it helpful to use auxiliary"
" losses in the decoder, especially to help the model output the correct "
"number of objects of each class. If you set the parameter "
":obj:`auxiliary_loss` of :class:`~transformers.DetrConfig` to "
":obj:`True`, then prediction feedforward neural networks and Hungarian "
"losses are added after each decoder layer (with the FFNs sharing "
"parameters)."
msgstr ""

#: ../../source/model_doc/detr.rst:101
msgid ""
"If you want to train the model in a distributed environment across "
"multiple nodes, then one should update the `num_boxes` variable in the "
"`DetrLoss` class of `modeling_detr.py`. When training on multiple nodes, "
"this should be set to the average number of target boxes across all "
"nodes, as can be seen in the original implementation `here "
"<https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232>`__."
msgstr ""

#: ../../source/model_doc/detr.rst:105
msgid ""
":class:`~transformers.DetrForObjectDetection` and "
":class:`~transformers.DetrForSegmentation` can be initialized with any "
"convolutional backbone available in the `timm library "
"<https://github.com/rwightman/pytorch-image-models>`__. Initializing with"
" a MobileNet backbone for example can be done by setting the "
":obj:`backbone` attribute of :class:`~transformers.DetrConfig` to "
":obj:`\"tf_mobilenetv3_small_075\"`, and then initializing the model with"
" that config."
msgstr ""

#: ../../source/model_doc/detr.rst:110
msgid ""
"DETR resizes the input images such that the shortest side is at least a "
"certain amount of pixels while the longest is at most 1333 pixels. At "
"training time, scale augmentation is used such that the shortest side is "
"randomly set to at least 480 and at most 800 pixels. At inference time, "
"the shortest side is set to 800. One can use "
":class:`~transformers.DetrFeatureExtractor` to prepare images (and "
"optional annotations in COCO format) for the model. Due to this resizing,"
" images in a batch can have different sizes. DETR solves this by padding "
"images up to the largest size in a batch, and by creating a pixel mask "
"that indicates which pixels are real/which are padding. Alternatively, "
"one can also define a custom :obj:`collate_fn` in order to batch images "
"together, using "
":meth:`~transformers.DetrFeatureExtractor.pad_and_create_pixel_mask`."
msgstr ""

#: ../../source/model_doc/detr.rst:118
msgid ""
"The size of the images will determine the amount of memory being used, "
"and will thus determine the :obj:`batch_size`. It is advised to use a "
"batch size of 2 per GPU. See `this Github thread "
"<https://github.com/facebookresearch/detr/issues/150>`__ for more info."
msgstr ""

#: ../../source/model_doc/detr.rst:122
msgid "As a summary, consider the following table:"
msgstr ""

#: ../../source/model_doc/detr.rst:125
msgid "**Task**"
msgstr ""

#: ../../source/model_doc/detr.rst:125
msgid "**Object detection**"
msgstr ""

#: ../../source/model_doc/detr.rst:125
msgid "**Instance segmentation**"
msgstr ""

#: ../../source/model_doc/detr.rst:125
msgid "**Panoptic segmentation**"
msgstr ""

#: ../../source/model_doc/detr.rst:127
msgid "**Description**"
msgstr ""

#: ../../source/model_doc/detr.rst:127
msgid "Predicting bounding boxes and class labels around objects in an image"
msgstr ""

#: ../../source/model_doc/detr.rst:127
msgid "Predicting masks around objects (i.e. instances) in an image"
msgstr ""

#: ../../source/model_doc/detr.rst:127
msgid ""
"Predicting masks around both objects (i.e. instances) as well as "
"\"stuff\" (i.e. background things like trees and roads) in an image"
msgstr ""

#: ../../source/model_doc/detr.rst:130
msgid "**Model**"
msgstr ""

#: ../../source/model_doc/detr.rst:130
msgid ":class:`~transformers.DetrForObjectDetection`"
msgstr ""

#: ../../source/model_doc/detr.rst:130
msgid ":class:`~transformers.DetrForSegmentation`"
msgstr ""

#: ../../source/model_doc/detr.rst:132
msgid "**Example dataset**"
msgstr ""

#: ../../source/model_doc/detr.rst:132
msgid "COCO detection"
msgstr ""

#: ../../source/model_doc/detr.rst:132
msgid "COCO detection, COCO panoptic"
msgstr ""

#: ../../source/model_doc/detr.rst:132
msgid "COCO panoptic"
msgstr ""

#: ../../source/model_doc/detr.rst:135
msgid ""
"**Format of annotations to provide to** "
":class:`~transformers.DetrFeatureExtractor`"
msgstr ""

#: ../../source/model_doc/detr.rst:135
msgid ""
"{‘image_id’: int, ‘annotations’: List[Dict]}, each Dict being a COCO "
"object annotation"
msgstr ""

#: ../../source/model_doc/detr.rst:135
msgid ""
"{‘image_id’: int, ‘annotations’: [List[Dict]] } (in case of COCO "
"detection)"
msgstr ""

#: ../../source/model_doc/detr.rst:138
msgid "or"
msgstr ""

#: ../../source/model_doc/detr.rst:140
msgid ""
"{‘file_name’: str, ‘image_id’: int, ‘segments_info’: List[Dict]} (in case"
" of COCO panoptic)"
msgstr ""

#: ../../source/model_doc/detr.rst:135
msgid "{‘file_name: str, ‘image_id: int, ‘segments_info’: List[Dict] }"
msgstr ""

#: ../../source/model_doc/detr.rst:139
msgid "and masks_path (path to directory containing PNG files of the masks)"
msgstr ""

#: ../../source/model_doc/detr.rst:144
msgid "**Postprocessing** (i.e. converting the output of the model to COCO API)"
msgstr ""

#: ../../source/model_doc/detr.rst:144
msgid ":meth:`~transformers.DetrFeatureExtractor.post_process`"
msgstr ""

#: ../../source/model_doc/detr.rst:144
msgid ":meth:`~transformers.DetrFeatureExtractor.post_process_segmentation`"
msgstr ""

#: ../../source/model_doc/detr.rst:144
msgid ""
":meth:`~transformers.DetrFeatureExtractor.post_process_segmentation`, "
":meth:`~transformers.DetrFeatureExtractor.post_process_panoptic`"
msgstr ""

#: ../../source/model_doc/detr.rst:147
msgid "**evaluators**"
msgstr ""

#: ../../source/model_doc/detr.rst:147
msgid ":obj:`CocoEvaluator` with iou_types = “bbox”"
msgstr ""

#: ../../source/model_doc/detr.rst:147
msgid ":obj:`CocoEvaluator` with iou_types = “bbox”, “segm”"
msgstr ""

#: ../../source/model_doc/detr.rst:147
msgid ":obj:`CocoEvaluator` with iou_tupes = “bbox, “segm”"
msgstr ""

#: ../../source/model_doc/detr.rst:149
msgid ":obj:`PanopticEvaluator`"
msgstr ""

#: ../../source/model_doc/detr.rst:152
msgid ""
"In short, one should prepare the data either in COCO detection or COCO "
"panoptic format, then use :class:`~transformers.DetrFeatureExtractor` to "
"create :obj:`pixel_values`, :obj:`pixel_mask` and optional :obj:`labels`,"
" which can then be used to train (or fine-tune) a model. For evaluation, "
"one should first convert the outputs of the model using one of the "
"postprocessing methods of :class:`~transformers.DetrFeatureExtractor`. "
"These can be be provided to either :obj:`CocoEvaluator` or "
":obj:`PanopticEvaluator`, which allow you to calculate metrics like mean "
"Average Precision (mAP) and Panoptic Quality (PQ). The latter objects are"
" implemented in the `original repository "
"<https://github.com/facebookresearch/detr>`__. See the `example notebooks"
" <https://github.com/NielsRogge/Transformers-"
"Tutorials/tree/master/DETR>`__ for more info regarding evaluation."
msgstr ""

#: ../../source/model_doc/detr.rst:163
msgid "DETR specific outputs"
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:1
msgid ""
"Base class for outputs of the DETR encoder-decoder model. This class adds"
" one attribute to Seq2SeqModelOutput, namely an optional stack of "
"intermediate decoder activations, i.e. the output of each decoder layer, "
"each of them gone through a layernorm. This is useful when training the "
"model with auxiliary decoding losses."
msgstr ""

#: of transformers.DetrConfig transformers.DetrFeatureExtractor
#: transformers.DetrFeatureExtractor.__call__
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask
#: transformers.DetrFeatureExtractor.post_process
#: transformers.DetrFeatureExtractor.post_process_panoptic
#: transformers.DetrFeatureExtractor.post_process_segmentation
#: transformers.DetrForObjectDetection
#: transformers.DetrForObjectDetection.forward transformers.DetrForSegmentation
#: transformers.DetrForSegmentation.forward transformers.DetrModel
#: transformers.DetrModel.forward
#: transformers.models.detr.modeling_detr.DetrModelOutput
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput
msgid "Parameters"
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:5
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:20
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:25
msgid ""
"Sequence of hidden-states at the output of the last layer of the decoder "
"of the model."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:7
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:22
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:27
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`. Hidden-states of the decoder at the "
"output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:11
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:26
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:31
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`. "
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:15
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:30
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:35
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`. "
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:19
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:34
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:39
msgid ""
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:21
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:36
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:41
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`. Hidden-states of the encoder at the "
"output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:25
#: transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:40
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:45
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`. "
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrModelOutput:29
msgid ""
"Intermediate decoder activations, i.e. the output of each decoder layer, "
"each of them gone through a layernorm."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:1
msgid "Output type of :class:`~transformers.DetrForObjectDetection`."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:3
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:3
msgid ""
"Total loss as a linear combination of a negative log-likehood (cross-"
"entropy) for class prediction and a bounding box loss. The latter is "
"defined as a linear combination of the L1 loss and the generalized scale-"
"invariant IoU loss."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:7
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:7
msgid "A dictionary containing the individual losses. Useful for logging."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:9
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:9
msgid "Classification logits (including no-object) for all queries."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:11
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:11
msgid ""
"Normalized boxes coordinates for all queries, represented as (center_x, "
"center_y, width, height). These values are normalized in [0, 1], relative"
" to the size of each individual image in the batch (disregarding possible"
" padding). You can use "
":meth:`~transformers.DetrFeatureExtractor.post_process` to retrieve the "
"unnormalized bounding boxes."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrObjectDetectionOutput:16
#: transformers.models.detr.modeling_detr.DetrSegmentationOutput:21
msgid ""
"Optional, only returned when auxilary losses are activated (i.e. "
":obj:`config.auxiliary_loss` is set to `True`) and labels are provided. "
"It is a list of dictionnaries containing the two above keys "
"(:obj:`logits` and :obj:`pred_boxes`) for each decoder layer."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrSegmentationOutput:1
msgid "Output type of :class:`~transformers.DetrForSegmentation`."
msgstr ""

#: of transformers.models.detr.modeling_detr.DetrSegmentationOutput:16
msgid ""
"Segmentation masks logits for all queries. See also "
":meth:`~transformers.DetrFeatureExtractor.post_process_segmentation` or "
":meth:`~transformers.DetrFeatureExtractor.post_process_panoptic` to "
"evaluate instance and panoptic segmentation masks respectively."
msgstr ""

#: ../../source/model_doc/detr.rst:176
msgid "DetrConfig"
msgstr ""

#: of transformers.DetrConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.DetrModel`. It is used to instantiate a DETR model "
"according to the specified arguments, defining the model architecture. "
"Instantiating a configuration with the defaults will yield a similar "
"configuration to that of the DETR `facebook/detr-resnet-50 "
"<https://huggingface.co/facebook/detr-resnet-50>`__ architecture."
msgstr ""

#: of transformers.DetrConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.DetrConfig:10
msgid ""
"Number of object queries, i.e. detection slots. This is the maximal "
"number of objects :class:`~transformers.DetrModel` can detect in a single"
" image. For COCO, we recommend 100 queries."
msgstr ""

#: of transformers.DetrConfig:13
msgid "Dimension of the layers."
msgstr ""

#: of transformers.DetrConfig:15
msgid "Number of encoder layers."
msgstr ""

#: of transformers.DetrConfig:17
msgid "Number of decoder layers."
msgstr ""

#: of transformers.DetrConfig:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.DetrConfig:21
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"decoder."
msgstr ""

#: of transformers.DetrConfig:23 transformers.DetrConfig:25
msgid ""
"Dimension of the \"intermediate\" (often named feed-forward) layer in "
"decoder."
msgstr ""

#: of transformers.DetrConfig:27
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.DetrConfig:30
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.DetrConfig:32
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.DetrConfig:34
msgid "The dropout ratio for activations inside the fully connected layer."
msgstr ""

#: of transformers.DetrConfig:36
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.DetrConfig:38
msgid ""
"The scaling factor used for the Xavier initialization gain in the HM "
"Attention map module."
msgstr ""

#: of transformers.DetrConfig:40
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the encoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.DetrConfig:43
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the decoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.DetrConfig:46
msgid ""
"Whether auxiliary decoding losses (loss at each decoder layer) are to be "
"used."
msgstr ""

#: of transformers.DetrConfig:48
msgid ""
"Type of position embeddings to be used on top of the image features. One "
"of :obj:`\"sine\"` or :obj:`\"learned\"`."
msgstr ""

#: of transformers.DetrConfig:51
msgid ""
"Name of convolutional backbone to use. Supports any convolutional "
"backbone from the timm package. For a list of all available models, see "
"`this page <https://rwightman.github.io/pytorch-image-models/#load-a"
"-pretrained-model>`__."
msgstr ""

#: of transformers.DetrConfig:55
msgid ""
"Whether to replace stride with dilation in the last convolutional block "
"(DC5)."
msgstr ""

#: of transformers.DetrConfig:57
msgid ""
"Relative weight of the classification error in the Hungarian matching "
"cost."
msgstr ""

#: of transformers.DetrConfig:59
msgid ""
"Relative weight of the L1 error of the bounding box coordinates in the "
"Hungarian matching cost."
msgstr ""

#: of transformers.DetrConfig:61
msgid ""
"Relative weight of the generalized IoU loss of the bounding box in the "
"Hungarian matching cost."
msgstr ""

#: of transformers.DetrConfig:63
msgid "Relative weight of the Focal loss in the panoptic segmentation loss."
msgstr ""

#: of transformers.DetrConfig:65
msgid "Relative weight of the DICE/F-1 loss in the panoptic segmentation loss."
msgstr ""

#: of transformers.DetrConfig:67
msgid "Relative weight of the L1 bounding box loss in the object detection loss."
msgstr ""

#: of transformers.DetrConfig:69
msgid "Relative weight of the generalized IoU loss in the object detection loss."
msgstr ""

#: of transformers.DetrConfig:71
msgid ""
"Relative classification weight of the 'no-object' class in the object "
"detection loss."
msgstr ""

#: of transformers.DetrConfig:74 transformers.DetrForObjectDetection.forward:83
#: transformers.DetrForSegmentation.forward:88
#: transformers.DetrModel.forward:67
msgid "Examples::"
msgstr ""

#: ../../source/model_doc/detr.rst:183
msgid "DetrFeatureExtractor"
msgstr ""

#: of transformers.DetrFeatureExtractor:1
msgid "Constructs a DETR feature extractor."
msgstr ""

#: of transformers.DetrFeatureExtractor:3
msgid ""
"This feature extractor inherits from "
":class:`~transformers.FeatureExtractionMixin` which contains most of the "
"main methods. Users should refer to this superclass for more information "
"regarding those methods."
msgstr ""

#: of transformers.DetrFeatureExtractor:7
msgid ""
"Data format of the annotations. One of \"coco_detection\" or "
"\"coco_panoptic\"."
msgstr ""

#: of transformers.DetrFeatureExtractor:9
msgid "Whether to resize the input to a certain :obj:`size`."
msgstr ""

#: of transformers.DetrFeatureExtractor:11
msgid ""
"Resize the input to the given size. Only has an effect if "
":obj:`do_resize` is set to :obj:`True`. If size is a sequence like "
":obj:`(width, height)`, output size will be matched to this. If size is "
"an int, smaller edge of the image will be matched to this number. i.e, if"
" :obj:`height > width`, then image will be rescaled to :obj:`(size * "
"height / width, size)`."
msgstr ""

#: of transformers.DetrFeatureExtractor:16
msgid ""
"The largest size an image dimension can have (otherwise it's capped). "
"Only has an effect if :obj:`do_resize` is set to :obj:`True`."
msgstr ""

#: of transformers.DetrFeatureExtractor:19
msgid "Whether or not to normalize the input with mean and standard deviation."
msgstr ""

#: of transformers.DetrFeatureExtractor:21
msgid ""
"The sequence of means for each channel, to be used when normalizing "
"images. Defaults to the ImageNet mean."
msgstr ""

#: of transformers.DetrFeatureExtractor:23
msgid ""
"The sequence of standard deviations for each channel, to be used when "
"normalizing images. Defaults to the ImageNet std."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:1
msgid ""
"Main method to prepare for the model one or several image(s) and optional"
" annotations. Images are by default padded up to the largest image in a "
"batch, and a pixel mask is created that indicates which pixels are "
"real/which are padding."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:7
msgid ""
"NumPy arrays and PyTorch tensors are converted to PIL images when "
"resizing, so the most efficient is to pass PIL images."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:10
msgid ""
"The image or batch of images to be prepared. Each image can be a PIL "
"image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch "
"tensor, each image should be of shape (C, H, W), where C is a number of "
"channels, H and W are image height and width."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:14
msgid ""
"The corresponding annotations in COCO format.  In case "
":class:`~transformers.DetrFeatureExtractor` was initialized with "
":obj:`format = \"coco_detection\"`, the annotations for each image should"
" have the following format: {'image_id': int, 'annotations': "
"[annotation]}, with the annotations being a list of COCO object "
"annotations.  In case :class:`~transformers.DetrFeatureExtractor` was "
"initialized with :obj:`format = \"coco_panoptic\"`, the annotations for "
"each image should have the following format: {'image_id': int, "
"'file_name': str, 'segments_info': [segment_info]} with segments_info "
"being a list of COCO panoptic annotations."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:14
msgid "The corresponding annotations in COCO format."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:16
msgid ""
"In case :class:`~transformers.DetrFeatureExtractor` was initialized with "
":obj:`format = \"coco_detection\"`, the annotations for each image should"
" have the following format: {'image_id': int, 'annotations': "
"[annotation]}, with the annotations being a list of COCO object "
"annotations."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:20
msgid ""
"In case :class:`~transformers.DetrFeatureExtractor` was initialized with "
":obj:`format = \"coco_panoptic\"`, the annotations for each image should "
"have the following format: {'image_id': int, 'file_name': str, "
"'segments_info': [segment_info]} with segments_info being a list of COCO "
"panoptic annotations."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:25
msgid ""
"Whether to also include instance segmentation masks as part of the labels"
" in case :obj:`format = \"coco_detection\"`."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:28
msgid ""
"Path to the directory containing the PNG files that store the class-"
"agnostic image segmentations. Only relevant in case "
":class:`~transformers.DetrFeatureExtractor` was initialized with "
":obj:`format = \"coco_panoptic\"`."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:32
msgid ""
"Whether or not to pad images up to the largest image in a batch and "
"create a pixel mask.  If left to the default, will return a pixel mask "
"that is:  - 1 for pixels that are real (i.e. **not masked**), - 0 for "
"pixels that are padding (i.e. **masked**)."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:32
msgid ""
"Whether or not to pad images up to the largest image in a batch and "
"create a pixel mask."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:34
msgid "If left to the default, will return a pixel mask that is:"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:36
#: transformers.DetrForObjectDetection.forward:15
#: transformers.DetrForSegmentation.forward:15
#: transformers.DetrModel.forward:15
msgid "1 for pixels that are real (i.e. **not masked**),"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:37
#: transformers.DetrForObjectDetection.forward:16
#: transformers.DetrForSegmentation.forward:16
#: transformers.DetrModel.forward:16
msgid "0 for pixels that are padding (i.e. **masked**)."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:39
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:5
msgid ""
"If set, will return tensors instead of NumPy arrays. If set to "
":obj:`'pt'`, return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask
#: transformers.DetrFeatureExtractor.post_process
#: transformers.DetrFeatureExtractor.post_process_panoptic
#: transformers.DetrFeatureExtractor.post_process_segmentation
#: transformers.DetrForObjectDetection.forward
#: transformers.DetrForSegmentation.forward transformers.DetrModel.forward
msgid "Returns"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:43
msgid ""
"A :class:`~transformers.BatchFeature` with the following fields:  - "
"**pixel_values** -- Pixel values to be fed to a model. - **pixel_mask** "
"-- Pixel mask to be fed to a model (when "
":obj:`pad_and_return_pixel_mask=True` or if   `\"pixel_mask\"` is in "
":obj:`self.model_input_names`). - **labels** -- Optional labels to be fed"
" to a model (when :obj:`annotations` are provided)"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:43
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:9
msgid "A :class:`~transformers.BatchFeature` with the following fields:"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:45
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:11
msgid "**pixel_values** -- Pixel values to be fed to a model."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:46
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:12
msgid ""
"**pixel_mask** -- Pixel mask to be fed to a model (when "
":obj:`pad_and_return_pixel_mask=True` or if `\"pixel_mask\"` is in "
":obj:`self.model_input_names`)."
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:48
msgid ""
"**labels** -- Optional labels to be fed to a model (when "
":obj:`annotations` are provided)"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask
#: transformers.DetrFeatureExtractor.post_process
#: transformers.DetrFeatureExtractor.post_process_panoptic
#: transformers.DetrFeatureExtractor.post_process_segmentation
#: transformers.DetrForObjectDetection.forward
#: transformers.DetrForSegmentation.forward transformers.DetrModel.forward
msgid "Return type"
msgstr ""

#: of transformers.DetrFeatureExtractor.__call__:49
#: transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:14
msgid ":class:`~transformers.BatchFeature`"
msgstr ""

#: of transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:1
msgid ""
"Pad images up to the largest image in a batch and create a corresponding "
":obj:`pixel_mask`."
msgstr ""

#: of transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:3
msgid ""
"List of images (pixel values) to be padded. Each image should be a tensor"
" of shape (C, H, W)."
msgstr ""

#: of transformers.DetrFeatureExtractor.pad_and_create_pixel_mask:9
msgid ""
"A :class:`~transformers.BatchFeature` with the following fields:  - "
"**pixel_values** -- Pixel values to be fed to a model. - **pixel_mask** "
"-- Pixel mask to be fed to a model (when "
":obj:`pad_and_return_pixel_mask=True` or if   `\"pixel_mask\"` is in "
":obj:`self.model_input_names`)."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process:1
msgid ""
"Converts the output of :class:`~transformers.DetrForObjectDetection` into"
" the format expected by the COCO api. Only supports PyTorch."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process:4
#: transformers.DetrFeatureExtractor.post_process_panoptic:4
#: transformers.DetrFeatureExtractor.post_process_segmentation:7
msgid "Raw outputs of the model."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process:6
msgid ""
"Tensor containing the size (h, w) of each image of the batch. For "
"evaluation, this must be the original image size (before any data "
"augmentation). For visualization, this should be the image size after "
"data augment, but before padding."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process:11
msgid ""
"A list of dictionaries, each dictionary containing the scores, labels and"
" boxes for an image in the batch as predicted by the model."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process:13
#: transformers.DetrFeatureExtractor.post_process_panoptic:20
#: transformers.DetrFeatureExtractor.post_process_segmentation:20
msgid ":obj:`List[Dict]`"
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_panoptic:1
msgid ""
"Converts the output of :class:`~transformers.DetrForSegmentation` into "
"actual panoptic predictions. Only supports PyTorch."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_panoptic:6
msgid ""
"Torch Tensor (or list) containing the size (h, w) of each image of the "
"batch, i.e. the size after data augmentation but before batching."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_panoptic:9
msgid ""
"Torch Tensor (or list) corresponding to the requested final size (h, w) "
"of each prediction. If left to None, it will default to the "
":obj:`processed_sizes`."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_panoptic:12
msgid ""
"Dictionary mapping class indices to either True or False, depending on "
"whether or not they are a thing. If not set, defaults to the "
":obj:`is_thing_map` of COCO panoptic."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_panoptic:15
msgid "Threshold to use to filter out queries."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_panoptic:18
msgid ""
"A list of dictionaries, each dictionary containing a PNG string and "
"segments_info values for an image in the batch as predicted by the model."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_segmentation:1
msgid ""
"Converts the output of :class:`~transformers.DetrForSegmentation` into "
"actual instance segmentation predictions. Only supports PyTorch."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_segmentation:4
msgid ""
"Results list obtained by "
":meth:`~transformers.DetrFeatureExtractor.post_process`, to which "
"\"masks\" results will be added."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_segmentation:9
msgid ""
"Tensor containing the size (h, w) of each image of the batch. For "
"evaluation, this must be the original image size (before any data "
"augmentation)."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_segmentation:12
msgid ""
"Tensor containing the maximum size (h, w) of each image of the batch. For"
" evaluation, this must be the original image size (before any data "
"augmentation)."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_segmentation:15
msgid "Threshold to use when turning the predicted masks into binary values."
msgstr ""

#: of transformers.DetrFeatureExtractor.post_process_segmentation:18
msgid ""
"A list of dictionaries, each dictionary containing the scores, labels, "
"boxes and masks for an image in the batch as predicted by the model."
msgstr ""

#: ../../source/model_doc/detr.rst:190
msgid "DetrModel"
msgstr ""

#: of transformers.DetrModel:1
msgid ""
"The bare DETR Model (consisting of a backbone and encoder-decoder "
"Transformer) outputting raw hidden-states without any specific head on "
"top."
msgstr ""

#: of transformers.DetrForObjectDetection:4 transformers.DetrForSegmentation:5
#: transformers.DetrModel:4
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.DetrForObjectDetection:8 transformers.DetrForSegmentation:9
#: transformers.DetrModel:8
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.DetrForObjectDetection:12
#: transformers.DetrForSegmentation:13 transformers.DetrModel:12
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.DetrModel.forward:1
msgid ""
"The :class:`~transformers.DetrModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:4
#: transformers.DetrForSegmentation.forward:4 transformers.DetrModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:8
#: transformers.DetrForSegmentation.forward:8 transformers.DetrModel.forward:8
msgid ""
"Pixel values. Padding will be ignored by default should you provide it.  "
"Pixel values can be obtained using "
":class:`~transformers.DetrFeatureExtractor`. See "
":meth:`transformers.DetrFeatureExtractor.__call__` for details."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:8
#: transformers.DetrForSegmentation.forward:8 transformers.DetrModel.forward:8
msgid "Pixel values. Padding will be ignored by default should you provide it."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:10
#: transformers.DetrForSegmentation.forward:10
#: transformers.DetrModel.forward:10
msgid ""
"Pixel values can be obtained using "
":class:`~transformers.DetrFeatureExtractor`. See "
":meth:`transformers.DetrFeatureExtractor.__call__` for details."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:13
#: transformers.DetrForSegmentation.forward:13
#: transformers.DetrModel.forward:13
msgid ""
"Mask to avoid performing attention on padding pixel values. Mask values "
"selected in ``[0, 1]``:  - 1 for pixels that are real (i.e. **not "
"masked**), - 0 for pixels that are padding (i.e. **masked**).  `What are "
"attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.DetrForObjectDetection.forward:13
#: transformers.DetrForSegmentation.forward:13
#: transformers.DetrModel.forward:13
msgid ""
"Mask to avoid performing attention on padding pixel values. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.DetrForObjectDetection.forward:18
#: transformers.DetrForSegmentation.forward:18
#: transformers.DetrModel.forward:18
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.DetrForObjectDetection.forward:20
#: transformers.DetrForSegmentation.forward:20
#: transformers.DetrModel.forward:20
msgid "Not used by default. Can be used to mask object queries."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:22
#: transformers.DetrForSegmentation.forward:22
#: transformers.DetrModel.forward:22
msgid ""
"Tuple consists of (:obj:`last_hidden_state`, `optional`: "
":obj:`hidden_states`, `optional`: :obj:`attentions`) "
":obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) is a sequence of hidden-states at the output "
"of the last layer of the encoder. Used in the cross-attention of the "
"decoder."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:27
#: transformers.DetrForSegmentation.forward:27
#: transformers.DetrModel.forward:27
msgid ""
"Optionally, instead of passing the flattened feature map (output of the "
"backbone + projection layer), you can choose to directly pass a flattened"
" representation of an image."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:30
#: transformers.DetrForSegmentation.forward:30
#: transformers.DetrModel.forward:30
msgid ""
"Optionally, instead of initializing the queries with a tensor of zeros, "
"you can choose to directly pass an embedded representation."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:33
#: transformers.DetrForSegmentation.forward:33
#: transformers.DetrModel.forward:33
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:36
#: transformers.DetrForSegmentation.forward:36
#: transformers.DetrModel.forward:36
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:39
#: transformers.DetrForSegmentation.forward:39
#: transformers.DetrModel.forward:39
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.DetrModel.forward:42
msgid ""
"A :class:`~transformers.models.detr.modeling_detr.DetrModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DetrConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model. - "
"**decoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the decoder at the output of   each layer"
" plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the decoder, "
"after the attention softmax, used to   compute the weighted average in "
"the self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the decoder's "
"cross-attention layer, after the   attention softmax, used to compute the"
" weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the encoder at the output of   each layer"
" plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the encoder, "
"after the attention softmax, used to   compute the weighted average in "
"the self-attention heads. - **intermediate_hidden_states** "
"(:obj:`torch.FloatTensor` of shape :obj:`(config.decoder_layers, "
"batch_size, sequence_length, hidden_size)`, `optional`, returned when "
"``config.auxiliary_loss=True``) -- Intermediate decoder activations, i.e."
" the output of each decoder layer, each of them gone through a   "
"layernorm.   Examples::      >>> from transformers import "
"DetrFeatureExtractor, DetrModel     >>> from PIL import Image     >>> "
"import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')     "
">>> model = DetrModel.from_pretrained('facebook/detr-resnet-50')     >>> "
"inputs = feature_extractor(images=image, return_tensors=\"pt\")     >>> "
"outputs = model(**inputs)     >>> last_hidden_states = "
"outputs.last_hidden_state"
msgstr ""

#: of transformers.DetrModel.forward:42
msgid ""
"A :class:`~transformers.models.detr.modeling_detr.DetrModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DetrConfig`) and inputs."
msgstr ""

#: of transformers.DetrModel.forward:46
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:65
#: transformers.DetrForSegmentation.forward:70
#: transformers.DetrModel.forward:47
msgid ""
"**decoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the decoder at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:68
#: transformers.DetrForSegmentation.forward:73
#: transformers.DetrModel.forward:50
msgid ""
"**decoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`. Attentions weights of the decoder, "
"after the attention softmax, used to compute the weighted average in the "
"self-attention heads."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:71
#: transformers.DetrForSegmentation.forward:76
#: transformers.DetrModel.forward:53
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`. Attentions weights of the decoder's "
"cross-attention layer, after the attention softmax, used to compute the "
"weighted average in the cross-attention heads."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:74
#: transformers.DetrForSegmentation.forward:79
#: transformers.DetrModel.forward:56
msgid ""
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:75
#: transformers.DetrForSegmentation.forward:80
#: transformers.DetrModel.forward:57
msgid ""
"**encoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the encoder at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:78
#: transformers.DetrForSegmentation.forward:83
#: transformers.DetrModel.forward:60
msgid ""
"**encoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`. Attentions weights of the encoder, "
"after the attention softmax, used to compute the weighted average in the "
"self-attention heads."
msgstr ""

#: of transformers.DetrModel.forward:63
msgid ""
"**intermediate_hidden_states** (:obj:`torch.FloatTensor` of shape "
":obj:`(config.decoder_layers, batch_size, sequence_length, hidden_size)`,"
" `optional`, returned when ``config.auxiliary_loss=True``) -- "
"Intermediate decoder activations, i.e. the output of each decoder layer, "
"each of them gone through a layernorm."
msgstr ""

#: of transformers.DetrModel.forward:81
msgid ""
":class:`~transformers.models.detr.modeling_detr.DetrModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/detr.rst:197
msgid "DetrForObjectDetection"
msgstr ""

#: of transformers.DetrForObjectDetection:1
msgid ""
"DETR Model (consisting of a backbone and encoder-decoder Transformer) "
"with object detection heads on top, for tasks such as COCO detection."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:1
msgid ""
"The :class:`~transformers.DetrForObjectDetection` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:41
msgid ""
"Labels for computing the bipartite matching loss. List of dicts, each "
"dictionary containing at least the following 2 keys: 'class_labels' and "
"'boxes' (the class labels and bounding boxes of an image in the batch "
"respectively). The class labels themselves should be a "
":obj:`torch.LongTensor` of len :obj:`(number of bounding boxes in the "
"image,)` and the boxes a :obj:`torch.FloatTensor` of shape :obj:`(number "
"of bounding boxes in the image, 4)`."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:48
msgid ""
"A "
":class:`~transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DetrConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` are provided)) -- Total loss as a"
" linear combination of a negative log-likehood (cross-entropy) for class "
"prediction and a   bounding box loss. The latter is defined as a linear "
"combination of the L1 loss and the generalized   scale-invariant IoU "
"loss. - **loss_dict** (:obj:`Dict`, `optional`) -- A dictionary "
"containing the individual losses. Useful for logging. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_queries, "
"num_classes + 1)`) -- Classification logits (including no-object) for all"
" queries. - **pred_boxes** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for "
"all queries, represented as (center_x, center_y, width, height). These   "
"values are normalized in [0, 1], relative to the size of each individual "
"image in the batch (disregarding   possible padding). You can use "
":meth:`~transformers.DetrFeatureExtractor.post_process` to retrieve the"
"   unnormalized bounding boxes. - **auxiliary_outputs** "
"(:obj:`list[Dict]`, `optional`) -- Optional, only returned when auxilary "
"losses are activated (i.e. :obj:`config.auxiliary_loss` is set to   "
"`True`) and labels are provided. It is a list of dictionnaries containing"
" the two above keys (:obj:`logits`   and :obj:`pred_boxes`) for each "
"decoder layer. - **last_hidden_state** (:obj:`torch.FloatTensor` of shape"
" :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the decoder "
"of the model. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the decoder at the output of   each layer"
" plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the decoder, "
"after the attention softmax, used to   compute the weighted average in "
"the self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the decoder's "
"cross-attention layer, after the   attention softmax, used to compute the"
" weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the encoder at the output of   each layer"
" plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the encoder, "
"after the attention softmax, used to   compute the weighted average in "
"the self-attention heads.   Examples::      >>> from transformers import "
"DetrFeatureExtractor, DetrForObjectDetection     >>> from PIL import "
"Image     >>> import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')     "
">>> model = DetrForObjectDetection.from_pretrained('facebook/detr-"
"resnet-50')      >>> inputs = feature_extractor(images=image, "
"return_tensors=\"pt\")     >>> outputs = model(**inputs)     >>> # model "
"predicts bounding boxes and corresponding COCO classes     >>> logits = "
"outputs.logits     >>> bboxes = outputs.pred_boxes"
msgstr ""

#: of transformers.DetrForObjectDetection.forward:48
msgid ""
"A "
":class:`~transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DetrConfig`) and "
"inputs."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:52
#: transformers.DetrForSegmentation.forward:53
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` are provided)) -- Total loss as a linear "
"combination of a negative log-likehood (cross-entropy) for class "
"prediction and a bounding box loss. The latter is defined as a linear "
"combination of the L1 loss and the generalized scale-invariant IoU loss."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:55
#: transformers.DetrForSegmentation.forward:56
msgid ""
"**loss_dict** (:obj:`Dict`, `optional`) -- A dictionary containing the "
"individual losses. Useful for logging."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:56
#: transformers.DetrForSegmentation.forward:57
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_queries, num_classes + 1)`) -- Classification logits (including no-"
"object) for all queries."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:57
#: transformers.DetrForSegmentation.forward:58
msgid ""
"**pred_boxes** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_queries, 4)`) -- Normalized boxes coordinates for all queries, "
"represented as (center_x, center_y, width, height). These values are "
"normalized in [0, 1], relative to the size of each individual image in "
"the batch (disregarding possible padding). You can use "
":meth:`~transformers.DetrFeatureExtractor.post_process` to retrieve the "
"unnormalized bounding boxes."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:61
#: transformers.DetrForSegmentation.forward:66
msgid ""
"**auxiliary_outputs** (:obj:`list[Dict]`, `optional`) -- Optional, only "
"returned when auxilary losses are activated (i.e. "
":obj:`config.auxiliary_loss` is set to `True`) and labels are provided. "
"It is a list of dictionnaries containing the two above keys "
"(:obj:`logits` and :obj:`pred_boxes`) for each decoder layer."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:64
#: transformers.DetrForSegmentation.forward:69
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the decoder "
"of the model."
msgstr ""

#: of transformers.DetrForObjectDetection.forward:100
msgid ""
":class:`~transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/detr.rst:204
msgid "DetrForSegmentation"
msgstr ""

#: of transformers.DetrForSegmentation:1
msgid ""
"DETR Model (consisting of a backbone and encoder-decoder Transformer) "
"with a segmentation head on top, for tasks such as COCO panoptic."
msgstr ""

#: of transformers.DetrForSegmentation.forward:1
msgid ""
"The :class:`~transformers.DetrForSegmentation` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.DetrForSegmentation.forward:41
msgid ""
"Labels for computing the bipartite matching loss, DICE/F-1 loss and Focal"
" loss. List of dicts, each dictionary containing at least the following 3"
" keys: 'class_labels', 'boxes' and 'masks' (the class labels, bounding "
"boxes and segmentation masks of an image in the batch respectively). The "
"class labels themselves should be a :obj:`torch.LongTensor` of len "
":obj:`(number of bounding boxes in the image,)`, the boxes a "
":obj:`torch.FloatTensor` of shape :obj:`(number of bounding boxes in the "
"image, 4)` and the masks a :obj:`torch.FloatTensor` of shape "
":obj:`(number of bounding boxes in the image, height, width)`."
msgstr ""

#: of transformers.DetrForSegmentation.forward:49
msgid ""
"A :class:`~transformers.models.detr.modeling_detr.DetrSegmentationOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DetrConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` are provided)) -- Total loss as a"
" linear combination of a negative log-likehood (cross-entropy) for class "
"prediction and a   bounding box loss. The latter is defined as a linear "
"combination of the L1 loss and the generalized   scale-invariant IoU "
"loss. - **loss_dict** (:obj:`Dict`, `optional`) -- A dictionary "
"containing the individual losses. Useful for logging. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_queries, "
"num_classes + 1)`) -- Classification logits (including no-object) for all"
" queries. - **pred_boxes** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for "
"all queries, represented as (center_x, center_y, width, height). These   "
"values are normalized in [0, 1], relative to the size of each individual "
"image in the batch (disregarding   possible padding). You can use "
":meth:`~transformers.DetrFeatureExtractor.post_process` to retrieve the"
"   unnormalized bounding boxes. - **pred_masks** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_queries, "
"height/4, width/4)`) -- Segmentation masks logits for all queries. See "
"also   "
":meth:`~transformers.DetrFeatureExtractor.post_process_segmentation` or"
"   :meth:`~transformers.DetrFeatureExtractor.post_process_panoptic` to "
"evaluate instance and panoptic   segmentation masks respectively. - "
"**auxiliary_outputs** (:obj:`list[Dict]`, `optional`) -- Optional, only "
"returned when auxilary losses are activated (i.e. "
":obj:`config.auxiliary_loss` is set to   `True`) and labels are provided."
" It is a list of dictionnaries containing the two above keys "
"(:obj:`logits`   and :obj:`pred_boxes`) for each decoder layer. - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the decoder "
"of the model. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the decoder at the output of   each layer"
" plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the decoder, "
"after the attention softmax, used to   compute the weighted average in "
"the self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the decoder's "
"cross-attention layer, after the   attention softmax, used to compute the"
" weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the encoder at the output of   each layer"
" plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights of the encoder, "
"after the attention softmax, used to   compute the weighted average in "
"the self-attention heads.   Examples::      >>> from transformers import "
"DetrFeatureExtractor, DetrForSegmentation     >>> from PIL import Image"
"     >>> import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = DetrFeatureExtractor.from_pretrained('facebook/detr-"
"resnet-50-panoptic')     >>> model = "
"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')"
"      >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")"
"     >>> outputs = model(**inputs)     >>> # model predicts COCO classes,"
" bounding boxes, and masks     >>> logits = outputs.logits     >>> bboxes"
" = outputs.pred_boxes     >>> masks = outputs.pred_masks"
msgstr ""

#: of transformers.DetrForSegmentation.forward:49
msgid ""
"A :class:`~transformers.models.detr.modeling_detr.DetrSegmentationOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DetrConfig`) and "
"inputs."
msgstr ""

#: of transformers.DetrForSegmentation.forward:62
msgid ""
"**pred_masks** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_queries, height/4, width/4)`) -- Segmentation masks logits for all "
"queries. See also "
":meth:`~transformers.DetrFeatureExtractor.post_process_segmentation` or "
":meth:`~transformers.DetrFeatureExtractor.post_process_panoptic` to "
"evaluate instance and panoptic segmentation masks respectively."
msgstr ""

#: of transformers.DetrForSegmentation.forward:106
msgid ""
":class:`~transformers.models.detr.modeling_detr.DetrSegmentationOutput` "
"or :obj:`tuple(torch.FloatTensor)`"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/hubert.rst:14
msgid "Hubert"
msgstr ""

#: ../../source/model_doc/hubert.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/hubert.rst:19
msgid ""
"Hubert was proposed in `HuBERT: Self-Supervised Speech Representation "
"Learning by Masked Prediction of Hidden Units "
"<https://arxiv.org/abs/2106.07447>`__ by Wei-Ning Hsu, Benjamin Bolte, "
"Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman "
"Mohamed."
msgstr ""

#: ../../source/model_doc/hubert.rst:23
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/hubert.rst:25
#, python-format
msgid ""
"*Self-supervised approaches for speech representation learning are "
"challenged by three unique problems: (1) there are multiple sound units "
"in each input utterance, (2) there is no lexicon of input sound units "
"during the pre-training phase, and (3) sound units have variable lengths "
"with no explicit segmentation. To deal with these three problems, we "
"propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech"
" representation learning, which utilizes an offline clustering step to "
"provide aligned target labels for a BERT-like prediction loss. A key "
"ingredient of our approach is applying the prediction loss over the "
"masked regions only, which forces the model to learn a combined acoustic "
"and language model over the continuous inputs. HuBERT relies primarily on"
" the consistency of the unsupervised clustering step rather than the "
"intrinsic quality of the assigned cluster labels. Starting with a simple "
"k-means teacher of 100 clusters, and using two iterations of clustering, "
"the HuBERT model either matches or improves upon the state-of-the-art "
"wav2vec 2.0 performance on the Librispeech (960h) and Libri-light "
"(60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning "
"subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% "
"relative WER reduction on the more challenging dev-other and test-other "
"evaluation subsets.*"
msgstr ""

#: ../../source/model_doc/hubert.rst:38
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/hubert.rst:40
msgid ""
"Hubert is a speech model that accepts a float array corresponding to the "
"raw waveform of the speech signal."
msgstr ""

#: ../../source/model_doc/hubert.rst:41
msgid ""
"Hubert model was fine-tuned using connectionist temporal classification "
"(CTC) so the model output has to be decoded using "
":class:`~transformers.Wav2Vec2CTCTokenizer`."
msgstr ""

#: ../../source/model_doc/hubert.rst:44
msgid ""
"This model was contributed by `patrickvonplaten "
"<https://huggingface.co/patrickvonplaten>`__."
msgstr ""

#: ../../source/model_doc/hubert.rst:48
msgid "HubertConfig"
msgstr ""

#: of transformers.HubertConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.HubertModel`. It is used to instantiate an Hubert "
"model according to the specified arguments, defining the model "
"architecture. Instantiating a configuration with the defaults will yield "
"a similar configuration to that of the Hubert `facebook/hubert-base-ls960"
" <https://huggingface.co/facebook/hubert-base-ls960>`__ architecture."
msgstr ""

#: of transformers.HubertConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.HubertConfig transformers.HubertForCTC
#: transformers.HubertForCTC.forward transformers.HubertModel
#: transformers.HubertModel.forward transformers.TFHubertForCTC
#: transformers.TFHubertForCTC.call transformers.TFHubertModel
#: transformers.TFHubertModel.call
msgid "Parameters"
msgstr ""

#: of transformers.HubertConfig:10
msgid ""
"Vocabulary size of the Hubert model. Defines the number of different "
"tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.HubertModel`. Vocabulary size of the model."
" Defines the different tokens that can be represented by the `inputs_ids`"
" passed to the forward method of :class:`~transformers.HubertModel`."
msgstr ""

#: of transformers.HubertConfig:15
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.HubertConfig:17
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.HubertConfig:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.HubertConfig:21
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.HubertConfig:23
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.HubertConfig:26
msgid ""
"The dropout probabilitiy for all fully connected layers in the "
"embeddings, encoder, and pooler."
msgstr ""

#: of transformers.HubertConfig:28
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.HubertConfig:30
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.HubertConfig:32
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.HubertConfig:34
msgid ""
"The norm to be applied to 1D convolutional layers in feature extractor. "
"One of :obj:`\"group\"` for group normalization of only the first 1D "
"convolutional layer or :obj:`\"layer\"` for layer normalization of all 1D"
" convolutional layers."
msgstr ""

#: of transformers.HubertConfig:38
msgid ""
"The dropout probabilitiy for all 1D convolutional layers in feature "
"extractor."
msgstr ""

#: of transformers.HubertConfig:40
msgid ""
"The non-linear activation function (function or string) in the 1D "
"convolutional layers of the feature extractor. If string, "
":obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` and :obj:`\"gelu_new\"`"
" are supported."
msgstr ""

#: of transformers.HubertConfig:43
msgid ""
"A tuple of integers defining the number of input and output channels of "
"each 1D convolutional layer in the feature extractor. The length of "
"`conv_dim` defines the number of 1D convolutional layers."
msgstr ""

#: of transformers.HubertConfig:46
msgid ""
"A tuple of integers defining the stride of each 1D convolutional layer in"
" the feature extractor. The length of `conv_stride` defines the number of"
" convolutional layers and has to match the the length of `conv_dim`."
msgstr ""

#: of transformers.HubertConfig:49
msgid ""
"A tuple of integers defining the kernel size of each 1D convolutional "
"layer in the feature extractor. The length of `conv_kernel` defines the "
"number of convolutional layers and has to match the the length of "
"`conv_dim`."
msgstr ""

#: of transformers.HubertConfig:53
msgid "Whether the 1D convolutional layers have a bias."
msgstr ""

#: of transformers.HubertConfig:55
msgid ""
"Number of convolutional positional embeddings. Defines the kernel size of"
" 1D convolutional positional embeddings layer."
msgstr ""

#: of transformers.HubertConfig:58
msgid "Number of groups of 1D convolutional positional embeddings layer."
msgstr ""

#: of transformers.HubertConfig:60
msgid ""
"Whether do apply `stable` layer norm architecture of the Transformer "
"encoder. ``do_stable_layer_norm is True`` corresponds to applying layer "
"norm before the attention layer, whereas ``do_stable_layer_norm is "
"False`` corresponds to applying layer norm after the attention layer."
msgstr ""

#: of transformers.HubertConfig:64
msgid ""
"Whether to apply *SpecAugment* data augmentation to the outputs of the "
"feature extractor. For reference see `SpecAugment: A Simple Data "
"Augmentation Method for Automatic Speech Recognition "
"<https://arxiv.org/abs/1904.08779>`__."
msgstr ""

#: of transformers.HubertConfig:68
msgid ""
"Propability of each feature vector along the time axis to be chosen as "
"the start of the vector span to be masked. Approximately ``mask_time_prob"
" * sequence_length // mask_time_length`` feature vectors will be masked "
"along the time axis. This is only relevant if ``apply_spec_augment is "
"True``."
msgstr ""

#: of transformers.HubertConfig:72
msgid "Length of vector span along the time axis."
msgstr ""

#: of transformers.HubertConfig:74
msgid ""
"Propability of each feature vector along the feature axis to be chosen as"
" the start of the vector span to be masked. Approximately "
"``mask_time_prob * hidden_size // mask_time_length`` feature vectors will"
" be masked along the time axis. This is only relevant if "
"``apply_spec_augment is True``."
msgstr ""

#: of transformers.HubertConfig:78
msgid "Length of vector span along the feature axis."
msgstr ""

#: of transformers.HubertConfig:80
msgid ""
"Specifies the reduction to apply to the output of ``torch.nn.CTCLoss``. "
"Only relevant when training an instance of "
":class:`~transformers.HubertForCTC`."
msgstr ""

#: of transformers.HubertConfig:83
msgid ""
"Whether to zero infinite losses and the associated gradients of "
"``torch.nn.CTCLoss``. Infinite losses mainly occur when the inputs are "
"too short to be aligned to the targets. Only relevant when training an "
"instance of :class:`~transformers.HubertForCTC`."
msgstr ""

#: of transformers.HubertConfig:87
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.HubertConfig:90 transformers.HubertForCTC.forward:62
#: transformers.HubertModel.forward:56 transformers.TFHubertForCTC.call:81
#: transformers.TFHubertModel.call:76
msgid "Example::"
msgstr ""

#: ../../source/model_doc/hubert.rst:55
msgid "HubertModel"
msgstr ""

#: of transformers.HubertModel:1
msgid ""
"The bare Hubert Model transformer outputting raw hidden-states without "
"any specific head on top. Hubert was proposed in `HuBERT: Self-Supervised"
" Speech Representation Learning by Masked Prediction of Hidden Units "
"<https://arxiv.org/abs/2106.07447>`__ by Wei-Ning Hsu, Benjamin Bolte, "
"Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman "
"Mohamed."
msgstr ""

#: of transformers.HubertForCTC:6 transformers.HubertModel:6
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving etc.)."
msgstr ""

#: of transformers.HubertForCTC:9 transformers.HubertModel:9
msgid ""
"This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.HubertForCTC:13 transformers.HubertModel:13
#: transformers.TFHubertForCTC:30 transformers.TFHubertModel:30
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.HubertModel.forward:1
msgid ""
"The :class:`~transformers.HubertModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.HubertForCTC.forward:4 transformers.HubertModel.forward:4
#: transformers.TFHubertForCTC.call:4 transformers.TFHubertModel.call:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.HubertForCTC.forward:8 transformers.HubertModel.forward:8
msgid ""
"Float values of input raw speech waveform. Values can be obtained by "
"loading a `.flac` or `.wav` audio file into an array of type "
"`List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library "
"(`pip install soundfile`). To prepare the array into `input_values`, the "
":class:`~transformers.Wav2Vec2Processor` should be used for padding and "
"conversion into a tensor of type `torch.FloatTensor`. See "
":meth:`transformers.Wav2Vec2Processor.__call__` for details."
msgstr ""

#: of transformers.HubertForCTC.forward:14 transformers.HubertModel.forward:14
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:  - 1 for tokens that are "
"**not masked**, - 0 for tokens that are **masked**.  `What are attention "
"masks? <../glossary.html#attention-mask>`__  .. warning::     "
":obj:`attention_mask` should only be passed if the corresponding "
"processor has     ``config.return_attention_mask == True``. For all "
"models whose processor has     ``config.return_attention_mask == False``,"
" such as `hubert-base     <https://huggingface.co/facebook/hubert-base-"
"ls960>`__, :obj:`attention_mask` should **not** be passed     to avoid "
"degraded performance when doing batched inference. For such models "
":obj:`input_values` should     simply be padded with 0 and passed without"
" :obj:`attention_mask`. Be aware that these models also yield     "
"slightly different results depending on whether :obj:`input_values` is "
"padded or not."
msgstr ""

#: of transformers.HubertForCTC.forward:14 transformers.HubertModel.forward:14
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.HubertForCTC.forward:17 transformers.HubertModel.forward:17
#: transformers.TFHubertForCTC.call:18 transformers.TFHubertModel.call:18
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.HubertForCTC.forward:18 transformers.HubertModel.forward:18
#: transformers.TFHubertForCTC.call:19 transformers.TFHubertModel.call:19
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.HubertForCTC.forward:20 transformers.HubertModel.forward:20
#: transformers.TFHubertForCTC.call:21 transformers.TFHubertModel.call:21
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.HubertForCTC.forward:23 transformers.HubertModel.forward:23
msgid ""
":obj:`attention_mask` should only be passed if the corresponding "
"processor has ``config.return_attention_mask == True``. For all models "
"whose processor has ``config.return_attention_mask == False``, such as "
"`hubert-base <https://huggingface.co/facebook/hubert-base-ls960>`__, "
":obj:`attention_mask` should **not** be passed to avoid degraded "
"performance when doing batched inference. For such models "
":obj:`input_values` should simply be padded with 0 and passed without "
":obj:`attention_mask`. Be aware that these models also yield slightly "
"different results depending on whether :obj:`input_values` is padded or "
"not."
msgstr ""

#: of transformers.HubertForCTC.forward:31 transformers.HubertModel.forward:31
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.HubertForCTC.forward:34 transformers.HubertModel.forward:34
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.HubertForCTC.forward:37 transformers.HubertModel.forward:37
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.HubertForCTC.forward transformers.HubertModel.forward
#: transformers.TFHubertForCTC.call transformers.TFHubertModel.call
msgid "Returns"
msgstr ""

#: of transformers.HubertModel.forward:40
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> from transformers import "
"Wav2Vec2Processor, HubertModel     >>> from datasets import load_dataset"
"     >>> import soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
"     >>> model = HubertModel.from_pretrained(\"facebook/hubert-large-"
"ls960-ft\")      >>> def map_to_array(batch):     ...     speech, _ = "
"sf.read(batch[\"file\"])     ...     batch[\"speech\"] = speech     ..."
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"pt\").input_values  # Batch size 1     >>> hidden_states"
" = model(input_values).last_hidden_state"
msgstr ""

#: of transformers.HubertModel.forward:40
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs."
msgstr ""

#: of transformers.HubertModel.forward:44
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.HubertForCTC.forward:51 transformers.HubertModel.forward:45
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.HubertForCTC.forward:54 transformers.HubertModel.forward:48
#: transformers.TFHubertForCTC.call:73 transformers.TFHubertModel.call:68
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.HubertForCTC.forward:55 transformers.HubertModel.forward:49
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.HubertForCTC.forward:58 transformers.HubertModel.forward:52
#: transformers.TFHubertForCTC.call:77 transformers.TFHubertModel.call:72
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.HubertForCTC.forward transformers.HubertModel.forward
#: transformers.TFHubertForCTC.call transformers.TFHubertModel.call
msgid "Return type"
msgstr ""

#: of transformers.HubertModel.forward:75
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/hubert.rst:62
msgid "HubertForCTC"
msgstr ""

#: of transformers.HubertForCTC:1
msgid ""
"Hubert Model with a `language modeling` head on top for Connectionist "
"Temporal Classification (CTC). Hubert was proposed in `HuBERT: Self-"
"Supervised Speech Representation Learning by Masked Prediction of Hidden "
"Units <https://arxiv.org/abs/2106.07447>`__ by Wei-Ning Hsu, Benjamin "
"Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, "
"Abdelrahman Mohamed."
msgstr ""

#: of transformers.HubertForCTC.forward:1
msgid ""
"The :class:`~transformers.HubertForCTC` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.HubertForCTC.forward:39
msgid ""
"Labels for connectionist temporal classification. Note that "
"``target_length`` has to be smaller or equal to the sequence length of "
"the output logits. Indices are selected in ``[-100, 0, ..., "
"config.vocab_size - 1]``. All labels set to ``-100`` are ignored "
"(masked), the loss is only computed for labels in ``[0, ..., "
"config.vocab_size - 1]``."
msgstr ""

#: of transformers.HubertForCTC.forward:45
msgid ""
"A :class:`~transformers.modeling_outputs.CausalLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction). - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> import torch     >>> from "
"transformers import Wav2Vec2Processor, HubertForCTC     >>> from datasets"
" import load_dataset     >>> import soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
"     >>> model = HubertForCTC.from_pretrained(\"facebook/hubert-large-"
"ls960-ft\")      >>> def map_to_array(batch):     ...     speech, _ = "
"sf.read(batch[\"file\"])     ...     batch[\"speech\"] = speech     ..."
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"pt\").input_values  # Batch size 1     >>> logits = "
"model(input_values).logits     >>> predicted_ids = torch.argmax(logits, "
"dim=-1)      >>> transcription = processor.decode(predicted_ids[0])      "
">>> # compute loss     >>> target_transcription = \"A MAN SAID TO THE "
"UNIVERSE SIR I EXIST\"      >>> # wrap processor as target processor to "
"encode labels     >>> with processor.as_target_processor():     ...     "
"labels = processor(target_transcription, return_tensors=\"pt\").input_ids"
"      >>> loss = model(input_values, labels=labels).loss"
msgstr ""

#: of transformers.HubertForCTC.forward:45
msgid ""
"A :class:`~transformers.modeling_outputs.CausalLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs."
msgstr ""

#: of transformers.HubertForCTC.forward:49
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction)."
msgstr ""

#: of transformers.HubertForCTC.forward:50
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.HubertForCTC.forward:94
msgid ""
":class:`~transformers.modeling_outputs.CausalLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/hubert.rst:68
msgid "TFHubertModel"
msgstr ""

#: of transformers.TFHubertModel:1
msgid ""
"The bare TFHubert Model transformer outputing raw hidden-states without "
"any specific head on top."
msgstr ""

#: of transformers.TFHubertForCTC:3 transformers.TFHubertModel:3
msgid ""
"This model inherits from :class:`~transformers.TFPreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TFHubertForCTC:7 transformers.TFHubertModel:7
msgid ""
"This model is also a `tf.keras.Model "
"<https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. "
"Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFHubertForCTC:13 transformers.TFHubertModel:13
msgid "TF 2.0 models accepts two formats as inputs:"
msgstr ""

#: of transformers.TFHubertForCTC:15 transformers.TFHubertModel:15
msgid "having all inputs as keyword arguments (like PyTorch models), or"
msgstr ""

#: of transformers.TFHubertForCTC:16 transformers.TFHubertModel:16
msgid ""
"having all inputs as a list, tuple or dict in the first positional "
"arguments."
msgstr ""

#: of transformers.TFHubertForCTC:18 transformers.TFHubertModel:18
msgid ""
"This second option is useful when using :meth:`tf.keras.Model.fit` method"
" which currently requires having all the tensors in the first argument of"
" the model call function: :obj:`model(inputs)`."
msgstr ""

#: of transformers.TFHubertForCTC:21 transformers.TFHubertModel:21
msgid ""
"If you choose this second option, there are three possibilities you can "
"use to gather all the input Tensors in the first positional argument :"
msgstr ""

#: of transformers.TFHubertForCTC:24 transformers.TFHubertModel:24
msgid ""
"a single Tensor with :obj:`input_values` only and nothing else: "
":obj:`model(inputs_ids)`"
msgstr ""

#: of transformers.TFHubertForCTC:25 transformers.TFHubertModel:25
msgid ""
"a list of varying length with one or several input Tensors IN THE ORDER "
"given in the docstring: :obj:`model([input_values, attention_mask])` or "
":obj:`model([input_values, attention_mask, token_type_ids])`"
msgstr ""

#: of transformers.TFHubertForCTC:27 transformers.TFHubertModel:27
msgid ""
"a dictionary with one or several input Tensors associated to the input "
"names given in the docstring: :obj:`model({\"input_values\": "
"input_values, \"token_type_ids\": token_type_ids})`"
msgstr ""

#: of transformers.TFHubertModel.call:1
msgid ""
"The :class:`~transformers.TFHubertModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFHubertForCTC.call:8 transformers.TFHubertModel.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.BertTokenizer`. See "
":func:`transformers.PreTrainedTokenizer.__call__` and "
":func:`transformers.PreTrainedTokenizer.encode` for details.  `What are "
"input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:8 transformers.TFHubertModel.call:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.TFHubertForCTC.call:10 transformers.TFHubertModel.call:10
msgid ""
"Indices can be obtained using :class:`~transformers.BertTokenizer`. See "
":func:`transformers.PreTrainedTokenizer.__call__` and "
":func:`transformers.PreTrainedTokenizer.encode` for details."
msgstr ""

#: of transformers.TFHubertForCTC.call:14 transformers.TFHubertModel.call:14
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:16 transformers.TFHubertModel.call:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:16 transformers.TFHubertModel.call:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFHubertForCTC.call:23 transformers.TFHubertModel.call:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:  - 0 corresponds to a "
"`sentence A` token, - 1 corresponds to a `sentence B` token.  `What are "
"token type IDs? <../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:23 transformers.TFHubertModel.call:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFHubertForCTC.call:26 transformers.TFHubertModel.call:26
msgid "0 corresponds to a `sentence A` token,"
msgstr ""

#: of transformers.TFHubertForCTC.call:27 transformers.TFHubertModel.call:27
msgid "1 corresponds to a `sentence B` token."
msgstr ""

#: of transformers.TFHubertForCTC.call:29 transformers.TFHubertModel.call:29
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:31 transformers.TFHubertModel.call:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``.  `What are position IDs? <../glossary.html#position-ids>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:31 transformers.TFHubertModel.call:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``."
msgstr ""

#: of transformers.TFHubertForCTC.call:34 transformers.TFHubertModel.call:34
msgid "`What are position IDs? <../glossary.html#position-ids>`__"
msgstr ""

#: of transformers.TFHubertForCTC.call:36 transformers.TFHubertModel.call:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.TFHubertForCTC.call:36 transformers.TFHubertModel.call:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFHubertForCTC.call:38 transformers.TFHubertModel.call:38
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.TFHubertForCTC.call:39 transformers.TFHubertModel.call:39
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.TFHubertForCTC.call:41 transformers.TFHubertModel.call:41
msgid ""
"Optionally, instead of passing :obj:`input_values` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_values` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.TFHubertForCTC.call:45 transformers.TFHubertModel.call:45
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFHubertForCTC.call:49 transformers.TFHubertModel.call:49
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFHubertForCTC.call:53 transformers.TFHubertModel.call:53
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple. This argument can be used in eager mode, in "
"graph mode the value will always be set to True."
msgstr ""

#: of transformers.TFHubertForCTC.call:56 transformers.TFHubertModel.call:56
msgid ""
"Whether or not to use the model in training mode (some modules like "
"dropout modules have different behaviors between training and "
"evaluation)."
msgstr ""

#: of transformers.TFHubertModel.call:60
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFBaseModelOutput` or a tuple"
" of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model. - **hidden_states** "
"(:obj:`tuple(tf.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads.   "
"Example::      >>> from transformers import Wav2Vec2Processor, "
"TFHubertModel     >>> from datasets import load_dataset     >>> import "
"soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/hubert-base-960h\")     >>> "
"model = TFHubertModel.from_pretrained(\"facebook/hubert-base-960h\")"
"      >>> def map_to_array(batch):     ...     speech, _ = "
"sf.read(batch[\"file\"])     ...     batch[\"speech\"] = speech     ..."
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"tf\").input_values  # Batch size 1     >>> hidden_states"
" = model(input_values).last_hidden_state"
msgstr ""

#: of transformers.TFHubertModel.call:60
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFBaseModelOutput` or a tuple"
" of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs."
msgstr ""

#: of transformers.TFHubertModel.call:64
msgid ""
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model."
msgstr ""

#: of transformers.TFHubertModel.call:65
msgid ""
"**hidden_states** (:obj:`tuple(tf.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFHubertForCTC.call:74 transformers.TFHubertModel.call:69
msgid ""
"**attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFHubertModel.call:95
msgid ""
":class:`~transformers.modeling_tf_outputs.TFBaseModelOutput` or "
":obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/hubert.rst:75
msgid "TFHubertForCTC"
msgstr ""

#: of transformers.TFHubertForCTC:1
msgid ""
"TFHubert Model with a `language modeling` head on top for Connectionist "
"Temporal Classification (CTC)."
msgstr ""

#: of transformers.TFHubertForCTC.call:1
msgid ""
"The :class:`~transformers.TFHubertForCTC` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFHubertForCTC.call:59
msgid ""
"Labels for computing the masked language modeling loss. Indices should be"
" in ``[-100, 0, ..., config.vocab_size]`` (see ``input_values`` "
"docstring) Tokens with indices set to ``-100`` are ignored (masked), the "
"loss is only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.TFHubertForCTC.call:64
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFCausalLMOutput` or a tuple "
"of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs.  - "
"**loss** (:obj:`tf.Tensor` of shape :obj:`(n,)`, `optional`, where n is "
"the number of non-masked labels, returned when :obj:`labels` is provided)"
" -- Language modeling loss (for next-token prediction). - **logits** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax). - **hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads.   "
"Example::      >>> import tensorflow as tf     >>> from transformers "
"import Wav2Vec2Processor, TFHubertForCTC     >>> from datasets import "
"load_dataset     >>> import soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/hubert-base-960h\")     >>> "
"model = TFHubertForCTC.from_pretrained(\"facebook/hubert-base-960h\")"
"      >>> def map_to_array(batch):     ...     speech, _ = "
"sf.read(batch[\"file\"])     ...     batch[\"speech\"] = speech     ..."
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"tf\").input_values # Batch size 1     >>> logits = "
"model(input_values).logits >>> predicted_ids = tf.argmax(logits, axis=-1)"
"      >>> transcription = processor.decode(predicted_ids[0])      >>> # "
"compute loss     >>> target_transcription = \"A MAN SAID TO THE UNIVERSE "
"SIR I EXIST\"      >>> # wrap processor as target processor to encode "
"labels     >>> with processor.as_target_processor():     ...     labels ="
" processor(transcription, return_tensors=\"tf\").input_values      >>> "
"loss = model(input_values, labels=labels).loss"
msgstr ""

#: of transformers.TFHubertForCTC.call:64
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFCausalLMOutput` or a tuple "
"of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.HubertConfig`) and inputs."
msgstr ""

#: of transformers.TFHubertForCTC.call:68
msgid ""
"**loss** (:obj:`tf.Tensor` of shape :obj:`(n,)`, `optional`, where n is "
"the number of non-masked labels, returned when :obj:`labels` is provided)"
" -- Language modeling loss (for next-token prediction)."
msgstr ""

#: of transformers.TFHubertForCTC.call:69
msgid ""
"**logits** (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length,"
" config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.TFHubertForCTC.call:70
msgid ""
"**hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFHubertForCTC.call:112
msgid ""
":class:`~transformers.modeling_tf_outputs.TFCausalLMOutput` or "
":obj:`tuple(tf.Tensor)`"
msgstr ""


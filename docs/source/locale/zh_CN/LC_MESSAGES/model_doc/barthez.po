# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/barthez.rst:14
msgid "BARThez"
msgstr ""

#: ../../source/model_doc/barthez.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/barthez.rst:19
msgid ""
"The BARThez model was proposed in `BARThez: a Skilled Pretrained French "
"Sequence-to-Sequence Model <https://arxiv.org/abs/2010.12321>`__ by "
"Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis on 23 "
"Oct, 2020."
msgstr ""

#: ../../source/model_doc/barthez.rst:23
msgid "The abstract of the paper:"
msgstr ""

#: ../../source/model_doc/barthez.rst:26
msgid ""
"*Inductive transfer learning, enabled by self-supervised learning, have "
"taken the entire Natural Language Processing (NLP) field by storm, with "
"models such as BERT and BART setting new state of the art on countless "
"natural language understanding tasks. While there are some notable "
"exceptions, most of the available models and research have been conducted"
" for the English language. In this work, we introduce BARThez, the first "
"BART model for the French language (to the best of our knowledge). "
"BARThez was pretrained on a very large monolingual French corpus from "
"past research that we adapted to suit BART's perturbation schemes. Unlike"
" already existing BERT-based French language models such as CamemBERT and"
" FlauBERT, BARThez is particularly well-suited for generative tasks, "
"since not only its encoder but also its decoder is pretrained. In "
"addition to discriminative tasks from the FLUE benchmark, we evaluate "
"BARThez on a novel summarization dataset, OrangeSum, that we release with"
" this paper. We also continue the pretraining of an already pretrained "
"multilingual BART on BARThez's corpus, and we show that the resulting "
"model, which we call mBARTHez, provides a significant boost over vanilla "
"BARThez, and is on par with or outperforms CamemBERT and FlauBERT.*"
msgstr ""

#: ../../source/model_doc/barthez.rst:38
msgid ""
"This model was contributed by `moussakam "
"<https://huggingface.co/moussakam>`__. The Authors' code can be found "
"`here <https://github.com/moussaKam/BARThez>`__."
msgstr ""

#: ../../source/model_doc/barthez.rst:43
msgid "Examples"
msgstr ""

#: ../../source/model_doc/barthez.rst:45
msgid ""
"BARThez can be fine-tuned on sequence-to-sequence tasks in a similar way "
"as BART, check: :prefix_link:`examples/pytorch/summarization/ "
"<examples/pytorch/summarization/README.md>`."
msgstr ""

#: ../../source/model_doc/barthez.rst:50
msgid "BarthezTokenizer"
msgstr ""

#: of transformers.BarthezTokenizer:1
msgid ""
"Adapted from :class:`~transformers.CamembertTokenizer` and "
":class:`~transformers.BartTokenizer`. Construct a BARThez tokenizer. "
"Based on `SentencePiece <https://github.com/google/sentencepiece>`__."
msgstr ""

#: of transformers.BarthezTokenizer:4
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods."
msgstr ""

#: of transformers.BarthezTokenizer
#: transformers.BarthezTokenizer.build_inputs_with_special_tokens
#: transformers.BarthezTokenizer.create_token_type_ids_from_sequences
#: transformers.BarthezTokenizer.get_special_tokens_mask
#: transformers.BarthezTokenizer.save_vocabulary
#: transformers.BarthezTokenizerFast
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences
#: transformers.BarthezTokenizerFast.save_vocabulary
msgid "Parameters"
msgstr ""

#: of transformers.BarthezTokenizer:7 transformers.BarthezTokenizerFast:7
msgid ""
"`SentencePiece <https://github.com/google/sentencepiece>`__ file "
"(generally has a `.spm` extension) that contains the vocabulary necessary"
" to instantiate a tokenizer."
msgstr ""

#: of transformers.BarthezTokenizer:10 transformers.BarthezTokenizerFast:10
msgid ""
"The beginning of sequence token that was used during pretraining. Can be "
"used a sequence classifier token.  .. note::      When building a "
"sequence using special tokens, this is not the token that is used for the"
" beginning of     sequence. The token used is the :obj:`cls_token`."
msgstr ""

#: of transformers.BarthezTokenizer:10 transformers.BarthezTokenizerFast:10
msgid ""
"The beginning of sequence token that was used during pretraining. Can be "
"used a sequence classifier token."
msgstr ""

#: of transformers.BarthezTokenizer:14 transformers.BarthezTokenizerFast:14
msgid ""
"When building a sequence using special tokens, this is not the token that"
" is used for the beginning of sequence. The token used is the "
":obj:`cls_token`."
msgstr ""

#: of transformers.BarthezTokenizer:17 transformers.BarthezTokenizerFast:17
msgid ""
"The end of sequence token.  .. note::      When building a sequence using"
" special tokens, this is not the token that is used for the end of     "
"sequence. The token used is the :obj:`sep_token`."
msgstr ""

#: of transformers.BarthezTokenizer:17 transformers.BarthezTokenizerFast:17
msgid "The end of sequence token."
msgstr ""

#: of transformers.BarthezTokenizer:21 transformers.BarthezTokenizerFast:21
msgid ""
"When building a sequence using special tokens, this is not the token that"
" is used for the end of sequence. The token used is the :obj:`sep_token`."
msgstr ""

#: of transformers.BarthezTokenizer:24 transformers.BarthezTokenizerFast:24
msgid ""
"The separator token, which is used when building a sequence from multiple"
" sequences, e.g. two sequences for sequence classification or for a text "
"and a question for question answering. It is also used as the last token "
"of a sequence built with special tokens."
msgstr ""

#: of transformers.BarthezTokenizer:28 transformers.BarthezTokenizerFast:28
msgid ""
"The classifier token which is used when doing sequence classification "
"(classification of the whole sequence instead of per-token "
"classification). It is the first token of the sequence when built with "
"special tokens."
msgstr ""

#: of transformers.BarthezTokenizer:31 transformers.BarthezTokenizerFast:31
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.BarthezTokenizer:34 transformers.BarthezTokenizerFast:34
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.BarthezTokenizer:36 transformers.BarthezTokenizerFast:36
msgid ""
"The token used for masking values. This is the token used when training "
"this model with masked language modeling. This is the token which the "
"model will try to predict."
msgstr ""

#: of transformers.BarthezTokenizer:39 transformers.BarthezTokenizerFast:39
msgid "Additional special tokens used by the tokenizer."
msgstr ""

#: of transformers.BarthezTokenizer:41
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:  - ``enable_sampling``: Enable subword "
"regularization. - ``nbest_size``: Sampling parameters for unigram. "
"Invalid for BPE-Dropout.    - ``nbest_size = {0,1}``: No sampling is "
"performed.   - ``nbest_size > 1``: samples from the nbest_size results."
"   - ``nbest_size < 0``: assuming that nbest_size is infinite and samples"
" from the all hypothesis (lattice)     using forward-filtering-and-"
"backward-sampling algorithm.  - ``alpha``: Smoothing parameter for "
"unigram sampling, and dropout probability of merge operations for   BPE-"
"dropout."
msgstr ""

#: of transformers.BarthezTokenizer:41
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:"
msgstr ""

#: of transformers.BarthezTokenizer:44
msgid "``enable_sampling``: Enable subword regularization."
msgstr ""

#: of transformers.BarthezTokenizer:45
msgid "``nbest_size``: Sampling parameters for unigram. Invalid for BPE-Dropout."
msgstr ""

#: of transformers.BarthezTokenizer:47
msgid "``nbest_size = {0,1}``: No sampling is performed."
msgstr ""

#: of transformers.BarthezTokenizer:48
msgid "``nbest_size > 1``: samples from the nbest_size results."
msgstr ""

#: of transformers.BarthezTokenizer:49
msgid ""
"``nbest_size < 0``: assuming that nbest_size is infinite and samples from"
" the all hypothesis (lattice) using forward-filtering-and-backward-"
"sampling algorithm."
msgstr ""

#: of transformers.BarthezTokenizer:52
msgid ""
"``alpha``: Smoothing parameter for unigram sampling, and dropout "
"probability of merge operations for BPE-dropout."
msgstr ""

#: of transformers.BarthezTokenizer:58
msgid ""
"The `SentencePiece` processor that is used for every conversion (string, "
"tokens and IDs)."
msgstr ""

#: of transformers.BarthezTokenizer transformers.BarthezTokenizer.vocab_size
msgid "type"
msgstr ""

#: of transformers.BarthezTokenizer:60
msgid ":obj:`SentencePieceProcessor`"
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:1
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens. A "
"BARThez sequence has the following format:"
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:4
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:4
msgid "single sequence: ``<s> X </s>``"
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:5
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:5
msgid "pair of sequences: ``<s> A </s></s> B </s>``"
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:7
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:7
msgid "List of IDs to which the special tokens will be added."
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:9
#: transformers.BarthezTokenizer.create_token_type_ids_from_sequences:5
#: transformers.BarthezTokenizer.get_special_tokens_mask:6
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:9
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences:5
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens
#: transformers.BarthezTokenizer.create_token_type_ids_from_sequences
#: transformers.BarthezTokenizer.get_special_tokens_mask
#: transformers.BarthezTokenizer.get_vocab
#: transformers.BarthezTokenizer.save_vocabulary
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences
#: transformers.BarthezTokenizerFast.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:12
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:12
msgid ""
"List of `input IDs <../glossary.html#input-ids>`__ with the appropriate "
"special tokens."
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens
#: transformers.BarthezTokenizer.create_token_type_ids_from_sequences
#: transformers.BarthezTokenizer.get_special_tokens_mask
#: transformers.BarthezTokenizer.get_vocab
#: transformers.BarthezTokenizer.save_vocabulary
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences
#: transformers.BarthezTokenizerFast.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.BarthezTokenizer.build_inputs_with_special_tokens:13
#: transformers.BarthezTokenizer.create_token_type_ids_from_sequences:9
#: transformers.BarthezTokenizer.get_special_tokens_mask:12
#: transformers.BarthezTokenizerFast.build_inputs_with_special_tokens:13
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences:9
msgid ":obj:`List[int]`"
msgstr ""

#: of transformers.BarthezTokenizer.convert_tokens_to_string:1
msgid "Converts a sequence of tokens (strings for sub-words) in a single string."
msgstr ""

#: of transformers.BarthezTokenizer.create_token_type_ids_from_sequences:1
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences:1
msgid ""
"Create a mask from the two sequences passed to be used in a sequence-pair"
" classification task."
msgstr ""

#: of transformers.BarthezTokenizer.create_token_type_ids_from_sequences:3
#: transformers.BarthezTokenizer.get_special_tokens_mask:4
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences:3
msgid "List of IDs."
msgstr ""

#: of transformers.BarthezTokenizer.create_token_type_ids_from_sequences:8
#: transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences:8
msgid "List of zeros."
msgstr ""

#: of transformers.BarthezTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieve sequence ids from a token list that has no special tokens added."
" This method is called when adding special tokens using the tokenizer "
"``prepare_for_model`` method."
msgstr ""

#: of transformers.BarthezTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model."
msgstr ""

#: of transformers.BarthezTokenizer.get_special_tokens_mask:11
msgid ""
"A list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of transformers.BarthezTokenizer.get_vocab:1
msgid "Returns the vocabulary as a dictionary of token to index."
msgstr ""

#: of transformers.BarthezTokenizer.get_vocab:3
msgid ""
":obj:`tokenizer.get_vocab()[token]` is equivalent to "
":obj:`tokenizer.convert_tokens_to_ids(token)` when :obj:`token` is in the"
" vocab."
msgstr ""

#: of transformers.BarthezTokenizer.get_vocab:6
msgid "The vocabulary."
msgstr ""

#: of transformers.BarthezTokenizer.get_vocab:7
msgid ":obj:`Dict[str, int]`"
msgstr ""

#: of transformers.BarthezTokenizer.save_vocabulary:1
#: transformers.BarthezTokenizerFast.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.BarthezTokenizer.save_vocabulary:3
#: transformers.BarthezTokenizerFast.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.BarthezTokenizer.save_vocabulary:6
#: transformers.BarthezTokenizerFast.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.BarthezTokenizer.save_vocabulary:8
#: transformers.BarthezTokenizerFast.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.BarthezTokenizer.save_vocabulary:11
#: transformers.BarthezTokenizerFast.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.BarthezTokenizer.save_vocabulary:12
#: transformers.BarthezTokenizerFast.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: of transformers.BarthezTokenizer.vocab_size:1
msgid "Size of the base vocabulary (without the added tokens)."
msgstr ""

#: of transformers.BarthezTokenizer.vocab_size:3
msgid ":obj:`int`"
msgstr ""

#: ../../source/model_doc/barthez.rst:57
msgid "BarthezTokenizerFast"
msgstr ""

#: of transformers.BarthezTokenizerFast:1
msgid ""
"Adapted from :class:`~transformers.CamembertTokenizer` and "
":class:`~transformers.BartTokenizer`. Construct a \"fast\" BARThez "
"tokenizer. Based on `SentencePiece "
"<https://github.com/google/sentencepiece>`__."
msgstr ""

#: of transformers.BarthezTokenizerFast:4
msgid ""
"This tokenizer inherits from "
":class:`~transformers.PreTrainedTokenizerFast` which contains most of the"
" main methods. Users should refer to this superclass for more information"
" regarding those methods."
msgstr ""


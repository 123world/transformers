# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:56+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/speech_to_text.rst:14
msgid "Speech2Text"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:19
msgid ""
"The Speech2Text model was proposed in `fairseq S2T: Fast Speech-to-Text "
"Modeling with fairseq <https://arxiv.org/abs/2010.05171>`__ by Changhan "
"Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It's a "
"transformer-based seq2seq (encoder-decoder) model designed for end-to-end"
" Automatic Speech Recognition (ASR) and Speech Translation (ST). It uses "
"a convolutional downsampler to reduce the length of speech inputs by "
"3/4th before they are fed into the encoder. The model is trained with "
"standard autoregressive cross-entropy loss and generates the "
"transcripts/translations autoregressively. Speech2Text has been fine-"
"tuned on several datasets for ASR and ST: `LibriSpeech "
"<http://www.openslr.org/12>`__, `CoVoST 2 "
"<https://github.com/facebookresearch/covost>`__, `MuST-C "
"<https://ict.fbk.eu/must-c/>`__."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:28
msgid ""
"This model was contributed by `valhalla "
"<https://huggingface.co/valhalla>`__. The original code can be found "
"`here "
"<https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text>`__."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:33
msgid "Inference"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:35
msgid ""
"Speech2Text is a speech model that accepts a float tensor of log-mel "
"filter-bank features extracted from the speech signal. It's a "
"transformer-based seq2seq model, so the transcripts/translations are "
"generated autoregressively. The :obj:`generate()` method can be used for "
"inference."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:39
msgid ""
"The :class:`~transformers.Speech2TextFeatureExtractor` class is "
"responsible for extracting the log-mel filter-bank features. The "
":class:`~transformers.Speech2TextProcessor` wraps "
":class:`~transformers.Speech2TextFeatureExtractor` and "
":class:`~transformers.Speech2TextTokenizer` into a single instance to "
"both extract the input features and decode the predicted token ids."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:44
msgid ""
"The feature extractor depends on :obj:`torchaudio` and the tokenizer "
"depends on :obj:`sentencepiece` so be sure to install those packages "
"before running the examples. You could either install those as extra "
"speech dependancies with ``pip install transformers\"[speech, "
"sentencepiece]\"`` or install the packages seperatly with ``pip install "
"torchaudio sentencepiece``. Also ``torchaudio`` requires the development "
"version of the `libsndfile <http://www.mega-nerd.com/libsndfile/>`__ "
"package which can be installed via a system package manager. On Ubuntu it"
" can be installed as follows: ``apt install libsndfile1-dev``"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:52
msgid "ASR and Speech Translation"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:78
msgid "Multilingual speech translation"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:80
msgid ""
"For multilingual speech translation models, :obj:`eos_token_id` is used "
"as the :obj:`decoder_start_token_id` and the target language id is forced"
" as the first generated token. To force the target language id as the "
"first generated token, pass the :obj:`forced_bos_token_id` parameter to "
"the :obj:`generate()` method. The following example shows how to transate"
" English speech to French text using the `facebook/s2t-medium-mustc-"
"multilingual-st` checkpoint."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:110
msgid ""
"See the `model hub "
"<https://huggingface.co/models?filter=speech_to_text>`__ to look for "
"Speech2Text checkpoints."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:114
msgid "Speech2TextConfig"
msgstr ""

#: of transformers.Speech2TextConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.Speech2TextModel`. It is used to instantiate an "
"Speech2Text model according to the specified arguments, defining the "
"model architecture. Instantiating a configuration with the defaults will "
"yield a similar configuration to that of the Speech2Text `facebook/s2t-"
"small-librispeech-asr <https://huggingface.co/facebook/s2t-small-"
"librispeech-asr>`__ architecture."
msgstr ""

#: of transformers.Speech2TextConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.Speech2TextConfig transformers.Speech2TextFeatureExtractor
#: transformers.Speech2TextFeatureExtractor.__call__
#: transformers.Speech2TextForConditionalGeneration
#: transformers.Speech2TextForConditionalGeneration.forward
#: transformers.Speech2TextModel transformers.Speech2TextModel.forward
#: transformers.Speech2TextProcessor
#: transformers.Speech2TextProcessor.from_pretrained
#: transformers.Speech2TextProcessor.save_pretrained
#: transformers.Speech2TextTokenizer
#: transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences
#: transformers.Speech2TextTokenizer.get_special_tokens_mask
#: transformers.Speech2TextTokenizer.save_vocabulary
msgid "Parameters"
msgstr ""

#: of transformers.Speech2TextConfig:10
msgid ""
"Vocabulary size of the Speech2Text model. Defines the number of different"
" tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.Speech2TextModel`"
msgstr ""

#: of transformers.Speech2TextConfig:13
msgid "Dimensionality of the layers and the pooler layer."
msgstr ""

#: of transformers.Speech2TextConfig:15
msgid "Number of encoder layers."
msgstr ""

#: of transformers.Speech2TextConfig:17
msgid "Number of decoder layers."
msgstr ""

#: of transformers.Speech2TextConfig:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.Speech2TextConfig:21
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"decoder."
msgstr ""

#: of transformers.Speech2TextConfig:23 transformers.Speech2TextConfig:25
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in decoder."
msgstr ""

#: of transformers.Speech2TextConfig:27
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.Speech2TextConfig:30
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.Speech2TextConfig:32
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.Speech2TextConfig:34
msgid "The dropout ratio for activations inside the fully connected layer."
msgstr ""

#: of transformers.Speech2TextConfig:36
msgid "The dropout ratio for classifier."
msgstr ""

#: of transformers.Speech2TextConfig:38
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.Speech2TextConfig:40
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the encoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.Speech2TextConfig:43
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the decoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.Speech2TextConfig:46
msgid ""
"Whether or not the model should return the last key/values attentions "
"(not used by all models)."
msgstr ""

#: of transformers.Speech2TextConfig:48
msgid ""
"The maximum sequence length of log-mel filter-bank features that this "
"model might ever be used with."
msgstr ""

#: of transformers.Speech2TextConfig:50
msgid ""
"(:obj:`int`, `optional`, defaults to 1024): The maximum sequence length "
"that this model might ever be used with. Typically set this to something "
"large just in case (e.g., 512 or 1024 or 2048)."
msgstr ""

#: of transformers.Speech2TextConfig:53
msgid "Number of 1D convolutional layers in the conv module."
msgstr ""

#: of transformers.Speech2TextConfig:55
msgid ""
"A tuple of integers defining the kernel size of each 1D convolutional "
"layer in the conv module. The length of :obj:`conv_kernel_sizes` has to "
"match :obj:`num_conv_layers`."
msgstr ""

#: of transformers.Speech2TextConfig:58
msgid ""
"An integer defining the number of output channels of each convolution "
"layers except the final one in the conv module."
msgstr ""

#: of transformers.Speech2TextConfig:61
msgid ""
"An integer specifying the size of feature vector. This is also the "
"dimensions of log-mel filter-bank features."
msgstr ""

#: of transformers.Speech2TextConfig:64
msgid ""
"An integer specifying number of input channels of the input feature "
"vector."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:121
msgid "Speech2TextTokenizer"
msgstr ""

#: of transformers.Speech2TextTokenizer:1
msgid "Construct an Speech2Text tokenizer."
msgstr ""

#: of transformers.Speech2TextTokenizer:3
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains some of the main methods. Users should refer to the "
"superclass for more information regarding such methods."
msgstr ""

#: of transformers.Speech2TextTokenizer:6
msgid "File containing the vocabulary."
msgstr ""

#: of transformers.Speech2TextTokenizer:8
msgid ""
"Path to the `SentencePiece <https://github.com/google/sentencepiece>`__ "
"model file"
msgstr ""

#: of transformers.Speech2TextTokenizer:10
msgid "The beginning of sentence token."
msgstr ""

#: of transformers.Speech2TextTokenizer:12
msgid "The end of sentence token."
msgstr ""

#: of transformers.Speech2TextTokenizer:14
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.Speech2TextTokenizer:17
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.Speech2TextTokenizer:19
msgid "Whether or not to uppercase the output when decoding."
msgstr ""

#: of transformers.Speech2TextTokenizer:21
msgid "Whether or not to lowercase the input when tokenizing."
msgstr ""

#: of transformers.Speech2TextTokenizer:23
msgid "A string representing the target language."
msgstr ""

#: of transformers.Speech2TextTokenizer:25
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:  - ``enable_sampling``: Enable subword "
"regularization. - ``nbest_size``: Sampling parameters for unigram. "
"Invalid for BPE-Dropout.    - ``nbest_size = {0,1}``: No sampling is "
"performed.   - ``nbest_size > 1``: samples from the nbest_size results."
"   - ``nbest_size < 0``: assuming that nbest_size is infinite and samples"
" from the all hypothesis (lattice)     using forward-filtering-and-"
"backward-sampling algorithm.  - ``alpha``: Smoothing parameter for "
"unigram sampling, and dropout probability of merge operations for   BPE-"
"dropout."
msgstr ""

#: of transformers.Speech2TextTokenizer:25
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:"
msgstr ""

#: of transformers.Speech2TextTokenizer:28
msgid "``enable_sampling``: Enable subword regularization."
msgstr ""

#: of transformers.Speech2TextTokenizer:29
msgid "``nbest_size``: Sampling parameters for unigram. Invalid for BPE-Dropout."
msgstr ""

#: of transformers.Speech2TextTokenizer:31
msgid "``nbest_size = {0,1}``: No sampling is performed."
msgstr ""

#: of transformers.Speech2TextTokenizer:32
msgid "``nbest_size > 1``: samples from the nbest_size results."
msgstr ""

#: of transformers.Speech2TextTokenizer:33
msgid ""
"``nbest_size < 0``: assuming that nbest_size is infinite and samples from"
" the all hypothesis (lattice) using forward-filtering-and-backward-"
"sampling algorithm."
msgstr ""

#: of transformers.Speech2TextTokenizer:36
msgid ""
"``alpha``: Smoothing parameter for unigram sampling, and dropout "
"probability of merge operations for BPE-dropout."
msgstr ""

#: of transformers.Speech2TextTokenizer:39
msgid ""
"Additional keyword arguments passed along to "
":class:`~transformers.PreTrainedTokenizer`"
msgstr ""

#: of transformers.Speech2TextTokenizer.build_inputs_with_special_tokens:1
msgid "Build model inputs from a sequence by appending eos_token_id."
msgstr ""

#: of transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences:1
msgid ""
"Create the token type IDs corresponding to the sequences passed. `What "
"are token type IDs? <../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences:4
msgid ""
"Should be overridden in a subclass if the model has a special way of "
"building those."
msgstr ""

#: of transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences:6
msgid "The first tokenized sequence."
msgstr ""

#: of transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences:8
msgid "The second tokenized sequence."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward
#: transformers.Speech2TextModel.forward
#: transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences
#: transformers.Speech2TextTokenizer.get_special_tokens_mask
#: transformers.Speech2TextTokenizer.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences:11
msgid "The token type ids."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward
#: transformers.Speech2TextModel.forward
#: transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences
#: transformers.Speech2TextTokenizer.get_special_tokens_mask
#: transformers.Speech2TextTokenizer.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.Speech2TextTokenizer.create_token_type_ids_from_sequences:12
#: transformers.Speech2TextTokenizer.get_special_tokens_mask:12
msgid ":obj:`List[int]`"
msgstr ""

#: of transformers.Speech2TextTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieve sequence ids from a token list that has no special tokens added."
" This method is called when adding special tokens using the tokenizer "
"``prepare_for_model`` method."
msgstr ""

#: of transformers.Speech2TextTokenizer.get_special_tokens_mask:4
msgid "List of IDs."
msgstr ""

#: of transformers.Speech2TextTokenizer.get_special_tokens_mask:6
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of transformers.Speech2TextTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model."
msgstr ""

#: of transformers.Speech2TextTokenizer.get_special_tokens_mask:11
msgid ""
"A list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of transformers.Speech2TextTokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.Speech2TextTokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.Speech2TextTokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.Speech2TextTokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.Speech2TextTokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.Speech2TextTokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:129
msgid "Speech2TextFeatureExtractor"
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:1
msgid "Constructs a Speech2Text feature extractor."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:3
msgid ""
"This feature extractor inherits from "
":class:`~transformers.Speech2TextFeatureExtractor` which contains most of"
" the main methods. Users should refer to this superclass for more "
"information regarding those methods."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:6
msgid ""
"This class extracts mel-filter bank features from raw speech using "
"TorchAudio and applies utterance-level cepstral mean and variance "
"normalization to the extracted features."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:9
msgid "The feature dimension of the extracted features."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:11
msgid ""
"The sampling rate at which the audio files should be digitalized "
"expressed in Hertz per second (Hz)."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:13
msgid "Number of Mel-frequency bins."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:15
msgid "The value that is used to fill the padding vectors."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:17
msgid ""
"Whether or not to apply utterance-level cepstral mean and variance "
"normalization to extracted features."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:19
msgid "Whether or not to zero-mean normalize the extracted features."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor:21
msgid "Whether or not to unit-variance normalize the extracted features."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:1
msgid ""
"Main method to featurize and prepare for the model one or several "
"sequence(s). sequences."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:3
msgid ""
"The sequence or batch of sequences to be padded. Each sequence can be a "
"numpy array, a list of float values, a list of numpy arrays or a list of "
"list of float values."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:6
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:  * :obj:`True` or "
":obj:`'longest'`: Pad to the longest sequence in the batch (or no padding"
" if only a   single sequence if provided). * :obj:`'max_length'`: Pad to "
"a maximum length specified with the argument :obj:`max_length` or to the"
"   maximum acceptable input length for the model if that argument is not "
"provided. * :obj:`False` or :obj:`'do_not_pad'` (default): No padding "
"(i.e., can output a batch with sequences of   different lengths)."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:6
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:"
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:9
msgid ""
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single sequence if provided)."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:11
msgid ""
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the maximum acceptable input length for the model"
" if that argument is not provided."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:13
msgid ""
":obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can "
"output a batch with sequences of different lengths)."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:16
msgid ""
"Maximum length of the returned list and optionally padding length (see "
"above)."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:18
msgid ""
"Activates truncation to cut input sequences longer than `max_length` to "
"`max_length`."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:20
msgid ""
"If set will pad the sequence to a multiple of the provided value.  This "
"is especially useful to enable the use of Tensor Cores on NVIDIA hardware"
" with compute capability >= 7.5 (Volta), or on TPUs which benefit from "
"having sequence lengths be a multiple of 128."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:20
msgid "If set will pad the sequence to a multiple of the provided value."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:22
msgid ""
"This is especially useful to enable the use of Tensor Cores on NVIDIA "
"hardware with compute capability >= 7.5 (Volta), or on TPUs which benefit"
" from having sequence lengths be a multiple of 128."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:25
msgid ""
"Whether to return the attention mask. If left to the default, will return"
" the attention mask according to the specific feature_extractor's "
"default.  `What are attention masks? <../glossary.html#attention-mask>`__"
"  .. note::      For Speech2TextTransoformer models, "
":obj:`attention_mask` should alwys be passed for batched     inference, "
"to avoid subtle bugs."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:25
msgid ""
"Whether to return the attention mask. If left to the default, will return"
" the attention mask according to the specific feature_extractor's "
"default."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:28
#: transformers.Speech2TextForConditionalGeneration.forward:21
#: transformers.Speech2TextModel.forward:21
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:32
msgid ""
"For Speech2TextTransoformer models, :obj:`attention_mask` should alwys be"
" passed for batched inference, to avoid subtle bugs."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:35
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:  * :obj:`'tf'`: Return TensorFlow "
":obj:`tf.constant` objects. * :obj:`'pt'`: Return PyTorch "
":obj:`torch.Tensor` objects. * :obj:`'np'`: Return Numpy "
":obj:`np.ndarray` objects."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:35
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:"
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:37
msgid ":obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:38
msgid ":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:39
msgid ":obj:`'np'`: Return Numpy :obj:`np.ndarray` objects."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:41
msgid ""
"The sampling rate at which the :obj:`raw_speech` input was sampled. It is"
" strongly recommended to pass :obj:`sampling_rate` at the forward call to"
" prevent silent errors."
msgstr ""

#: of transformers.Speech2TextFeatureExtractor.__call__:44
msgid "The value that is used to fill the padding values / vectors."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:136
msgid "Speech2TextProcessor"
msgstr ""

#: of transformers.Speech2TextProcessor:1
msgid ""
"Constructs a Speech2Text processor which wraps a Speech2Text feature "
"extractor and a Speech2Text tokenizer into a single processor."
msgstr ""

#: of transformers.Speech2TextProcessor:4
msgid ""
":class:`~transformers.Speech2TextProcessor` offers all the "
"functionalities of :class:`~transformers.Speech2TextFeatureExtractor` and"
" :class:`~transformers.Speech2TextTokenizer`. See the "
":meth:`~transformers.Speech2TextProcessor.__call__` and "
":meth:`~transformers.Speech2TextProcessor.decode` for more information."
msgstr ""

#: of transformers.Speech2TextProcessor:9
msgid ""
"An instance of :class:`~transformers.Speech2TextFeatureExtractor`. The "
"feature extractor is a required input."
msgstr ""

#: of transformers.Speech2TextProcessor:12
msgid ""
"An instance of :class:`~transformers.Speech2TextTokenizer`. The tokenizer"
" is a required input."
msgstr ""

#: of transformers.Speech2TextProcessor.__call__:1
msgid ""
"When used in normal mode, this method forwards all its arguments to "
"Speech2TextFeatureExtractor's "
":meth:`~transformers.Speech2TextFeatureExtractor.__call__` and returns "
"its output. If used in the context "
":meth:`~transformers.Speech2TextProcessor.as_target_processor` this "
"method forwards all its arguments to Speech2TextTokenizer's "
":meth:`~transformers.Speech2TextTokenizer.__call__`. Please refer to the "
"doctsring of the above two methods for more information."
msgstr ""

#: of transformers.Speech2TextProcessor.as_target_processor:1
msgid ""
"Temporarily sets the tokenizer for processing the input. Useful for "
"encoding the labels when fine-tuning Speech2Text."
msgstr ""

#: of transformers.Speech2TextProcessor.batch_decode:1
msgid ""
"This method forwards all its arguments to Speech2TextTokenizer's "
":meth:`~transformers.PreTrainedTokenizer.batch_decode`. Please refer to "
"the docstring of this method for more information."
msgstr ""

#: of transformers.Speech2TextProcessor.decode:1
msgid ""
"This method forwards all its arguments to Speech2TextTokenizer's "
":meth:`~transformers.PreTrainedTokenizer.decode`. Please refer to the "
"docstring of this method for more information."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:1
msgid ""
"Instantiate a :class:`~transformers.Speech2TextProcessor` from a "
"pretrained Speech2Text processor."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:5
msgid ""
"This class method is simply calling Speech2TextFeatureExtractor's "
":meth:`~transformers.PreTrainedFeatureExtractor.from_pretrained` and "
"Speech2TextTokenizer's "
":meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained`."
" Please refer to the docstrings of the methods above for more "
"information."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:10
msgid ""
"This can be either:  - a string, the `model id` of a pretrained "
"feature_extractor hosted inside a model repo on   huggingface.co. Valid "
"model ids can be located at the root-level, like ``bert-base-uncased``, "
"or   namespaced under a user or organization name, like ``dbmdz/bert-"
"base-german-cased``. - a path to a `directory` containing a feature "
"extractor file saved using the   "
":meth:`~transformers.PreTrainedFeatureExtractor.save_pretrained` method, "
"e.g.,   ``./my_model_directory/``. - a path or url to a saved feature "
"extractor JSON `file`, e.g.,   "
"``./my_model_directory/preprocessor_config.json``."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:10
msgid "This can be either:"
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:12
msgid ""
"a string, the `model id` of a pretrained feature_extractor hosted inside "
"a model repo on huggingface.co. Valid model ids can be located at the "
"root-level, like ``bert-base-uncased``, or namespaced under a user or "
"organization name, like ``dbmdz/bert-base-german-cased``."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:15
msgid ""
"a path to a `directory` containing a feature extractor file saved using "
"the :meth:`~transformers.PreTrainedFeatureExtractor.save_pretrained` "
"method, e.g., ``./my_model_directory/``."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:18
msgid ""
"a path or url to a saved feature extractor JSON `file`, e.g., "
"``./my_model_directory/preprocessor_config.json``."
msgstr ""

#: of transformers.Speech2TextProcessor.from_pretrained:21
msgid ""
"Additional keyword arguments passed along to both "
":class:`~transformers.PreTrainedFeatureExtractor` and "
":class:`~transformers.PreTrainedTokenizer`"
msgstr ""

#: of transformers.Speech2TextProcessor.save_pretrained:1
msgid ""
"Save a Speech2Text feature extractor object and Speech2Text tokenizer "
"object to the directory ``save_directory``, so that it can be re-loaded "
"using the :func:`~transformers.Speech2TextProcessor.from_pretrained` "
"class method."
msgstr ""

#: of transformers.Speech2TextProcessor.save_pretrained:7
msgid ""
"This class method is simply calling "
":meth:`~transformers.PreTrainedFeatureExtractor.save_pretrained` and "
":meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.save_pretrained`."
" Please refer to the docstrings of the methods above for more "
"information."
msgstr ""

#: of transformers.Speech2TextProcessor.save_pretrained:11
msgid ""
"Directory where the feature extractor JSON file and the tokenizer files "
"will be saved (directory will be created if it does not exist)."
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:143
msgid "Speech2TextModel"
msgstr ""

#: of transformers.Speech2TextModel:1
msgid ""
"The bare Speech2Text Model outputting raw hidden-states without any "
"specific head on top. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration:6
#: transformers.Speech2TextModel:6
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration:10
#: transformers.Speech2TextModel:10
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.Speech2TextModel.forward:1
msgid ""
"The :class:`~transformers.Speech2TextModel` forward method, overrides the"
" :func:`__call__` special method."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:4
#: transformers.Speech2TextModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:8
#: transformers.Speech2TextModel.forward:8
msgid ""
"Float values of fbank features extracted from the raw speech waveform. "
"Raw speech waveform can be obtained by loading a ``.flac`` or ``.wav`` "
"audio file into an array of type :obj:`List[float]` or a "
":obj:`numpy.ndarray`, *e.g.* via the soundfile library (``pip install "
"soundfile``). To prepare the array into :obj:`input_features`, the "
":class:`~transformers.Speech2TextTokenizer` should be used for extracting"
" the fbank features, padding and conversion into a tensor of type "
":obj:`torch.FloatTensor`. See "
":meth:`~transformers.Speech2TextTokenizer.__call__`"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:15
#: transformers.Speech2TextModel.forward:15
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:  - 1 for tokens that are "
"**not masked**, - 0 for tokens that are **masked**.  `What are attention "
"masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:15
#: transformers.Speech2TextModel.forward:15
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:18
#: transformers.Speech2TextModel.forward:18
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:19
#: transformers.Speech2TextModel.forward:19
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:23
#: transformers.Speech2TextModel.forward:23
msgid ""
"Indices of decoder input sequence tokens in the vocabulary.  Indices can "
"be obtained using :class:`~transformers.SpeechToTextTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" decoder input IDs? <../glossary.html#decoder-input-ids>`__  SpeechToText"
" uses the :obj:`eos_token_id` as the starting token for "
":obj:`decoder_input_ids` generation. If :obj:`past_key_values` is used, "
"optionally only the last :obj:`decoder_input_ids` have to be input (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:23
#: transformers.Speech2TextModel.forward:23
msgid "Indices of decoder input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:25
#: transformers.Speech2TextModel.forward:25
msgid ""
"Indices can be obtained using "
":class:`~transformers.SpeechToTextTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:29
#: transformers.Speech2TextModel.forward:29
msgid "`What are decoder input IDs? <../glossary.html#decoder-input-ids>`__"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:31
#: transformers.Speech2TextModel.forward:31
msgid ""
"SpeechToText uses the :obj:`eos_token_id` as the starting token for "
":obj:`decoder_input_ids` generation. If :obj:`past_key_values` is used, "
"optionally only the last :obj:`decoder_input_ids` have to be input (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:35
#: transformers.Speech2TextModel.forward:35
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default. "
"<<<<<<< HEAD  If you want to change padding behavior, you should read "
":func:`modeling_speech_to_text._prepare_decoder_inputs` and modify to "
"your needs. See diagram 1 in `the paper "
"<https://arxiv.org/abs/1910.13461>`__ for more information on the default"
" strategy."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:35
#: transformers.Speech2TextModel.forward:35
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default. "
"<<<<<<< HEAD"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:38
#: transformers.Speech2TextModel.forward:38
msgid ""
"If you want to change padding behavior, you should read "
":func:`modeling_speech_to_text._prepare_decoder_inputs` and modify to "
"your needs. See diagram 1 in `the paper "
"<https://arxiv.org/abs/1910.13461>`__ for more information on the default"
" strategy."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:42
#: transformers.Speech2TextModel.forward:42
msgid ""
"Mask to nullify selected heads of the attention modules in the encoder. "
"Mask values selected in ``[0, 1]``:  - 1 indicates the head is **not "
"masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:42
#: transformers.Speech2TextModel.forward:42
msgid ""
"Mask to nullify selected heads of the attention modules in the encoder. "
"Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:44
#: transformers.Speech2TextForConditionalGeneration.forward:49
#: transformers.Speech2TextForConditionalGeneration.forward:54
#: transformers.Speech2TextModel.forward:44
#: transformers.Speech2TextModel.forward:49
#: transformers.Speech2TextModel.forward:54
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:45
#: transformers.Speech2TextForConditionalGeneration.forward:50
#: transformers.Speech2TextForConditionalGeneration.forward:55
#: transformers.Speech2TextModel.forward:45
#: transformers.Speech2TextModel.forward:50
#: transformers.Speech2TextModel.forward:55
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:47
#: transformers.Speech2TextModel.forward:47
msgid ""
"Mask to nullify selected heads of the attention modules in the decoder. "
"Mask values selected in ``[0, 1]``:  - 1 indicates the head is **not "
"masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:47
#: transformers.Speech2TextModel.forward:47
msgid ""
"Mask to nullify selected heads of the attention modules in the decoder. "
"Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:52
#: transformers.Speech2TextModel.forward:52
msgid ""
"Mask to nullify selected heads of the cross-attention modules. Mask "
"values selected in ``[0, 1]``:  - 1 indicates the head is **not masked**,"
" - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:52
#: transformers.Speech2TextModel.forward:52
msgid ""
"Mask to nullify selected heads of the cross-attention modules. Mask "
"values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:57
#: transformers.Speech2TextModel.forward:57
msgid ""
"Tuple consists of (:obj:`last_hidden_state`, `optional`: "
":obj:`hidden_states`, `optional`: :obj:`attentions`) "
":obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) is a sequence of hidden-states at the output "
"of the last layer of the encoder. Used in the cross-attention of the "
"decoder."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:62
#: transformers.Speech2TextModel.forward:62
msgid ""
"Tuple of :obj:`tuple(torch.FloatTensor)` of length "
":obj:`config.n_layers`, with each tuple having 2 tensors of shape "
":obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and"
" 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`.  Contains pre-computed "
"hidden-states (key and values in the self-attention blocks and in the "
"cross-attention blocks) that can be used (see :obj:`past_key_values` "
"input) to speed up sequential decoding.  If :obj:`past_key_values` are "
"used, the user can optionally input only the last "
":obj:`decoder_input_ids` (those that don't have their past key value "
"states given to this model) of shape :obj:`(batch_size, 1)` instead of "
"all :obj:`decoder_input_ids`` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:62
#: transformers.Speech2TextModel.forward:62
msgid ""
"Tuple of :obj:`tuple(torch.FloatTensor)` of length "
":obj:`config.n_layers`, with each tuple having 2 tensors of shape "
":obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and"
" 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:66
#: transformers.Speech2TextForConditionalGeneration.forward:107
#: transformers.Speech2TextModel.forward:66
#: transformers.Speech2TextModel.forward:105
msgid ""
"Contains pre-computed hidden-states (key and values in the self-attention"
" blocks and in the cross-attention blocks) that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:69
#: transformers.Speech2TextModel.forward:69
msgid ""
"If :obj:`past_key_values` are used, the user can optionally input only "
"the last :obj:`decoder_input_ids` (those that don't have their past key "
"value states given to this model) of shape :obj:`(batch_size, 1)` instead"
" of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:73
#: transformers.Speech2TextModel.forward:73
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. If :obj:`past_key_values` is "
"used, optionally only the last :obj:`decoder_inputs_embeds` have to be "
"input (see :obj:`past_key_values`). This is useful if you want more "
"control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix.  If"
" :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both "
"unset, :obj:`decoder_inputs_embeds` takes the value of "
":obj:`inputs_embeds`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:73
#: transformers.Speech2TextModel.forward:73
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. If :obj:`past_key_values` is "
"used, optionally only the last :obj:`decoder_inputs_embeds` have to be "
"input (see :obj:`past_key_values`). This is useful if you want more "
"control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:78
#: transformers.Speech2TextModel.forward:78
msgid ""
"If :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both "
"unset, :obj:`decoder_inputs_embeds` takes the value of "
":obj:`inputs_embeds`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:81
#: transformers.Speech2TextModel.forward:81
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:84
#: transformers.Speech2TextModel.forward:84
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:87
#: transformers.Speech2TextModel.forward:87
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:90
#: transformers.Speech2TextModel.forward:90
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.Speech2TextModel.forward:93
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or a tuple "
"of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Speech2TextConfig`) and inputs."
"  - **last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model.    If"
" :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size,   1, hidden_size)` is output. - "
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads."
msgstr ""

#: of transformers.Speech2TextModel.forward:93
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or a tuple "
"of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Speech2TextConfig`) and inputs."
msgstr ""

#: of transformers.Speech2TextModel.forward:97
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model."
msgstr ""

#: of transformers.Speech2TextModel.forward:99
msgid ""
"If :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size, 1, hidden_size)` is output."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:103
#: transformers.Speech2TextModel.forward:101
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors of shape"
" :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) "
"and 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:109
#: transformers.Speech2TextModel.forward:107
msgid ""
"**decoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:112
#: transformers.Speech2TextModel.forward:110
msgid ""
"Hidden-states of the decoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:113
#: transformers.Speech2TextModel.forward:111
msgid ""
"**decoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:116
#: transformers.Speech2TextModel.forward:114
msgid ""
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:118
#: transformers.Speech2TextModel.forward:116
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:121
#: transformers.Speech2TextModel.forward:119
msgid ""
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:123
#: transformers.Speech2TextModel.forward:121
msgid ""
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:124
#: transformers.Speech2TextModel.forward:122
msgid ""
"**encoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:127
#: transformers.Speech2TextModel.forward:125
msgid ""
"Hidden-states of the encoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:128
#: transformers.Speech2TextModel.forward:126
msgid ""
"**encoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:131
#: transformers.Speech2TextModel.forward:129
msgid ""
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.Speech2TextModel.forward:131
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:135
#: transformers.Speech2TextModel.forward:133
msgid "Example::"
msgstr ""

#: ../../source/model_doc/speech_to_text.rst:150
msgid "Speech2TextForConditionalGeneration"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration:1
msgid ""
"The Speech2Text Model with a language modeling head. Can be used for "
"summarization. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:1
msgid ""
"The :class:`~transformers.Speech2TextForConditionalGeneration` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:92
msgid ""
"Labels for computing the language modeling loss. Indices should either be"
" in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` "
"docstring). Tokens with indices set to ``-100`` are ignored (masked), the"
" loss is only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:97
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Speech2TextConfig`) and inputs."
"  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss. - "
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
" - **past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads.   Example::      >>> import torch     >>> from "
"transformers import Speech2TextProcessor, "
"Speech2TextForConditionalGeneration     >>> from datasets import "
"load_dataset     >>> import soundfile as sf      >>> model = "
"Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-"
"librispeech-asr\")     >>> processor = "
"Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-"
"asr\")      >>> def map_to_array(batch):     >>>     speech, _ = "
"sf.read(batch[\"file\"])     >>>     batch[\"speech\"] = speech     >>>"
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_features = processor(ds[\"speech\"][0], sampling_rate=16000, "
"return_tensors=\"pt\").input_features  # Batch size 1     >>> "
"generated_ids = model.generate(input_ids=input_features)      >>> "
"transcription = processor.batch_decode(generated_ids)"
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:97
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Speech2TextConfig`) and inputs."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:101
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:102
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.Speech2TextForConditionalGeneration.forward:157
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""


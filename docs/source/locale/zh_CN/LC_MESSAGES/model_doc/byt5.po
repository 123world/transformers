# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/byt5.rst:14
msgid "ByT5"
msgstr ""

#: ../../source/model_doc/byt5.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/byt5.rst:19
msgid ""
"The ByT5 model was presented in `ByT5: Towards a token-free future with "
"pre-trained byte-to-byte models <https://arxiv.org/abs/2105.13626>`_ by "
"Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, "
"Mihir Kale, Adam Roberts, Colin Raffel."
msgstr ""

#: ../../source/model_doc/byt5.rst:23
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/byt5.rst:25
msgid ""
"*Most widely-used pre-trained language models operate on sequences of "
"tokens corresponding to word or subword units. Encoding text as a "
"sequence of tokens requires a tokenizer, which is typically created as an"
" independent artifact from the model. Token-free models that instead "
"operate directly on raw text (bytes or characters) have many benefits: "
"they can process text in any language out of the box, they are more "
"robust to noise, and they minimize technical debt by removing complex and"
" error-prone text preprocessing pipelines. Since byte or character "
"sequences are longer than token sequences, past work on token-free models"
" has often introduced new model architectures designed to amortize the "
"cost of operating directly on raw text. In this paper, we show that a "
"standard Transformer architecture can be used with minimal modifications "
"to process byte sequences. We carefully characterize the trade-offs in "
"terms of parameter count, training FLOPs, and inference speed, and show "
"that byte-level models are competitive with their token-level "
"counterparts. We also demonstrate that byte-level models are "
"significantly more robust to noise and perform better on tasks that are "
"sensitive to spelling and pronunciation. As part of our contribution, we "
"release a new set of pre-trained byte-level Transformer models based on "
"the T5 architecture, as well as all code and data used in our "
"experiments.*"
msgstr ""

#: ../../source/model_doc/byt5.rst:39
msgid ""
"This model was contributed by `patrickvonplaten "
"<https://huggingface.co/patrickvonplaten>`__. The original code can be "
"found `here <https://github.com/google-research/byt5>`__."
msgstr ""

#: ../../source/model_doc/byt5.rst:43
msgid ""
"ByT5's architecture is based on the T5 model, so one can refer to "
":doc:`T5's documentation page <t5>`."
msgstr ""

#: ../../source/model_doc/byt5.rst:47
msgid "Example"
msgstr ""

#: ../../source/model_doc/byt5.rst:49
msgid "ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:"
msgstr ""

#: ../../source/model_doc/byt5.rst:64
msgid ""
"For batched inference and training it is however recommended to make use "
"of the tokenizer:"
msgstr ""

#: ../../source/model_doc/byt5.rst:79
msgid "ByT5Tokenizer"
msgstr ""

#: of transformers.ByT5Tokenizer:1
msgid "Construct a ByT5 tokenizer. ByT5 simply uses raw bytes utf-8 encoding."
msgstr ""

#: of transformers.ByT5Tokenizer:3
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods."
msgstr ""

#: of transformers.ByT5Tokenizer
msgid "Parameters"
msgstr ""

#: of transformers.ByT5Tokenizer:6
msgid ""
"The end of sequence token.  .. note::      When building a sequence using"
" special tokens, this is not the token that is used for the end of     "
"sequence. The token used is the :obj:`sep_token`."
msgstr ""

#: of transformers.ByT5Tokenizer:6
msgid "The end of sequence token."
msgstr ""

#: of transformers.ByT5Tokenizer:10
msgid ""
"When building a sequence using special tokens, this is not the token that"
" is used for the end of sequence. The token used is the :obj:`sep_token`."
msgstr ""

#: of transformers.ByT5Tokenizer:13
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.ByT5Tokenizer:16
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.ByT5Tokenizer:18
#, python-format
msgid ""
"Add a number of extra ids added to the end of the vocabulary for use as "
"sentinels. These tokens are accessible as \"<extra_id_{%d}>\" where "
"\"{%d}\" is a number between 0 and extra_ids-1. Extra tokens are indexed "
"from the end of the vocabulary up to beginning (\"<extra_id_0>\" is the "
"last token in the vocabulary like in ByT5 preprocessing see `here "
"<https://github.com/google-research/text-to-text-transfer-"
"transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117>`__)."
msgstr ""

#: of transformers.ByT5Tokenizer:24
msgid "Additional special tokens used by the tokenizer."
msgstr ""

#: ../../source/model_doc/byt5.rst:83
msgid "See :class:`~transformers.ByT5Tokenizer` for all details."
msgstr ""


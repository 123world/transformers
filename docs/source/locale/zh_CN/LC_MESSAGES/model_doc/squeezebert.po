# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/squeezebert.rst:14
msgid "SqueezeBERT"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:19
msgid ""
"The SqueezeBERT model was proposed in `SqueezeBERT: What can computer "
"vision teach NLP about efficient neural networks? "
"<https://arxiv.org/abs/2006.11316>`__ by Forrest N. Iandola, Albert E. "
"Shaw, Ravi Krishna, Kurt W. Keutzer. It's a bidirectional transformer "
"similar to the BERT model. The key difference between the BERT "
"architecture and the SqueezeBERT architecture is that SqueezeBERT uses "
"`grouped convolutions <https://blog.yani.io/filter-group-tutorial>`__ "
"instead of fully-connected layers for the Q, K, V and FFN layers."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:25
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:27
msgid ""
"*Humans read and write hundreds of billions of messages every day. "
"Further, due to the availability of large datasets, large computing "
"systems, and better neural network models, natural language processing "
"(NLP) technology has made significant strides in understanding, "
"proofreading, and organizing these messages. Thus, there is a significant"
" opportunity to deploy NLP in myriad applications to help web users, "
"social networks, and businesses. In particular, we consider smartphones "
"and other mobile devices as crucial platforms for deploying NLP models at"
" scale. However, today's highly-accurate NLP neural network models such "
"as BERT and RoBERTa are extremely computationally expensive, with BERT-"
"base taking 1.7 seconds to classify a text snippet on a Pixel 3 "
"smartphone. In this work, we observe that methods such as grouped "
"convolutions have yielded significant speedups for computer vision "
"networks, but many of these techniques have not been adopted by NLP "
"neural network designers. We demonstrate how to replace several "
"operations in self-attention layers with grouped convolutions, and we use"
" this technique in a novel network architecture called SqueezeBERT, which"
" runs 4.3x faster than BERT-base on the Pixel 3 while achieving "
"competitive accuracy on the GLUE test set. The SqueezeBERT code will be "
"released.*"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:40
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:42
msgid ""
"SqueezeBERT is a model with absolute position embeddings so it's usually "
"advised to pad the inputs on the right rather than the left."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:44
msgid ""
"SqueezeBERT is similar to BERT and therefore relies on the masked "
"language modeling (MLM) objective. It is therefore efficient at "
"predicting masked tokens and at NLU in general, but is not optimal for "
"text generation. Models trained with a causal language modeling (CLM) "
"objective are better in that regard."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:47
msgid ""
"For best results when finetuning on sequence classification tasks, it is "
"recommended to start with the `squeezebert/squeezebert-mnli-headless` "
"checkpoint."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:50
msgid ""
"This model was contributed by `forresti "
"<https://huggingface.co/forresti>`__."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:54
msgid "SqueezeBertConfig"
msgstr ""

#: of transformers.SqueezeBertConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.SqueezeBertModel`. It is used to instantiate a "
"SqueezeBERT model according to the specified arguments, defining the "
"model architecture."
msgstr ""

#: of transformers.SqueezeBertConfig:4
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.SqueezeBertConfig transformers.SqueezeBertForMaskedLM
#: transformers.SqueezeBertForMaskedLM.forward
#: transformers.SqueezeBertForMultipleChoice
#: transformers.SqueezeBertForMultipleChoice.forward
#: transformers.SqueezeBertForQuestionAnswering
#: transformers.SqueezeBertForQuestionAnswering.forward
#: transformers.SqueezeBertForSequenceClassification
#: transformers.SqueezeBertForSequenceClassification.forward
#: transformers.SqueezeBertForTokenClassification
#: transformers.SqueezeBertForTokenClassification.forward
#: transformers.SqueezeBertModel transformers.SqueezeBertModel.forward
#: transformers.SqueezeBertModel.set_input_embeddings
#: transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens
#: transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences
#: transformers.SqueezeBertTokenizer.get_special_tokens_mask
#: transformers.SqueezeBertTokenizer.save_vocabulary
msgid "Parameters"
msgstr ""

#: of transformers.SqueezeBertConfig:8
msgid ""
"Vocabulary size of the SqueezeBERT model. Defines the number of different"
" tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.SqueezeBertModel`."
msgstr ""

#: of transformers.SqueezeBertConfig:11
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.SqueezeBertConfig:13
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.SqueezeBertConfig:15
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.SqueezeBertConfig:17
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in the Transformer encoder."
msgstr ""

#: of transformers.SqueezeBertConfig:19
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.SqueezeBertConfig:22
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.SqueezeBertConfig:24
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.SqueezeBertConfig:26
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.SqueezeBertConfig:29
msgid ""
"The vocabulary size of the :obj:`token_type_ids` passed when calling "
":class:`~transformers.BertModel` or :class:`~transformers.TFBertModel`."
msgstr ""

#: of transformers.SqueezeBertConfig:32
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.SqueezeBertConfig:36
msgid "The ID of the token in the word embedding to use as padding."
msgstr ""

#: of transformers.SqueezeBertConfig:38
msgid "The dimension of the word embedding vectors."
msgstr ""

#: of transformers.SqueezeBertConfig:40
msgid "The number of groups in Q layer."
msgstr ""

#: of transformers.SqueezeBertConfig:42
msgid "The number of groups in K layer."
msgstr ""

#: of transformers.SqueezeBertConfig:44
msgid "The number of groups in V layer."
msgstr ""

#: of transformers.SqueezeBertConfig:46
msgid "The number of groups in the first feed forward network layer."
msgstr ""

#: of transformers.SqueezeBertConfig:48
msgid "The number of groups in the second feed forward network layer."
msgstr ""

#: of transformers.SqueezeBertConfig:50
msgid "The number of groups in the third feed forward network layer."
msgstr ""

#: of transformers.SqueezeBertConfig:53
msgid "Examples::"
msgstr ""

#: of transformers.SqueezeBertConfig:66
msgid ""
"Attributes: pretrained_config_archive_map (Dict[str, str]): A dictionary "
"containing all the available pre-trained checkpoints."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:61
msgid "SqueezeBertTokenizer"
msgstr ""

#: of transformers.SqueezeBertTokenizer:1
msgid "Constructs a SqueezeBert tokenizer."
msgstr ""

#: of transformers.SqueezeBertTokenizer:3
msgid ""
":class:`~transformers.SqueezeBertTokenizer is identical to "
":class:`~transformers.BertTokenizer` and runs end-to-end tokenization: "
"punctuation splitting + wordpiece."
msgstr ""

#: of transformers.SqueezeBertTokenizer:6
msgid ""
"Refer to superclass :class:`~transformers.BertTokenizer` for usage "
"examples and documentation concerning parameters."
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens. A BERT "
"sequence has the following format:"
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:4
msgid "single sequence: ``[CLS] X [SEP]``"
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:5
msgid "pair of sequences: ``[CLS] A [SEP] B [SEP]``"
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:7
msgid "List of IDs to which the special tokens will be added."
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:9
#: transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences:13
#: transformers.SqueezeBertTokenizer.get_special_tokens_mask:6
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward
#: transformers.SqueezeBertForMaskedLM.get_output_embeddings
#: transformers.SqueezeBertForMultipleChoice.forward
#: transformers.SqueezeBertForQuestionAnswering.forward
#: transformers.SqueezeBertForSequenceClassification.forward
#: transformers.SqueezeBertForTokenClassification.forward
#: transformers.SqueezeBertModel.forward
#: transformers.SqueezeBertModel.get_input_embeddings
#: transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens
#: transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences
#: transformers.SqueezeBertTokenizer.get_special_tokens_mask
#: transformers.SqueezeBertTokenizer.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:12
msgid ""
"List of `input IDs <../glossary.html#input-ids>`__ with the appropriate "
"special tokens."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward
#: transformers.SqueezeBertForMaskedLM.get_output_embeddings
#: transformers.SqueezeBertForMultipleChoice.forward
#: transformers.SqueezeBertForQuestionAnswering.forward
#: transformers.SqueezeBertForSequenceClassification.forward
#: transformers.SqueezeBertForTokenClassification.forward
#: transformers.SqueezeBertModel.forward
#: transformers.SqueezeBertModel.get_input_embeddings
#: transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens
#: transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences
#: transformers.SqueezeBertTokenizer.get_special_tokens_mask
#: transformers.SqueezeBertTokenizer.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.SqueezeBertTokenizer.build_inputs_with_special_tokens:13
#: transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences:18
#: transformers.SqueezeBertTokenizer.get_special_tokens_mask:12
msgid ":obj:`List[int]`"
msgstr ""

#: of transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences:1
msgid ""
"Create a mask from the two sequences passed to be used in a sequence-pair"
" classification task. A BERT sequence pair mask has the following format:"
msgstr ""

#: of transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences:9
msgid ""
"If :obj:`token_ids_1` is :obj:`None`, this method only returns the first "
"portion of the mask (0s)."
msgstr ""

#: of transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences:11
#: transformers.SqueezeBertTokenizer.get_special_tokens_mask:4
msgid "List of IDs."
msgstr ""

#: of transformers.SqueezeBertTokenizer.create_token_type_ids_from_sequences:16
msgid ""
"List of `token type IDs <../glossary.html#token-type-ids>`_ according to "
"the given sequence(s)."
msgstr ""

#: of transformers.SqueezeBertTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieve sequence ids from a token list that has no special tokens added."
" This method is called when adding special tokens using the tokenizer "
"``prepare_for_model`` method."
msgstr ""

#: of transformers.SqueezeBertTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model."
msgstr ""

#: of transformers.SqueezeBertTokenizer.get_special_tokens_mask:11
msgid ""
"A list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of transformers.SqueezeBertTokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.SqueezeBertTokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.SqueezeBertTokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.SqueezeBertTokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.SqueezeBertTokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.SqueezeBertTokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:69
msgid "SqueezeBertTokenizerFast"
msgstr ""

#: of transformers.SqueezeBertTokenizerFast:1
msgid ""
"Constructs a \"Fast\" SqueezeBert tokenizer (backed by HuggingFace's "
"`tokenizers` library)."
msgstr ""

#: of transformers.SqueezeBertTokenizerFast:3
msgid ""
":class:`~transformers.SqueezeBertTokenizerFast` is identical to "
":class:`~transformers.BertTokenizerFast` and runs end-to-end "
"tokenization: punctuation splitting + wordpiece."
msgstr ""

#: of transformers.SqueezeBertTokenizerFast:6
msgid ""
"Refer to superclass :class:`~transformers.BertTokenizerFast` for usage "
"examples and documentation concerning parameters."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:76
msgid "SqueezeBertModel"
msgstr ""

#: of transformers.SqueezeBertModel:1
msgid ""
"The bare SqueezeBERT Model transformer outputting raw hidden-states "
"without any specific head on top."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:3
#: transformers.SqueezeBertForMultipleChoice:5
#: transformers.SqueezeBertForQuestionAnswering:5
#: transformers.SqueezeBertForSequenceClassification:5
#: transformers.SqueezeBertForTokenClassification:5
#: transformers.SqueezeBertModel:3
msgid ""
"The SqueezeBERT model was proposed in `SqueezeBERT: What can computer "
"vision teach NLP about efficient neural networks? "
"<https://arxiv.org/abs/2006.11316>`__ by Forrest N. Iandola, Albert E. "
"Shaw, Ravi Krishna, and Kurt W. Keutzer"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:7
#: transformers.SqueezeBertForMultipleChoice:9
#: transformers.SqueezeBertForQuestionAnswering:9
#: transformers.SqueezeBertForSequenceClassification:9
#: transformers.SqueezeBertForTokenClassification:9
#: transformers.SqueezeBertModel:7
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:11
#: transformers.SqueezeBertForMultipleChoice:13
#: transformers.SqueezeBertForQuestionAnswering:13
#: transformers.SqueezeBertForSequenceClassification:13
#: transformers.SqueezeBertForTokenClassification:13
#: transformers.SqueezeBertModel:11
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:15
#: transformers.SqueezeBertForMultipleChoice:17
#: transformers.SqueezeBertForQuestionAnswering:17
#: transformers.SqueezeBertForSequenceClassification:17
#: transformers.SqueezeBertForTokenClassification:17
#: transformers.SqueezeBertModel:15
msgid ""
"For best results finetuning SqueezeBERT on text classification tasks, it "
"is recommended to use the `squeezebert/squeezebert-mnli-headless` "
"checkpoint as a starting point."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:18
#: transformers.SqueezeBertForMultipleChoice:20
#: transformers.SqueezeBertForQuestionAnswering:20
#: transformers.SqueezeBertForSequenceClassification:20
#: transformers.SqueezeBertForTokenClassification:20
#: transformers.SqueezeBertModel:18
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:24
#: transformers.SqueezeBertForMultipleChoice:26
#: transformers.SqueezeBertForQuestionAnswering:26
#: transformers.SqueezeBertForSequenceClassification:26
#: transformers.SqueezeBertForTokenClassification:26
#: transformers.SqueezeBertModel:24
msgid "Hierarchy::"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:34
#: transformers.SqueezeBertForMultipleChoice:36
#: transformers.SqueezeBertForQuestionAnswering:36
#: transformers.SqueezeBertForSequenceClassification:36
#: transformers.SqueezeBertForTokenClassification:36
#: transformers.SqueezeBertModel:34
msgid "Data layouts::"
msgstr ""

#: of transformers.SqueezeBertModel.forward:1
msgid ""
"The :class:`~transformers.SqueezeBertModel` forward method, overrides the"
" :func:`__call__` special method."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:4
#: transformers.SqueezeBertForMultipleChoice.forward:4
#: transformers.SqueezeBertForQuestionAnswering.forward:4
#: transformers.SqueezeBertForSequenceClassification.forward:4
#: transformers.SqueezeBertForTokenClassification.forward:4
#: transformers.SqueezeBertModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:8
#: transformers.SqueezeBertForMultipleChoice.forward:8
#: transformers.SqueezeBertForQuestionAnswering.forward:8
#: transformers.SqueezeBertForSequenceClassification.forward:8
#: transformers.SqueezeBertForTokenClassification.forward:8
#: transformers.SqueezeBertModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.SqueezeBertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:8
#: transformers.SqueezeBertForMultipleChoice.forward:8
#: transformers.SqueezeBertForQuestionAnswering.forward:8
#: transformers.SqueezeBertForSequenceClassification.forward:8
#: transformers.SqueezeBertForTokenClassification.forward:8
#: transformers.SqueezeBertModel.forward:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:10
#: transformers.SqueezeBertForMultipleChoice.forward:10
#: transformers.SqueezeBertForQuestionAnswering.forward:10
#: transformers.SqueezeBertForSequenceClassification.forward:10
#: transformers.SqueezeBertForTokenClassification.forward:10
#: transformers.SqueezeBertModel.forward:10
msgid ""
"Indices can be obtained using "
":class:`~transformers.SqueezeBertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:14
#: transformers.SqueezeBertForMultipleChoice.forward:14
#: transformers.SqueezeBertForQuestionAnswering.forward:14
#: transformers.SqueezeBertForSequenceClassification.forward:14
#: transformers.SqueezeBertForTokenClassification.forward:14
#: transformers.SqueezeBertModel.forward:14
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:16
#: transformers.SqueezeBertForMultipleChoice.forward:16
#: transformers.SqueezeBertForQuestionAnswering.forward:16
#: transformers.SqueezeBertForSequenceClassification.forward:16
#: transformers.SqueezeBertForTokenClassification.forward:16
#: transformers.SqueezeBertModel.forward:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:16
#: transformers.SqueezeBertForMultipleChoice.forward:16
#: transformers.SqueezeBertForQuestionAnswering.forward:16
#: transformers.SqueezeBertForSequenceClassification.forward:16
#: transformers.SqueezeBertForTokenClassification.forward:16
#: transformers.SqueezeBertModel.forward:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:18
#: transformers.SqueezeBertForMultipleChoice.forward:18
#: transformers.SqueezeBertForQuestionAnswering.forward:18
#: transformers.SqueezeBertForSequenceClassification.forward:18
#: transformers.SqueezeBertForTokenClassification.forward:18
#: transformers.SqueezeBertModel.forward:18
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:19
#: transformers.SqueezeBertForMultipleChoice.forward:19
#: transformers.SqueezeBertForQuestionAnswering.forward:19
#: transformers.SqueezeBertForSequenceClassification.forward:19
#: transformers.SqueezeBertForTokenClassification.forward:19
#: transformers.SqueezeBertModel.forward:19
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:21
#: transformers.SqueezeBertForMultipleChoice.forward:21
#: transformers.SqueezeBertForQuestionAnswering.forward:21
#: transformers.SqueezeBertForSequenceClassification.forward:21
#: transformers.SqueezeBertForTokenClassification.forward:21
#: transformers.SqueezeBertModel.forward:21
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:23
#: transformers.SqueezeBertForMultipleChoice.forward:23
#: transformers.SqueezeBertForQuestionAnswering.forward:23
#: transformers.SqueezeBertForSequenceClassification.forward:23
#: transformers.SqueezeBertForTokenClassification.forward:23
#: transformers.SqueezeBertModel.forward:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:  - 0 corresponds to a "
"`sentence A` token, - 1 corresponds to a `sentence B` token.  `What are "
"token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:23
#: transformers.SqueezeBertForMultipleChoice.forward:23
#: transformers.SqueezeBertForQuestionAnswering.forward:23
#: transformers.SqueezeBertForSequenceClassification.forward:23
#: transformers.SqueezeBertForTokenClassification.forward:23
#: transformers.SqueezeBertModel.forward:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:26
#: transformers.SqueezeBertForMultipleChoice.forward:26
#: transformers.SqueezeBertForQuestionAnswering.forward:26
#: transformers.SqueezeBertForSequenceClassification.forward:26
#: transformers.SqueezeBertForTokenClassification.forward:26
#: transformers.SqueezeBertModel.forward:26
msgid "0 corresponds to a `sentence A` token,"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:27
#: transformers.SqueezeBertForMultipleChoice.forward:27
#: transformers.SqueezeBertForQuestionAnswering.forward:27
#: transformers.SqueezeBertForSequenceClassification.forward:27
#: transformers.SqueezeBertForTokenClassification.forward:27
#: transformers.SqueezeBertModel.forward:27
msgid "1 corresponds to a `sentence B` token."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:29
#: transformers.SqueezeBertForMultipleChoice.forward:29
#: transformers.SqueezeBertForQuestionAnswering.forward:29
#: transformers.SqueezeBertForSequenceClassification.forward:29
#: transformers.SqueezeBertForTokenClassification.forward:29
#: transformers.SqueezeBertModel.forward:29
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:31
#: transformers.SqueezeBertForMultipleChoice.forward:31
#: transformers.SqueezeBertForQuestionAnswering.forward:31
#: transformers.SqueezeBertForSequenceClassification.forward:31
#: transformers.SqueezeBertForTokenClassification.forward:31
#: transformers.SqueezeBertModel.forward:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``.  `What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:31
#: transformers.SqueezeBertForMultipleChoice.forward:31
#: transformers.SqueezeBertForQuestionAnswering.forward:31
#: transformers.SqueezeBertForSequenceClassification.forward:31
#: transformers.SqueezeBertForTokenClassification.forward:31
#: transformers.SqueezeBertModel.forward:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:34
#: transformers.SqueezeBertForMultipleChoice.forward:34
#: transformers.SqueezeBertForQuestionAnswering.forward:34
#: transformers.SqueezeBertForSequenceClassification.forward:34
#: transformers.SqueezeBertForTokenClassification.forward:34
#: transformers.SqueezeBertModel.forward:34
msgid "`What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:36
#: transformers.SqueezeBertForMultipleChoice.forward:36
#: transformers.SqueezeBertForQuestionAnswering.forward:36
#: transformers.SqueezeBertForSequenceClassification.forward:36
#: transformers.SqueezeBertForTokenClassification.forward:36
#: transformers.SqueezeBertModel.forward:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:36
#: transformers.SqueezeBertForMultipleChoice.forward:36
#: transformers.SqueezeBertForQuestionAnswering.forward:36
#: transformers.SqueezeBertForSequenceClassification.forward:36
#: transformers.SqueezeBertForTokenClassification.forward:36
#: transformers.SqueezeBertModel.forward:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:38
#: transformers.SqueezeBertForMultipleChoice.forward:38
#: transformers.SqueezeBertForQuestionAnswering.forward:38
#: transformers.SqueezeBertForSequenceClassification.forward:38
#: transformers.SqueezeBertForTokenClassification.forward:38
#: transformers.SqueezeBertModel.forward:38
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:39
#: transformers.SqueezeBertForMultipleChoice.forward:39
#: transformers.SqueezeBertForQuestionAnswering.forward:39
#: transformers.SqueezeBertForSequenceClassification.forward:39
#: transformers.SqueezeBertForTokenClassification.forward:39
#: transformers.SqueezeBertModel.forward:39
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:41
#: transformers.SqueezeBertForMultipleChoice.forward:41
#: transformers.SqueezeBertForQuestionAnswering.forward:41
#: transformers.SqueezeBertForSequenceClassification.forward:41
#: transformers.SqueezeBertForTokenClassification.forward:41
#: transformers.SqueezeBertModel.forward:41
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:45
#: transformers.SqueezeBertForMultipleChoice.forward:45
#: transformers.SqueezeBertForQuestionAnswering.forward:45
#: transformers.SqueezeBertForSequenceClassification.forward:45
#: transformers.SqueezeBertForTokenClassification.forward:45
#: transformers.SqueezeBertModel.forward:45
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:48
#: transformers.SqueezeBertForMultipleChoice.forward:48
#: transformers.SqueezeBertForQuestionAnswering.forward:48
#: transformers.SqueezeBertForSequenceClassification.forward:48
#: transformers.SqueezeBertForTokenClassification.forward:48
#: transformers.SqueezeBertModel.forward:48
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:51
#: transformers.SqueezeBertForMultipleChoice.forward:51
#: transformers.SqueezeBertForQuestionAnswering.forward:51
#: transformers.SqueezeBertForSequenceClassification.forward:51
#: transformers.SqueezeBertForTokenClassification.forward:51
#: transformers.SqueezeBertModel.forward:51
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.SqueezeBertModel.forward:54
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs.  - **last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **pooler_output** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`) -- "
"Last layer hidden-state of the first token of the sequence "
"(classification token) further processed by a   Linear layer and a Tanh "
"activation function. The Linear layer weights are trained from the next "
"sentence   prediction (classification) objective during pretraining. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.SqueezeBertModel.forward:54
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs."
msgstr ""

#: of transformers.SqueezeBertModel.forward:58
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.SqueezeBertModel.forward:59
msgid ""
"**pooler_output** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"hidden_size)`) -- Last layer hidden-state of the first token of the "
"sequence (classification token) further processed by a Linear layer and a"
" Tanh activation function. The Linear layer weights are trained from the "
"next sentence prediction (classification) objective during pretraining."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:64
#: transformers.SqueezeBertForMultipleChoice.forward:66
#: transformers.SqueezeBertForQuestionAnswering.forward:69
#: transformers.SqueezeBertForSequenceClassification.forward:64
#: transformers.SqueezeBertForTokenClassification.forward:63
#: transformers.SqueezeBertModel.forward:62
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:67
#: transformers.SqueezeBertForMultipleChoice.forward:69
#: transformers.SqueezeBertForQuestionAnswering.forward:72
#: transformers.SqueezeBertForSequenceClassification.forward:67
#: transformers.SqueezeBertForTokenClassification.forward:66
#: transformers.SqueezeBertModel.forward:65
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:68
#: transformers.SqueezeBertForMultipleChoice.forward:70
#: transformers.SqueezeBertForQuestionAnswering.forward:73
#: transformers.SqueezeBertForSequenceClassification.forward:68
#: transformers.SqueezeBertForTokenClassification.forward:67
#: transformers.SqueezeBertModel.forward:66
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:71
#: transformers.SqueezeBertForMultipleChoice.forward:73
#: transformers.SqueezeBertForQuestionAnswering.forward:76
#: transformers.SqueezeBertForSequenceClassification.forward:71
#: transformers.SqueezeBertForTokenClassification.forward:70
#: transformers.SqueezeBertModel.forward:69
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.SqueezeBertModel.forward:71
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:75
#: transformers.SqueezeBertForMultipleChoice.forward:77
#: transformers.SqueezeBertForQuestionAnswering.forward:80
#: transformers.SqueezeBertForSequenceClassification.forward:75
#: transformers.SqueezeBertForTokenClassification.forward:74
#: transformers.SqueezeBertModel.forward:73
msgid "Example::"
msgstr ""

#: of transformers.SqueezeBertModel.get_input_embeddings:1
msgid "Returns the model's input embeddings."
msgstr ""

#: of transformers.SqueezeBertModel.get_input_embeddings:3
msgid "A torch module mapping vocabulary to hidden states."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.get_output_embeddings:4
#: transformers.SqueezeBertModel.get_input_embeddings:4
msgid ":obj:`nn.Module`"
msgstr ""

#: of transformers.SqueezeBertModel.set_input_embeddings:1
msgid "Set model's input embeddings."
msgstr ""

#: of transformers.SqueezeBertModel.set_input_embeddings:3
msgid "A module mapping vocabulary to hidden states."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:83
msgid "SqueezeBertForMaskedLM"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM:1
msgid "SqueezeBERT Model with a `language modeling` head on top."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:1
msgid ""
"The :class:`~transformers.SqueezeBertForMaskedLM` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:53
msgid ""
"Labels for computing the masked language modeling loss. Indices should be"
" in ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) "
"Tokens with indices set to ``-100`` are ignored (masked), the loss is "
"only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:58
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.SqueezeBertConfig`) and inputs."
"  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss. - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:58
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.SqueezeBertConfig`) and inputs."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:62
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:63
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.forward:73
msgid ""
":class:`~transformers.modeling_outputs.MaskedLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.get_output_embeddings:1
msgid "Returns the model's output embeddings."
msgstr ""

#: of transformers.SqueezeBertForMaskedLM.get_output_embeddings:3
msgid "A torch module mapping hidden states to vocabulary."
msgstr ""

#: ../../source/model_doc/squeezebert.rst:90
msgid "SqueezeBertForSequenceClassification"
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification:1
msgid ""
"SqueezeBERT Model transformer with a sequence classification/regression "
"head on top (a linear layer on top of the pooled output) e.g. for GLUE "
"tasks."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:1
msgid ""
"The :class:`~transformers.SqueezeBertForSequenceClassification` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:53
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy)."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:58
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"(or regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:58
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:62
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:63
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.SqueezeBertForSequenceClassification.forward:73
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:97
msgid "SqueezeBertForMultipleChoice"
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice:1
msgid ""
"SqueezeBERT Model with a multiple choice classification head on top (a "
"linear layer on top of the pooled output and a softmax) e.g. for "
"RocStories/SWAG tasks."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:1
msgid ""
"The :class:`~transformers.SqueezeBertForMultipleChoice` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:53
msgid ""
"Labels for computing the multiple choice classification loss. Indices "
"should be in ``[0, ..., num_choices-1]`` where `num_choices` is the size "
"of the second dimension of the input tensors. (see `input_ids` above)"
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:58
msgid ""
"A :class:`~transformers.modeling_outputs.MultipleChoiceModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape `(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"loss. - **logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_choices)`) -- `num_choices` is the second dimension of the input "
"tensors. (see `input_ids` above).    Classification scores (before "
"SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:58
msgid ""
"A :class:`~transformers.modeling_outputs.MultipleChoiceModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:62
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape `(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Classification loss."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:63
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_choices)`) -- `num_choices` is the second dimension of the input "
"tensors. (see `input_ids` above)."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:65
msgid "Classification scores (before SoftMax)."
msgstr ""

#: of transformers.SqueezeBertForMultipleChoice.forward:75
msgid ""
":class:`~transformers.modeling_outputs.MultipleChoiceModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:104
msgid "SqueezeBertForTokenClassification"
msgstr ""

#: of transformers.SqueezeBertForTokenClassification:1
msgid ""
"SqueezeBERT Model with a token classification head on top (a linear layer"
" on top of the hidden-states output) e.g. for Named-Entity-Recognition "
"(NER) tasks."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:1
msgid ""
"The :class:`~transformers.SqueezeBertForTokenClassification` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:53
msgid ""
"Labels for computing the token classification loss. Indices should be in "
"``[0, ..., config.num_labels - 1]``."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:57
msgid ""
"A :class:`~transformers.modeling_outputs.TokenClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when ``labels`` is provided)  -- Classification "
"loss. - **logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.num_labels)`) -- Classification scores (before "
"SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:57
msgid ""
"A :class:`~transformers.modeling_outputs.TokenClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.SqueezeBertConfig`) and "
"inputs."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:61
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when ``labels`` is provided)  -- Classification loss."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:62
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.num_labels)`) -- Classification scores (before "
"SoftMax)."
msgstr ""

#: of transformers.SqueezeBertForTokenClassification.forward:72
msgid ""
":class:`~transformers.modeling_outputs.TokenClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/squeezebert.rst:111
msgid "SqueezeBertForQuestionAnswering"
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering:1
msgid ""
"SqueezeBERT Model with a span classification head on top for extractive "
"question-answering tasks like SQuAD (a linear layers on top of the "
"hidden-states output to compute `span start logits` and `span end "
"logits`)."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:1
msgid ""
"The :class:`~transformers.SqueezeBertForQuestionAnswering` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:53
msgid ""
"Labels for position (index) of the start of the labelled span for "
"computing the token classification loss. Positions are clamped to the "
"length of the sequence (`sequence_length`). Position outside of the "
"sequence are not taken into account for computing the loss."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:57
msgid ""
"Labels for position (index) of the end of the labelled span for computing"
" the token classification loss. Positions are clamped to the length of "
"the sequence (`sequence_length`). Position outside of the sequence are "
"not taken into account for computing the loss."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:62
msgid ""
"A :class:`~transformers.modeling_outputs.QuestionAnsweringModelOutput` or"
" a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed "
"or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.SqueezeBertConfig`)"
" and inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Total span "
"extraction loss is the sum of a Cross-Entropy for the start and end "
"positions. - **start_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Span-start scores (before "
"SoftMax). - **end_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Span-end scores (before "
"SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:62
msgid ""
"A :class:`~transformers.modeling_outputs.QuestionAnsweringModelOutput` or"
" a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed "
"or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.SqueezeBertConfig`)"
" and inputs."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:66
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Total span extraction loss is"
" the sum of a Cross-Entropy for the start and end positions."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:67
msgid ""
"**start_logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Span-start scores (before SoftMax)."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:68
msgid ""
"**end_logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Span-end scores (before SoftMax)."
msgstr ""

#: of transformers.SqueezeBertForQuestionAnswering.forward:78
msgid ""
":class:`~transformers.modeling_outputs.QuestionAnsweringModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""


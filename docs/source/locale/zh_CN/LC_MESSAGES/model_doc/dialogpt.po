# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/dialogpt.rst:14
msgid "DialoGPT"
msgstr ""

#: ../../source/model_doc/dialogpt.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/dialogpt.rst:19
msgid ""
"DialoGPT was proposed in `DialoGPT: Large-Scale Generative Pre-training "
"for Conversational Response Generation "
"<https://arxiv.org/abs/1911.00536>`_ by Yizhe Zhang, Siqi Sun, Michel "
"Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing "
"Liu, Bill Dolan. It's a GPT2 Model trained on 147M conversation-like "
"exchanges extracted from Reddit."
msgstr ""

#: ../../source/model_doc/dialogpt.rst:24
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/dialogpt.rst:26
msgid ""
"*We present a large, tunable neural conversational response generation "
"model, DialoGPT (dialogue generative pre-trained transformer). Trained on"
" 147M conversation-like exchanges extracted from Reddit comment chains "
"over a period spanning from 2005 through 2017, DialoGPT extends the "
"Hugging Face PyTorch transformer to attain a performance close to human "
"both in terms of automatic and human evaluation in single-turn dialogue "
"settings. We show that conversational systems that leverage DialoGPT "
"generate more relevant, contentful and context-consistent responses than "
"strong baseline systems. The pre-trained model and training pipeline are "
"publicly released to facilitate research into neural response generation "
"and the development of more intelligent open-domain dialogue systems.*"
msgstr ""

#: ../../source/model_doc/dialogpt.rst:34
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/dialogpt.rst:36
msgid ""
"DialoGPT is a model with absolute position embeddings so it's usually "
"advised to pad the inputs on the right rather than the left."
msgstr ""

#: ../../source/model_doc/dialogpt.rst:38
msgid ""
"DialoGPT was trained with a causal language modeling (CLM) objective on "
"conversational data and is therefore powerful at response generation in "
"open-domain dialogue systems."
msgstr ""

#: ../../source/model_doc/dialogpt.rst:40
msgid ""
"DialoGPT enables the user to create a chat bot in just 10 lines of code "
"as shown on `DialoGPT's model card <https://huggingface.co/microsoft"
"/DialoGPT-medium>`_."
msgstr ""

#: ../../source/model_doc/dialogpt.rst:43
msgid "Training:"
msgstr ""

#: ../../source/model_doc/dialogpt.rst:45
msgid ""
"In order to train or fine-tune DialoGPT, one can use causal language "
"modeling training. To cite the official paper: *We follow the OpenAI "
"GPT-2 to model a multiturn dialogue session as a long text and frame the "
"generation task as language modeling. We first concatenate all dialog "
"turns within a dialogue session into a long text x_1,..., x_N (N is the "
"sequence length), ended by the end-of-text token.* For more information "
"please confer to the original paper."
msgstr ""

#: ../../source/model_doc/dialogpt.rst:51
msgid ""
"DialoGPT's architecture is based on the GPT2 model, so one can refer to "
":doc:`GPT2's documentation page <gpt2>`."
msgstr ""

#: ../../source/model_doc/dialogpt.rst:53
msgid ""
"The original code can be found `here "
"<https://github.com/microsoft/DialoGPT>`_."
msgstr ""


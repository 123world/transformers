# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/encoderdecoder.rst:14
msgid "Encoder Decoder Models"
msgstr ""

#: ../../source/model_doc/encoderdecoder.rst:16
msgid ""
"The :class:`~transformers.EncoderDecoderModel` can be used to initialize "
"a sequence-to-sequence model with any pretrained autoencoding model as "
"the encoder and any pretrained autoregressive model as the decoder."
msgstr ""

#: ../../source/model_doc/encoderdecoder.rst:19
msgid ""
"The effectiveness of initializing sequence-to-sequence models with "
"pretrained checkpoints for sequence generation tasks was shown in "
"`Leveraging Pre-trained Checkpoints for Sequence Generation Tasks "
"<https://arxiv.org/abs/1907.12461>`__ by Sascha Rothe, Shashi Narayan, "
"Aliaksei Severyn."
msgstr ""

#: ../../source/model_doc/encoderdecoder.rst:23
msgid ""
"After such an :class:`~transformers.EncoderDecoderModel` has been trained"
"/fine-tuned, it can be saved/loaded just like any other models (see the "
"examples for more information)."
msgstr ""

#: ../../source/model_doc/encoderdecoder.rst:26
msgid ""
"An application of this architecture could be to leverage two pretrained "
":class:`~transformers.BertModel` as the encoder and decoder for a "
"summarization model as was shown in: `Text Summarization with Pretrained "
"Encoders <https://arxiv.org/abs/1908.08345>`__ by Yang Liu and Mirella "
"Lapata."
msgstr ""

#: ../../source/model_doc/encoderdecoder.rst:32
msgid "EncoderDecoderConfig"
msgstr ""

#: of transformers.EncoderDecoderConfig:1
msgid ""
":class:`~transformers.EncoderDecoderConfig` is the configuration class to"
" store the configuration of a :class:`~transformers.EncoderDecoderModel`."
" It is used to instantiate an Encoder Decoder model according to the "
"specified arguments, defining the encoder and decoder configs."
msgstr ""

#: of transformers.EncoderDecoderConfig:5
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.EncoderDecoderConfig transformers.EncoderDecoderModel
#: transformers.EncoderDecoderModel.forward
msgid "Parameters"
msgstr ""

#: of transformers.EncoderDecoderConfig:8
msgid ""
"Dictionary of keyword arguments. Notably:      - **encoder** "
"(:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a"
" configuration       object that defines the encoder config.     - "
"**decoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An "
"instance of a configuration       object that defines the decoder config."
msgstr ""

#: of transformers.EncoderDecoderConfig:9
msgid "Dictionary of keyword arguments. Notably:"
msgstr ""

#: of transformers.EncoderDecoderConfig:11
msgid ""
"**encoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An "
"instance of a configuration object that defines the encoder config."
msgstr ""

#: of transformers.EncoderDecoderConfig:13
msgid ""
"**decoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An "
"instance of a configuration object that defines the decoder config."
msgstr ""

#: of transformers.EncoderDecoderConfig:17
#: transformers.EncoderDecoderModel.forward:119
msgid "Examples::"
msgstr ""

#: of transformers.EncoderDecoderConfig.from_encoder_decoder_configs:1
msgid ""
"Instantiate a :class:`~transformers.EncoderDecoderConfig` (or a derived "
"class) from a pre-trained encoder model configuration and decoder model "
"configuration."
msgstr ""

#: of transformers.EncoderDecoderConfig.from_encoder_decoder_configs
#: transformers.EncoderDecoderConfig.to_dict
#: transformers.EncoderDecoderModel.forward
msgid "Returns"
msgstr ""

#: of transformers.EncoderDecoderConfig.from_encoder_decoder_configs:4
msgid "An instance of a configuration object"
msgstr ""

#: of transformers.EncoderDecoderConfig.from_encoder_decoder_configs
#: transformers.EncoderDecoderConfig.to_dict
#: transformers.EncoderDecoderModel.forward
msgid "Return type"
msgstr ""

#: of transformers.EncoderDecoderConfig.from_encoder_decoder_configs:5
msgid ":class:`EncoderDecoderConfig`"
msgstr ""

#: of transformers.EncoderDecoderConfig.to_dict:1
msgid ""
"Serializes this instance to a Python dictionary. Override the default "
"`to_dict()` from `PretrainedConfig`."
msgstr ""

#: of transformers.EncoderDecoderConfig.to_dict:3
msgid "Dictionary of all the attributes that make up this configuration instance,"
msgstr ""

#: of transformers.EncoderDecoderConfig.to_dict:4
msgid ":obj:`Dict[str, any]`"
msgstr ""

#: ../../source/model_doc/encoderdecoder.rst:39
msgid "EncoderDecoderModel"
msgstr ""

#: of transformers.EncoderDecoderModel:1
msgid ""
"This class can be used to initialize a sequence-to-sequence model with "
"any pretrained autoencoding model as the encoder and any pretrained "
"autoregressive model as the decoder. The encoder is loaded via "
":meth:`~transformers.AutoModel.from_pretrained` function and the decoder "
"is loaded via :meth:`~transformers.AutoModelForCausalLM.from_pretrained` "
"function. Cross-attention layers are automatically added to the decoder "
"and should be fine-tuned on a downstream generative task, like "
"summarization."
msgstr ""

#: of transformers.EncoderDecoderModel:7
msgid ""
"The effectiveness of initializing sequence-to-sequence models with "
"pretrained checkpoints for sequence generation tasks was shown in "
"`Leveraging Pre-trained Checkpoints for Sequence Generation Tasks "
"<https://arxiv.org/abs/1907.12461>`__ by Sascha Rothe, Shashi Narayan, "
"Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu."
msgstr ""

#: of transformers.EncoderDecoderModel:12
msgid ""
"After such an Encoder Decoder model has been trained/fine-tuned, it can "
"be saved/loaded just like any other models (see the examples for more "
"information)."
msgstr ""

#: of transformers.EncoderDecoderModel:15
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.EncoderDecoderModel:19
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.EncoderDecoderModel:23
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.EncoderDecoderModel:29
msgid ""
":class:`~transformers.EncoderDecoder` is a generic model class that will "
"be instantiated as a transformer architecture with one of the base model "
"classes of the library as encoder and another one as decoder when created"
" with the :meth`~transformers.AutoModel.from_pretrained` class method for"
" the encoder and "
":meth`~transformers.AutoModelForCausalLM.from_pretrained` class method "
"for the decoder."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:1
msgid ""
"The :class:`~transformers.EncoderDecoderModel` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:10
#: transformers.EncoderDecoderModel.forward:25
msgid ""
"Indices can be obtained using :class:`~transformers.PreTrainedTokenizer`."
" See :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:14
#: transformers.EncoderDecoderModel.forward:29
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:18
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:19
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:21
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:23
msgid ""
"Indices of decoder input sequence tokens in the vocabulary.  Indices can "
"be obtained using :class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__  If :obj:`past_key_values` is"
" used, optionally only the last :obj:`decoder_input_ids` have to be input"
" (see :obj:`past_key_values`).  Provide for sequence to sequence training"
" to the decoder. Indices can be obtained using "
":class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:23
msgid "Indices of decoder input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:31
msgid ""
"If :obj:`past_key_values` is used, optionally only the last "
":obj:`decoder_input_ids` have to be input (see :obj:`past_key_values`)."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:34
msgid ""
"Provide for sequence to sequence training to the decoder. Indices can be "
"obtained using :class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:38
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:41
msgid ""
"This tuple must consist of (:obj:`last_hidden_state`, `optional`: "
":obj:`hidden_states`, `optional`: :obj:`attentions`) "
":obj:`last_hidden_state` (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) is a tensor of hidden-"
"states at the output of the last layer of the encoder. Used in the cross-"
"attention of the decoder."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:46
msgid ""
"Contains precomputed key and value hidden states of the attention blocks."
" Can be used to speed up decoding.  If :obj:`past_key_values` are used, "
"the user can optionally input only the last :obj:`decoder_input_ids` "
"(those that don't have their past key value states given to this model) "
"of shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids` "
"of shape :obj:`(batch_size, sequence_length)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:46
msgid ""
"Contains precomputed key and value hidden states of the attention blocks."
" Can be used to speed up decoding."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:48
msgid ""
"If :obj:`past_key_values` are used, the user can optionally input only "
"the last :obj:`decoder_input_ids` (those that don't have their past key "
"value states given to this model) of shape :obj:`(batch_size, 1)` instead"
" of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:52
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:56
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. This is useful if you want "
"more control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:60
msgid ""
"Labels for computing the masked language modeling loss for the decoder. "
"Indices should be in ``[-100, 0, ..., config.vocab_size]`` (see "
"``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored "
"(masked), the loss is only computed for the tokens with labels in ``[0, "
"..., config.vocab_size]``"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:64
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:67
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:70
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:73
msgid ""
"If set to ``True``, the model will return a "
":class:`~transformers.file_utils.Seq2SeqLMOutput` instead of a plain "
"tuple."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:76
msgid ""
"(`optional`) Remaining dictionary of keyword arguments. Keyword arguments"
" come in two flavors:  - Without a prefix which will be input as "
"``**encoder_kwargs`` for the encoder forward function. - With a "
"`decoder_` prefix which will be input as ``**decoder_kwargs`` for the "
"decoder forward function."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:76
msgid ""
"(`optional`) Remaining dictionary of keyword arguments. Keyword arguments"
" come in two flavors:"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:78
msgid ""
"Without a prefix which will be input as ``**encoder_kwargs`` for the "
"encoder forward function."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:79
msgid ""
"With a `decoder_` prefix which will be input as ``**decoder_kwargs`` for "
"the decoder forward function."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:81
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.EncoderDecoderConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Language modeling"
" loss. - **logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size,"
" sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
" - **past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads.   Examples::      >>> from transformers import "
"EncoderDecoderModel, BertTokenizer     >>> import torch      >>> "
"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')     >>> "
"model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-"
"uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained "
"checkpoints      >>> # forward     >>> input_ids = "
"torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", "
"add_special_tokens=True)).unsqueeze(0)  # Batch size 1     >>> outputs = "
"model(input_ids=input_ids, decoder_input_ids=input_ids)      >>> # "
"training     >>> outputs = model(input_ids=input_ids, "
"decoder_input_ids=input_ids, labels=input_ids)     >>> loss, logits = "
"outputs.loss, outputs.logits      >>> # save and load from pretrained"
"     >>> model.save_pretrained(\"bert2bert\")     >>> model = "
"EncoderDecoderModel.from_pretrained(\"bert2bert\")      >>> # generation"
"     >>> generated = model.generate(input_ids, "
"decoder_start_token_id=model.config.decoder.pad_token_id)"
msgstr ""

#: of transformers.EncoderDecoderModel.forward:81
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.EncoderDecoderConfig`) and "
"inputs."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:85
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:86
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:87
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors of shape"
" :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) "
"and 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:91
msgid ""
"Contains pre-computed hidden-states (key and values in the self-attention"
" blocks and in the cross-attention blocks) that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:93
msgid ""
"**decoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:96
msgid ""
"Hidden-states of the decoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:97
msgid ""
"**decoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:100
msgid ""
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:102
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:105
msgid ""
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:107
msgid ""
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:108
msgid ""
"**encoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:111
msgid ""
"Hidden-states of the encoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:112
msgid ""
"**encoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:115
msgid ""
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.EncoderDecoderModel.forward:141
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:1
msgid ""
"Instantiate an encoder and a decoder from one or two base classes of the "
"library from pretrained model checkpoints."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:5
msgid ""
"The model is set in evaluation mode by default using :obj:`model.eval()` "
"(Dropout modules are deactivated). To train the model, you need to first "
"set it back in training mode with :obj:`model.train()`."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:46
msgid "Params:"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:20
msgid "encoder_pretrained_model_name_or_path (:obj: `str`, `optional`):"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:10
msgid "Information necessary to initiate the encoder. Can be either:"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:12
#: transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:25
msgid ""
"A string, the `model id` of a pretrained model hosted inside a model repo"
" on huggingface.co. Valid model ids can be located at the root-level, "
"like ``bert-base-uncased``, or namespaced under a user or organization "
"name, like ``dbmdz/bert-base-german-cased``."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:15
#: transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:28
msgid ""
"A path to a `directory` containing model weights saved using "
":func:`~transformers.PreTrainedModel.save_pretrained`, e.g., "
"``./my_model_directory/``."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:17
#: transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:30
msgid ""
"A path or url to a `tensorflow index checkpoint file` (e.g, "
"``./tf_model/model.ckpt.index``). In this case, ``from_tf`` should be set"
" to :obj:`True` and a configuration object should be provided as "
"``config`` argument. This loading path is slower than converting the "
"TensorFlow checkpoint in a PyTorch model using the provided conversion "
"scripts and loading the PyTorch model afterwards."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:33
msgid ""
"decoder_pretrained_model_name_or_path (:obj: `str`, `optional`, defaults "
"to `None`):"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:23
msgid "Information necessary to initiate the decoder. Can be either:"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:36
msgid "model_args (remaining positional arguments, `optional`):"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:36
msgid ""
"All remaning positional arguments will be passed to the underlying "
"model's ``__init__`` method."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:46
msgid "kwargs (remaining dictionary of keyword arguments, `optional`):"
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:39
msgid ""
"Can be used to update the configuration object (after it being loaded) "
"and initiate the model (e.g., :obj:`output_attentions=True`)."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:42
msgid ""
"To update the encoder configuration, use the prefix `encoder_` for each "
"configuration parameter."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:43
msgid ""
"To update the decoder configuration, use the prefix `decoder_` for each "
"configuration parameter."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:44
msgid ""
"To update the parent model configuration, do not use a prefix for each "
"configuration parameter."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:46
msgid ""
"Behaves differently depending on whether a :obj:`config` is provided or "
"automatically loaded."
msgstr ""

#: of transformers.EncoderDecoderModel.from_encoder_decoder_pretrained:48
msgid "Example::"
msgstr ""


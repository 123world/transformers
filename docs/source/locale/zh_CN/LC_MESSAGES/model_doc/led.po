# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/led.rst:14
msgid "LED"
msgstr ""

#: ../../source/model_doc/led.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/led.rst:19
msgid ""
"The LED model was proposed in `Longformer: The Long-Document Transformer "
"<https://arxiv.org/abs/2004.05150>`__ by Iz Beltagy, Matthew E. Peters, "
"Arman Cohan."
msgstr ""

#: ../../source/model_doc/led.rst:22
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/led.rst:24
msgid ""
"*Transformer-based models are unable to process long sequences due to "
"their self-attention operation, which scales quadratically with the "
"sequence length. To address this limitation, we introduce the Longformer "
"with an attention mechanism that scales linearly with sequence length, "
"making it easy to process documents of thousands of tokens or longer. "
"Longformer's attention mechanism is a drop-in replacement for the "
"standard self-attention and combines a local windowed attention with a "
"task motivated global attention. Following prior work on long-sequence "
"transformers, we evaluate Longformer on character-level language modeling"
" and achieve state-of-the-art results on text8 and enwik8. In contrast to"
" most prior work, we also pretrain Longformer and finetune it on a "
"variety of downstream tasks. Our pretrained Longformer consistently "
"outperforms RoBERTa on long document tasks and sets new state-of-the-art "
"results on WikiHop and TriviaQA. We finally introduce the Longformer-"
"Encoder-Decoder (LED), a Longformer variant for supporting long document "
"generative sequence-to-sequence tasks, and demonstrate its effectiveness "
"on the arXiv summarization dataset.*"
msgstr ""

#: ../../source/model_doc/led.rst:36
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/led.rst:38
msgid ""
":class:`~transformers.LEDForConditionalGeneration` is an extension of "
":class:`~transformers.BartForConditionalGeneration` exchanging the "
"traditional *self-attention* layer with *Longformer*'s *chunked self-"
"attention* layer. :class:`~transformers.LEDTokenizer` is an alias of "
":class:`~transformers.BartTokenizer`."
msgstr ""

#: ../../source/model_doc/led.rst:42
msgid ""
"LED works very well on long-range *sequence-to-sequence* tasks where the "
"``input_ids`` largely exceed a length of 1024 tokens."
msgstr ""

#: ../../source/model_doc/led.rst:44
msgid ""
"LED pads the ``input_ids`` to be a multiple of "
"``config.attention_window`` if required. Therefore a small speed-up is "
"gained, when :class:`~transformers.LEDTokenizer` is used with the "
"``pad_to_multiple_of`` argument."
msgstr ""

#: ../../source/model_doc/led.rst:46
msgid ""
"LED makes use of *global attention* by means of the "
"``global_attention_mask`` (see :class:`~transformers.LongformerModel`). "
"For summarization, it is advised to put *global attention* only on the "
"first ``<s>`` token. For question answering, it is advised to put *global"
" attention* on all tokens of the question."
msgstr ""

#: ../../source/model_doc/led.rst:49
msgid ""
"To fine-tune LED on all 16384, it is necessary to enable *gradient "
"checkpointing* by setting ``config.gradient_checkpointing = True``."
msgstr ""

#: ../../source/model_doc/led.rst:51
msgid ""
"A notebook showing how to evaluate LED, can be accessed `here "
"<https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing>`__."
msgstr ""

#: ../../source/model_doc/led.rst:53
msgid ""
"A notebook showing how to fine-tune LED, can be accessed `here "
"<https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing>`__."
msgstr ""

#: ../../source/model_doc/led.rst:56
msgid ""
"This model was contributed by `patrickvonplaten "
"<https://huggingface.co/patrickvonplaten>`__."
msgstr ""

#: ../../source/model_doc/led.rst:60
msgid "LEDConfig"
msgstr ""

#: of transformers.LEDConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.LEDModel`. It is used to instantiate an LED model "
"according to the specified arguments, defining the model architecture. "
"Instantiating a configuration with the defaults will yield a similar "
"configuration to that of the LED `allenai/led-base-16384 "
"<https://huggingface.co/allenai/led-base-16384>`__ architecture."
msgstr ""

#: of transformers.LEDConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.LEDConfig transformers.LEDForConditionalGeneration
#: transformers.LEDForConditionalGeneration.forward
#: transformers.LEDForQuestionAnswering
#: transformers.LEDForQuestionAnswering.forward
#: transformers.LEDForSequenceClassification
#: transformers.LEDForSequenceClassification.forward transformers.LEDModel
#: transformers.LEDModel.forward
#: transformers.LEDTokenizer.build_inputs_with_special_tokens
#: transformers.LEDTokenizer.create_token_type_ids_from_sequences
#: transformers.LEDTokenizer.get_special_tokens_mask
#: transformers.LEDTokenizer.save_vocabulary
#: transformers.TFLEDForConditionalGeneration
#: transformers.TFLEDForConditionalGeneration.call transformers.TFLEDModel
#: transformers.TFLEDModel.call
#: transformers.models.led.modeling_led.LEDEncoderBaseModelOutput
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput
#: transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput
msgid "Parameters"
msgstr ""

#: of transformers.LEDConfig:10
msgid ""
"Vocabulary size of the LED model. Defines the number of different tokens "
"that can be represented by the :obj:`inputs_ids` passed when calling "
":class:`~transformers.LEDModel` or :class:`~transformers.TFLEDModel`."
msgstr ""

#: of transformers.LEDConfig:13
msgid "Dimensionality of the layers and the pooler layer."
msgstr ""

#: of transformers.LEDConfig:15
msgid "Number of encoder layers."
msgstr ""

#: of transformers.LEDConfig:17
msgid "Number of decoder layers."
msgstr ""

#: of transformers.LEDConfig:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.LEDConfig:21
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"decoder."
msgstr ""

#: of transformers.LEDConfig:23 transformers.LEDConfig:25
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in decoder."
msgstr ""

#: of transformers.LEDConfig:27
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.LEDConfig:30
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.LEDConfig:32
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.LEDConfig:34
msgid "The dropout ratio for activations inside the fully connected layer."
msgstr ""

#: of transformers.LEDConfig:36
msgid "The dropout ratio for classifier."
msgstr ""

#: of transformers.LEDConfig:38
msgid "The maximum sequence length that the encoder might ever be used with."
msgstr ""

#: of transformers.LEDConfig:40
msgid "The maximum sequence length that the decoder might ever be used with."
msgstr ""

#: of transformers.LEDConfig:42
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.LEDConfig:44
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the encoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.LEDConfig:47
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the decoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.LEDConfig:50
msgid ""
"Whether or not the model should return the last key/values attentions "
"(not used by all models)"
msgstr ""

#: of transformers.LEDConfig:52
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: ../../source/model_doc/led.rst:67
msgid "LEDTokenizer"
msgstr ""

#: of transformers.LEDTokenizer:1
msgid "Construct a LED tokenizer."
msgstr ""

#: of transformers.LEDTokenizer:3
msgid ""
":class:`~transformers.LEDTokenizer` is identical to "
":class:`~transformers.BartTokenizer` and runs end-to-end tokenization: "
"punctuation splitting and wordpiece."
msgstr ""

#: of transformers.LEDTokenizer:6
msgid ""
"Refer to superclass :class:`~transformers.BartTokenizer` for usage "
"examples and documentation concerning parameters."
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens. A "
"RoBERTa sequence has the following format:"
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:4
msgid "single sequence: ``<s> X </s>``"
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:5
msgid "pair of sequences: ``<s> A </s></s> B </s>``"
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:7
msgid "List of IDs to which the special tokens will be added."
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:9
#: transformers.LEDTokenizer.create_token_type_ids_from_sequences:6
#: transformers.LEDTokenizer.get_special_tokens_mask:6
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward
#: transformers.LEDForQuestionAnswering.forward
#: transformers.LEDForSequenceClassification.forward
#: transformers.LEDModel.forward
#: transformers.LEDTokenizer.build_inputs_with_special_tokens
#: transformers.LEDTokenizer.create_token_type_ids_from_sequences
#: transformers.LEDTokenizer.get_special_tokens_mask
#: transformers.LEDTokenizer.save_vocabulary
#: transformers.TFLEDForConditionalGeneration.call transformers.TFLEDModel.call
msgid "Returns"
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:12
msgid ""
"List of `input IDs <../glossary.html#input-ids>`__ with the appropriate "
"special tokens."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward
#: transformers.LEDForQuestionAnswering.forward
#: transformers.LEDForSequenceClassification.forward
#: transformers.LEDModel.forward
#: transformers.LEDTokenizer.build_inputs_with_special_tokens
#: transformers.LEDTokenizer.create_token_type_ids_from_sequences
#: transformers.LEDTokenizer.get_special_tokens_mask
#: transformers.LEDTokenizer.save_vocabulary
#: transformers.TFLEDForConditionalGeneration.call transformers.TFLEDModel.call
msgid "Return type"
msgstr ""

#: of transformers.LEDTokenizer.build_inputs_with_special_tokens:13
#: transformers.LEDTokenizer.create_token_type_ids_from_sequences:10
#: transformers.LEDTokenizer.get_special_tokens_mask:12
msgid ":obj:`List[int]`"
msgstr ""

#: of transformers.LEDTokenizer.create_token_type_ids_from_sequences:1
msgid ""
"Create a mask from the two sequences passed to be used in a sequence-pair"
" classification task. RoBERTa does not make use of token type ids, "
"therefore a list of zeros is returned."
msgstr ""

#: of transformers.LEDTokenizer.create_token_type_ids_from_sequences:4
#: transformers.LEDTokenizer.get_special_tokens_mask:4
msgid "List of IDs."
msgstr ""

#: of transformers.LEDTokenizer.create_token_type_ids_from_sequences:9
msgid "List of zeros."
msgstr ""

#: of transformers.LEDTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieve sequence ids from a token list that has no special tokens added."
" This method is called when adding special tokens using the tokenizer "
"``prepare_for_model`` method."
msgstr ""

#: of transformers.LEDTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model."
msgstr ""

#: of transformers.LEDTokenizer.get_special_tokens_mask:11
msgid ""
"A list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of transformers.LEDTokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.LEDTokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.LEDTokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.LEDTokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.LEDTokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.LEDTokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/led.rst:75
msgid "LEDTokenizerFast"
msgstr ""

#: of transformers.LEDTokenizerFast:1
msgid ""
"Construct a \"fast\" LED tokenizer (backed by HuggingFace's `tokenizers` "
"library)."
msgstr ""

#: of transformers.LEDTokenizerFast:3
msgid ""
":class:`~transformers.LEDTokenizerFast` is identical to "
":class:`~transformers.BartTokenizerFast` and runs end-to-end "
"tokenization: punctuation splitting and wordpiece."
msgstr ""

#: of transformers.LEDTokenizerFast:6
msgid ""
"Refer to superclass :class:`~transformers.BartTokenizerFast` for usage "
"examples and documentation concerning parameters."
msgstr ""

#: ../../source/model_doc/led.rst:82
msgid "LED specific outputs"
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:1
msgid ""
"Base class for LEDEncoder's outputs, with potential hidden states, local "
"and global attentions."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:3
#: transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:3
msgid "Sequence of hidden-states at the output of the last layer of the model."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:5
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the model at the output"
" of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:5
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:13
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:32
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:15
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:34
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:15
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:34
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:13
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:32
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:8
#: transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:8
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:10
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x + attention_window + "
"1)`, where ``x`` is the number of tokens with global attention mask.  "
"Local attentions weights after the attention softmax, used to compute the"
" weighted average in the self-attention heads. Those are the attention "
"weights from every token in the sequence to every token with global "
"attention (first ``x`` values) and to every token in the attention window"
" (remaining ``attention_window + 1`` values). Note that the first ``x`` "
"values refer to tokens with fixed positions in the text, but the "
"remaining ``attention_window + 1`` values refer to tokens with relative "
"positions: the attention weight of a token to itself is located at index "
"``x + attention_window / 2`` and the ``attention_window / 2`` preceding "
"(succeeding) values are the attention weights to the ``attention_window /"
" 2`` preceding (succeeding) tokens. If the attention window contains a "
"token with global attention, the attention weight at the corresponding "
"index is set to 0; the value should be accessed from the first ``x`` "
"attention weights. If a token has global attention, the attention weights"
" to all other tokens in :obj:`attentions` is set to 0, the values should "
"be accessed from :obj:`global_attentions`."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:10
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x + attention_window + "
"1)`, where ``x`` is the number of tokens with global attention mask."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:14
#: transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:13
msgid ""
"Local attentions weights after the attention softmax, used to compute the"
" weighted average in the self-attention heads. Those are the attention "
"weights from every token in the sequence to every token with global "
"attention (first ``x`` values) and to every token in the attention window"
" (remaining ``attention_window + 1`` values). Note that the first ``x`` "
"values refer to tokens with fixed positions in the text, but the "
"remaining ``attention_window + 1`` values refer to tokens with relative "
"positions: the attention weight of a token to itself is located at index "
"``x + attention_window / 2`` and the ``attention_window / 2`` preceding "
"(succeeding) values are the attention weights to the ``attention_window /"
" 2`` preceding (succeeding) tokens. If the attention window contains a "
"token with global attention, the attention weight at the corresponding "
"index is set to 0; the value should be accessed from the first ``x`` "
"attention weights. If a token has global attention, the attention weights"
" to all other tokens in :obj:`attentions` is set to 0, the values should "
"be accessed from :obj:`global_attentions`."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:26
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:43
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:45
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:45
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:43
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x)`, where ``x`` is the "
"number of tokens with global attention mask.  Global attentions weights "
"after the attention softmax, used to compute the weighted average in the "
"self-attention heads. Those are the attention weights from every token "
"with global attention to every token in the sequence."
msgstr ""

#: of transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:26
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:43
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:45
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:45
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:43
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x)`, where ``x`` is the "
"number of tokens with global attention mask."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:111
#: transformers.TFLEDModel.call:113
#: transformers.models.led.modeling_led.LEDEncoderBaseModelOutput:29
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:46
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:48
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:48
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:46
#: transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:28
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:46
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:48
msgid ""
"Global attentions weights after the attention softmax, used to compute "
"the weighted average in the self-attention heads. Those are the attention"
" weights from every token with global attention to every token in the "
"sequence."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:1
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:1
msgid ""
"Base class for model encoder's outputs that also contains : pre-computed "
"hidden states that can speed up sequential decoding."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:4
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:4
msgid ""
"Sequence of hidden-states at the output of the last layer of the decoder "
"of the model.  If :obj:`past_key_values` is used only the last hidden-"
"state of the sequences of shape :obj:`(batch_size, 1, hidden_size)` is "
"output."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:4
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:4
msgid ""
"Sequence of hidden-states at the output of the last layer of the decoder "
"of the model."
msgstr ""

#: of transformers.LEDModel.forward:115 transformers.TFLEDModel.call:79
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:6
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:6
msgid ""
"If :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size, 1, hidden_size)` is output."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:7
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:9
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:9
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:7
msgid ""
"List of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with "
"each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`).  Contains pre-computed hidden-states (key and "
"values in the attention blocks) of the decoder that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:7
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:9
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:9
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:7
msgid ""
"List of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with "
"each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`)."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:82
#: transformers.TFLEDModel.call:84
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:10
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:12
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:12
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:10
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:10
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:12
msgid ""
"Contains pre-computed hidden-states (key and values in the attention "
"blocks) of the decoder that can be used (see :obj:`past_key_values` "
"input) to speed up sequential decoding."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:13
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:15
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:15
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:13
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the decoder at the "
"output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:128
#: transformers.LEDForQuestionAnswering.forward:133
#: transformers.LEDForSequenceClassification.forward:127
#: transformers.LEDModel.forward:126
#: transformers.TFLEDForConditionalGeneration.call:87
#: transformers.TFLEDModel.call:89
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:16
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:18
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:18
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:16
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:16
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:18
msgid ""
"Hidden-states of the decoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:18
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:20
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:20
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:18
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:18
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:24
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:37
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:20
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:26
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:39
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:20
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:26
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:39
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:18
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:24
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:37
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:132
#: transformers.LEDForQuestionAnswering.forward:137
#: transformers.LEDForSequenceClassification.forward:131
#: transformers.LEDModel.forward:130
#: transformers.TFLEDForConditionalGeneration.call:91
#: transformers.TFLEDModel.call:93
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:21
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:23
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:23
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:21
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:21
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:23
msgid ""
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:24
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:26
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:26
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:24
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:137
#: transformers.LEDForQuestionAnswering.forward:142
#: transformers.LEDForSequenceClassification.forward:136
#: transformers.LEDModel.forward:135
#: transformers.TFLEDForConditionalGeneration.call:96
#: transformers.TFLEDModel.call:98
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:27
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:29
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:29
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:27
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:27
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:29
msgid ""
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:30
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:32
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:32
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:30
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:30
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:32
msgid ""
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:32
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:34
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:34
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:32
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the encoder at the "
"output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:143
#: transformers.LEDForQuestionAnswering.forward:148
#: transformers.LEDForSequenceClassification.forward:142
#: transformers.LEDModel.forward:141
#: transformers.TFLEDForConditionalGeneration.call:102
#: transformers.TFLEDModel.call:104
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:35
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:37
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:37
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:35
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:35
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:37
msgid ""
"Hidden-states of the encoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:37
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:39
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:39
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:37
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:147
#: transformers.LEDForQuestionAnswering.forward:152
#: transformers.LEDForSequenceClassification.forward:146
#: transformers.LEDModel.forward:145
#: transformers.TFLEDForConditionalGeneration.call:106
#: transformers.TFLEDModel.call:108
#: transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:40
#: transformers.models.led.modeling_led.LEDSeq2SeqModelOutput:42
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:42
#: transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:40
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:40
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:42
msgid ""
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:1
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:1
msgid "Base class for sequence-to-sequence language models outputs."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:3
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:3
msgid "Language modeling loss."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqLMOutput:5
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:5
msgid ""
"Prediction scores of the language modeling head (scores for each "
"vocabulary token before SoftMax)."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:1
msgid ""
"Base class for outputs of sequence-to-sequence sentence classification "
"models."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:3
msgid "Classification (or regression if config.num_labels==1) loss."
msgstr ""

#: of transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput:5
msgid ""
"Classification (or regression if config.num_labels==1) scores (before "
"SoftMax)."
msgstr ""

#: of
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:1
msgid "Base class for outputs of sequence-to-sequence question answering models."
msgstr ""

#: of
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:3
msgid ""
"Total span extraction loss is the sum of a Cross-Entropy for the start "
"and end positions."
msgstr ""

#: of
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:5
msgid "Span-start scores (before SoftMax)."
msgstr ""

#: of
#: transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput:7
msgid "Span-end scores (before SoftMax)."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:1
msgid ""
"Base class for Longformer's outputs, with potential hidden states, local "
"and global attentions."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:5
msgid ""
"Tuple of :obj:`tf.Tensor` (one for the output of the embeddings + one for"
" the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.  Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:5
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:13
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:32
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:15
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:34
msgid ""
"Tuple of :obj:`tf.Tensor` (one for the output of the embeddings + one for"
" the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:10
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x + attention_window + "
"1)`, where ``x`` is the number of tokens with global attention mask.  "
"Local attentions weights after the attention softmax, used to compute the"
" weighted average in the self-attention heads. Those are the attention "
"weights from every token in the sequence to every token with global "
"attention (first ``x`` values) and to every token in the attention window"
" (remaining ``attention_window + 1`` values). Note that the first ``x`` "
"values refer to tokens with fixed positions in the text, but the "
"remaining ``attention_window + 1`` values refer to tokens with relative "
"positions: the attention weight of a token to itself is located at index "
"``x + attention_window / 2`` and the ``attention_window / 2`` preceding "
"(succeeding) values are the attention weights to the ``attention_window /"
" 2`` preceding (succeeding) tokens. If the attention window contains a "
"token with global attention, the attention weight at the corresponding "
"index is set to 0; the value should be accessed from the first ``x`` "
"attention weights. If a token has global attention, the attention weights"
" to all other tokens in :obj:`attentions` is set to 0, the values should "
"be accessed from :obj:`global_attentions`."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:10
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x + attention_window + "
"1)`, where ``x`` is the number of tokens with global attention mask."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:25
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:43
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:45
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x)`, where ``x`` is the "
"number of tokens with global attention mask.  Global attentions weights "
"after the attention softmax, used to compute the weighted average in the "
"self-attention heads. Those are the attention weights from every token "
"with global attention to every token in the sequence."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput:25
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:43
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:45
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, x)`, where ``x`` is the "
"number of tokens with global attention mask."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:7
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:9
msgid ""
"List of :obj:`tf.Tensor` of length :obj:`config.n_layers`, with each "
"tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`).  Contains pre-computed hidden-states (key and "
"values in the attention blocks) of the decoder that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:7
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:9
msgid ""
"List of :obj:`tf.Tensor` of length :obj:`config.n_layers`, with each "
"tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`)."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:13
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:15
msgid ""
"Tuple of :obj:`tf.Tensor` (one for the output of the embeddings + one for"
" the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.  Hidden-states of the decoder at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:18
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:20
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:18
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:24
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:37
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:20
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:26
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:39
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:24
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:26
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:32
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:34
msgid ""
"Tuple of :obj:`tf.Tensor` (one for the output of the embeddings + one for"
" the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.  Hidden-states of the encoder at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput:37
#: transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput:39
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: ../../source/model_doc/led.rst:112
msgid "LEDModel"
msgstr ""

#: of transformers.LEDModel:1
msgid ""
"The bare LED Model outputting raw hidden-states without any specific head"
" on top. This model inherits from :class:`~transformers.PreTrainedModel`."
" Check the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.LEDForConditionalGeneration:6
#: transformers.LEDForQuestionAnswering:8
#: transformers.LEDForSequenceClassification:8 transformers.LEDModel:6
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.LEDForConditionalGeneration:10
#: transformers.LEDForQuestionAnswering:12
#: transformers.LEDForSequenceClassification:12 transformers.LEDModel:10
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.LEDModel.forward:1
msgid ""
"The :class:`~transformers.LEDModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:4
#: transformers.LEDForQuestionAnswering.forward:4
#: transformers.LEDForSequenceClassification.forward:4
#: transformers.LEDModel.forward:4
#: transformers.TFLEDForConditionalGeneration.call:4
#: transformers.TFLEDModel.call:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:8
#: transformers.LEDForQuestionAnswering.forward:8
#: transformers.LEDForSequenceClassification.forward:8
#: transformers.LEDModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. Padding will be "
"ignored by default should you provide it.  Indices can be obtained using "
":class:`~transformers.LEDTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:8
#: transformers.LEDForQuestionAnswering.forward:8
#: transformers.LEDForSequenceClassification.forward:8
#: transformers.LEDModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. Padding will be "
"ignored by default should you provide it."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:11
#: transformers.LEDForQuestionAnswering.forward:11
#: transformers.LEDForSequenceClassification.forward:11
#: transformers.LEDModel.forward:11
msgid ""
"Indices can be obtained using :class:`~transformers.LEDTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:15
#: transformers.LEDForConditionalGeneration.forward:30
#: transformers.LEDForQuestionAnswering.forward:15
#: transformers.LEDForQuestionAnswering.forward:30
#: transformers.LEDForSequenceClassification.forward:15
#: transformers.LEDForSequenceClassification.forward:30
#: transformers.LEDModel.forward:15 transformers.LEDModel.forward:30
#: transformers.TFLEDForConditionalGeneration.call:14
#: transformers.TFLEDForConditionalGeneration.call:29
#: transformers.TFLEDModel.call:14 transformers.TFLEDModel.call:29
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:17
#: transformers.LEDForQuestionAnswering.forward:17
#: transformers.LEDForSequenceClassification.forward:17
#: transformers.LEDModel.forward:17
#: transformers.TFLEDForConditionalGeneration.call:16
#: transformers.TFLEDModel.call:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:17
#: transformers.LEDForQuestionAnswering.forward:17
#: transformers.LEDForSequenceClassification.forward:17
#: transformers.LEDModel.forward:17
#: transformers.TFLEDForConditionalGeneration.call:16
#: transformers.TFLEDModel.call:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:19
#: transformers.LEDForQuestionAnswering.forward:19
#: transformers.LEDForSequenceClassification.forward:19
#: transformers.LEDModel.forward:19
#: transformers.TFLEDForConditionalGeneration.call:18
#: transformers.TFLEDModel.call:18
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:20
#: transformers.LEDForQuestionAnswering.forward:20
#: transformers.LEDForSequenceClassification.forward:20
#: transformers.LEDModel.forward:20
#: transformers.TFLEDForConditionalGeneration.call:19
#: transformers.TFLEDModel.call:19
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:22
#: transformers.LEDForQuestionAnswering.forward:22
#: transformers.LEDForSequenceClassification.forward:22
#: transformers.LEDModel.forward:22
#: transformers.TFLEDForConditionalGeneration.call:21
#: transformers.TFLEDModel.call:21
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:24
#: transformers.LEDForQuestionAnswering.forward:24
#: transformers.LEDForSequenceClassification.forward:24
#: transformers.LEDModel.forward:24
#: transformers.TFLEDForConditionalGeneration.call:23
#: transformers.TFLEDModel.call:23
msgid ""
"Indices of decoder input sequence tokens in the vocabulary.  Indices can "
"be obtained using :class:`~transformers.LedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__  LED uses the "
":obj:`eos_token_id` as the starting token for :obj:`decoder_input_ids` "
"generation. If :obj:`past_key_values` is used, optionally only the last "
":obj:`decoder_input_ids` have to be input (see :obj:`past_key_values`)."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:24
#: transformers.LEDForQuestionAnswering.forward:24
#: transformers.LEDForSequenceClassification.forward:24
#: transformers.LEDModel.forward:24
#: transformers.TFLEDForConditionalGeneration.call:23
#: transformers.TFLEDModel.call:23
msgid "Indices of decoder input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:26
#: transformers.LEDForQuestionAnswering.forward:26
#: transformers.LEDForSequenceClassification.forward:26
#: transformers.LEDModel.forward:26
#: transformers.TFLEDForConditionalGeneration.call:25
#: transformers.TFLEDModel.call:25
msgid ""
"Indices can be obtained using :class:`~transformers.LedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:32
#: transformers.LEDForQuestionAnswering.forward:32
#: transformers.LEDForSequenceClassification.forward:32
#: transformers.LEDModel.forward:32
#: transformers.TFLEDForConditionalGeneration.call:31
#: transformers.TFLEDModel.call:31
msgid ""
"LED uses the :obj:`eos_token_id` as the starting token for "
":obj:`decoder_input_ids` generation. If :obj:`past_key_values` is used, "
"optionally only the last :obj:`decoder_input_ids` have to be input (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:36
#: transformers.LEDForQuestionAnswering.forward:36
#: transformers.LEDForSequenceClassification.forward:36
#: transformers.LEDModel.forward:36
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default.  If "
"you want to change padding behavior, you should read "
":func:`modeling_led._prepare_decoder_inputs` and modify to your needs. "
"See diagram 1 in `the paper <https://arxiv.org/abs/1910.13461>`__ for "
"more information on the default strategy."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:36
#: transformers.LEDForQuestionAnswering.forward:36
#: transformers.LEDForSequenceClassification.forward:36
#: transformers.LEDModel.forward:36
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:39
#: transformers.LEDForQuestionAnswering.forward:39
#: transformers.LEDForSequenceClassification.forward:39
#: transformers.LEDModel.forward:39
msgid ""
"If you want to change padding behavior, you should read "
":func:`modeling_led._prepare_decoder_inputs` and modify to your needs. "
"See diagram 1 in `the paper <https://arxiv.org/abs/1910.13461>`__ for "
"more information on the default strategy."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:43
#: transformers.LEDForQuestionAnswering.forward:43
#: transformers.LEDForSequenceClassification.forward:43
#: transformers.LEDModel.forward:43
msgid ""
"Mask to decide the attention given on each token, local attention or "
"global attention for the encoder. Tokens with global attention attends to"
" all other tokens, and all other tokens attend to them. This is important"
" for task-specific finetuning because it makes the model more flexible at"
" representing the task. For example, for classification, the <s> token "
"should be given global attention. For QA, all question tokens should also"
" have global attention. Please refer to the `Longformer paper "
"<https://arxiv.org/abs/2004.05150>`__ for more details. Mask values "
"selected in ``[0, 1]``:  - 0 for local attention (a sliding window "
"attention), - 1 for global attention (tokens that attend to all other "
"tokens, and all other tokens attend to them)."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:43
#: transformers.LEDForQuestionAnswering.forward:43
#: transformers.LEDForSequenceClassification.forward:43
#: transformers.LEDModel.forward:43
msgid ""
"Mask to decide the attention given on each token, local attention or "
"global attention for the encoder. Tokens with global attention attends to"
" all other tokens, and all other tokens attend to them. This is important"
" for task-specific finetuning because it makes the model more flexible at"
" representing the task. For example, for classification, the <s> token "
"should be given global attention. For QA, all question tokens should also"
" have global attention. Please refer to the `Longformer paper "
"<https://arxiv.org/abs/2004.05150>`__ for more details. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:50
#: transformers.LEDForQuestionAnswering.forward:50
#: transformers.LEDForSequenceClassification.forward:50
#: transformers.LEDModel.forward:50
msgid "0 for local attention (a sliding window attention),"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:51
#: transformers.LEDForQuestionAnswering.forward:51
#: transformers.LEDForSequenceClassification.forward:51
#: transformers.LEDModel.forward:51
msgid ""
"1 for global attention (tokens that attend to all other tokens, and all "
"other tokens attend to them)."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:53
#: transformers.LEDForQuestionAnswering.forward:53
#: transformers.LEDForSequenceClassification.forward:53
#: transformers.LEDModel.forward:53
#: transformers.TFLEDForConditionalGeneration.call:37
#: transformers.TFLEDModel.call:37
msgid ""
"Mask to nullify selected heads of the attention modules in the encoder. "
"Mask values selected in ``[0, 1]``:  - 1 indicates the head is **not "
"masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:53
#: transformers.LEDForQuestionAnswering.forward:53
#: transformers.LEDForSequenceClassification.forward:53
#: transformers.LEDModel.forward:53
#: transformers.TFLEDForConditionalGeneration.call:37
#: transformers.TFLEDModel.call:37
msgid ""
"Mask to nullify selected heads of the attention modules in the encoder. "
"Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:55
#: transformers.LEDForConditionalGeneration.forward:60
#: transformers.LEDForConditionalGeneration.forward:66
#: transformers.LEDForQuestionAnswering.forward:55
#: transformers.LEDForQuestionAnswering.forward:60
#: transformers.LEDForQuestionAnswering.forward:66
#: transformers.LEDForSequenceClassification.forward:55
#: transformers.LEDForSequenceClassification.forward:60
#: transformers.LEDForSequenceClassification.forward:66
#: transformers.LEDModel.forward:55 transformers.LEDModel.forward:60
#: transformers.LEDModel.forward:66
#: transformers.TFLEDForConditionalGeneration.call:39
#: transformers.TFLEDForConditionalGeneration.call:44
#: transformers.TFLEDModel.call:39 transformers.TFLEDModel.call:44
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:56
#: transformers.LEDForConditionalGeneration.forward:61
#: transformers.LEDForConditionalGeneration.forward:67
#: transformers.LEDForQuestionAnswering.forward:56
#: transformers.LEDForQuestionAnswering.forward:61
#: transformers.LEDForQuestionAnswering.forward:67
#: transformers.LEDForSequenceClassification.forward:56
#: transformers.LEDForSequenceClassification.forward:61
#: transformers.LEDForSequenceClassification.forward:67
#: transformers.LEDModel.forward:56 transformers.LEDModel.forward:61
#: transformers.LEDModel.forward:67
#: transformers.TFLEDForConditionalGeneration.call:40
#: transformers.TFLEDForConditionalGeneration.call:45
#: transformers.TFLEDModel.call:40 transformers.TFLEDModel.call:45
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:58
#: transformers.LEDForQuestionAnswering.forward:58
#: transformers.LEDForSequenceClassification.forward:58
#: transformers.LEDModel.forward:58
#: transformers.TFLEDForConditionalGeneration.call:42
#: transformers.TFLEDModel.call:42
msgid ""
"Mask to nullify selected heads of the attention modules in the decoder. "
"Mask values selected in ``[0, 1]``:  - 1 indicates the head is **not "
"masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:58
#: transformers.LEDForQuestionAnswering.forward:58
#: transformers.LEDForSequenceClassification.forward:58
#: transformers.LEDModel.forward:58
#: transformers.TFLEDForConditionalGeneration.call:42
#: transformers.TFLEDModel.call:42
msgid ""
"Mask to nullify selected heads of the attention modules in the decoder. "
"Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:63
#: transformers.LEDForQuestionAnswering.forward:63
#: transformers.LEDForSequenceClassification.forward:63
#: transformers.LEDModel.forward:63
msgid ""
"Mask to nullify selected heads of the cross-attention modules in the "
"decoder. Mask values selected in ``[0, 1]``:  - 1 indicates the head is "
"**not masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:63
#: transformers.LEDForQuestionAnswering.forward:63
#: transformers.LEDForSequenceClassification.forward:63
#: transformers.LEDModel.forward:63
msgid ""
"Mask to nullify selected heads of the cross-attention modules in the "
"decoder. Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:69
#: transformers.LEDForQuestionAnswering.forward:69
#: transformers.LEDForSequenceClassification.forward:69
#: transformers.LEDModel.forward:69
msgid ""
"Tuple consists of (:obj:`last_hidden_state`, `optional`: "
":obj:`hidden_states`, `optional`: :obj:`attentions`) "
":obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) is a sequence of hidden-states at the output "
"of the last layer of the encoder. Used in the cross-attention of the "
"decoder."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:74
#: transformers.LEDForQuestionAnswering.forward:74
#: transformers.LEDForSequenceClassification.forward:74
#: transformers.LEDModel.forward:74
msgid ""
"Tuple of :obj:`tuple(torch.FloatTensor)` of length "
":obj:`config.n_layers`, with each tuple having 2 tensors of shape "
":obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and"
" 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`.  Contains pre-computed "
"hidden-states (key and values in the self-attention blocks and in the "
"cross-attention blocks) that can be used (see :obj:`past_key_values` "
"input) to speed up sequential decoding.  If :obj:`past_key_values` are "
"used, the user can optionally input only the last "
":obj:`decoder_input_ids` (those that don't have their past key value "
"states given to this model) of shape :obj:`(batch_size, 1)` instead of "
"all :obj:`decoder_input_ids`` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:74
#: transformers.LEDForQuestionAnswering.forward:74
#: transformers.LEDForSequenceClassification.forward:74
#: transformers.LEDModel.forward:74
msgid ""
"Tuple of :obj:`tuple(torch.FloatTensor)` of length "
":obj:`config.n_layers`, with each tuple having 2 tensors of shape "
":obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and"
" 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:78
#: transformers.LEDForConditionalGeneration.forward:123
#: transformers.LEDForQuestionAnswering.forward:78
#: transformers.LEDForQuestionAnswering.forward:128
#: transformers.LEDForSequenceClassification.forward:78
#: transformers.LEDForSequenceClassification.forward:122
#: transformers.LEDModel.forward:78 transformers.LEDModel.forward:121
msgid ""
"Contains pre-computed hidden-states (key and values in the self-attention"
" blocks and in the cross-attention blocks) that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:81
#: transformers.LEDForQuestionAnswering.forward:81
#: transformers.LEDForSequenceClassification.forward:81
#: transformers.LEDModel.forward:81
msgid ""
"If :obj:`past_key_values` are used, the user can optionally input only "
"the last :obj:`decoder_input_ids` (those that don't have their past key "
"value states given to this model) of shape :obj:`(batch_size, 1)` instead"
" of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:85
#: transformers.LEDForQuestionAnswering.forward:85
#: transformers.LEDForSequenceClassification.forward:85
#: transformers.LEDModel.forward:85
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:89
#: transformers.LEDForQuestionAnswering.forward:89
#: transformers.LEDForSequenceClassification.forward:89
#: transformers.LEDModel.forward:89
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. If :obj:`past_key_values` is "
"used, optionally only the last :obj:`decoder_inputs_embeds` have to be "
"input (see :obj:`past_key_values`). This is useful if you want more "
"control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix.  If"
" :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both "
"unset, :obj:`decoder_inputs_embeds` takes the value of "
":obj:`inputs_embeds`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:89
#: transformers.LEDForQuestionAnswering.forward:89
#: transformers.LEDForSequenceClassification.forward:89
#: transformers.LEDModel.forward:89
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. If :obj:`past_key_values` is "
"used, optionally only the last :obj:`decoder_inputs_embeds` have to be "
"input (see :obj:`past_key_values`). This is useful if you want more "
"control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:94
#: transformers.LEDForQuestionAnswering.forward:94
#: transformers.LEDForSequenceClassification.forward:94
#: transformers.LEDModel.forward:94
msgid ""
"If :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both "
"unset, :obj:`decoder_inputs_embeds` takes the value of "
":obj:`inputs_embeds`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:97
#: transformers.LEDForQuestionAnswering.forward:97
#: transformers.LEDForSequenceClassification.forward:97
#: transformers.LEDModel.forward:97
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:100
#: transformers.LEDForQuestionAnswering.forward:100
#: transformers.LEDForSequenceClassification.forward:100
#: transformers.LEDModel.forward:100
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:103
#: transformers.LEDForQuestionAnswering.forward:103
#: transformers.LEDForSequenceClassification.forward:103
#: transformers.LEDModel.forward:103
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:106
#: transformers.LEDForQuestionAnswering.forward:106
#: transformers.LEDForSequenceClassification.forward:106
#: transformers.LEDModel.forward:106
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.LEDModel.forward:109
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or a tuple "
"of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.LEDConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model.    If"
" :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size,   1, hidden_size)` is output. - "
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads."
msgstr ""

#: of transformers.LEDModel.forward:109
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or a tuple "
"of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.LEDConfig`) and inputs."
msgstr ""

#: of transformers.LEDModel.forward:113
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:119
#: transformers.LEDForQuestionAnswering.forward:124
#: transformers.LEDForSequenceClassification.forward:118
#: transformers.LEDModel.forward:117
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors of shape"
" :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) "
"and 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:125
#: transformers.LEDForQuestionAnswering.forward:130
#: transformers.LEDForSequenceClassification.forward:124
#: transformers.LEDModel.forward:123
msgid ""
"**decoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:129
#: transformers.LEDForQuestionAnswering.forward:134
#: transformers.LEDForSequenceClassification.forward:128
#: transformers.LEDModel.forward:127
msgid ""
"**decoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:134
#: transformers.LEDForQuestionAnswering.forward:139
#: transformers.LEDForSequenceClassification.forward:133
#: transformers.LEDModel.forward:132
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:139
#: transformers.LEDForQuestionAnswering.forward:144
#: transformers.LEDForSequenceClassification.forward:138
#: transformers.LEDModel.forward:137
msgid ""
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:140
#: transformers.LEDForQuestionAnswering.forward:145
#: transformers.LEDForSequenceClassification.forward:139
#: transformers.LEDModel.forward:138
msgid ""
"**encoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:144
#: transformers.LEDForQuestionAnswering.forward:149
#: transformers.LEDForSequenceClassification.forward:143
#: transformers.LEDModel.forward:142
msgid ""
"**encoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.LEDModel.forward:147
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:156
#: transformers.LEDForSequenceClassification.forward:150
#: transformers.LEDModel.forward:149 transformers.TFLEDModel.call:118
msgid "Example::"
msgstr ""

#: ../../source/model_doc/led.rst:119
msgid "LEDForConditionalGeneration"
msgstr ""

#: of transformers.LEDForConditionalGeneration:1
msgid ""
"The LED Model with a language modeling head. Can be used for "
"summarization. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:1
msgid ""
"The :class:`~transformers.LEDForConditionalGeneration` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:108
msgid ""
"Labels for computing the masked language modeling loss. Indices should "
"either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` "
"docstring). Tokens with indices set to ``-100`` are ignored (masked), the"
" loss is only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:113
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.LEDConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss. - "
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
" - **past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads.   Conditional generation example::      >>> from"
" transformers import LEDTokenizer, LEDForConditionalGeneration     >>> "
"tokenizer = LEDTokenizer.from_pretrained('allenai/led-base-16384')     "
">>> TXT = \"My friends are <mask> but they eat too many carbs.\"      >>>"
" model = LEDForConditionalGeneration.from_pretrained('allenai/led-"
"base-16384')     >>> input_ids = tokenizer([TXT], "
"return_tensors='pt')['input_ids']      >>> prediction = "
"model.generate(input_ids)[0]     >>> print(tokenizer.decode(prediction, "
"skip_special_tokens=True))"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:113
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.LEDConfig`) and inputs."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:117
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:118
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:151
msgid "Conditional generation example::"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:162
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.LEDForConditionalGeneration.forward:164
msgid "Summarization example::"
msgstr ""

#: ../../source/model_doc/led.rst:126
msgid "LEDForSequenceClassification"
msgstr ""

#: of transformers.LEDForSequenceClassification:1
msgid ""
"LED model with a sequence classification/head on top (a linear layer on "
"top of the pooled output) e.g. for GLUE tasks."
msgstr ""

#: of transformers.LEDForQuestionAnswering:4
#: transformers.LEDForSequenceClassification:4
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:1
msgid ""
"The :class:`~transformers.LEDForSequenceClassification` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:108
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels > 1` a classification loss is computed (Cross-"
"Entropy)."
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:112
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.LEDConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`label` is provided) -- Classification (or"
" regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **past_key_values** "
"(:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- Tuple "
"of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with"
" each tuple having 2 tensors   of shape :obj:`(batch_size, num_heads, "
"sequence_length, embed_size_per_head)`) and 2 additional tensors of   "
"shape :obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads."
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:112
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.LEDConfig`) and "
"inputs."
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:116
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`label` is provided) -- Classification (or regression "
"if config.num_labels==1) loss."
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:117
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.LEDForSequenceClassification.forward:148
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput` "
"or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/led.rst:133
msgid "LEDForQuestionAnswering"
msgstr ""

#: of transformers.LEDForQuestionAnswering:1
msgid ""
"LED Model with a span classification head on top for extractive question-"
"answering tasks like SQuAD (a linear layer on top of the hidden-states "
"output to compute `span start logits` and `span end logits`)."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:1
msgid ""
"The :class:`~transformers.LEDForQuestionAnswering` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:108
msgid ""
"Labels for position (index) of the start of the labelled span for "
"computing the token classification loss. Positions are clamped to the "
"length of the sequence (`sequence_length`). Position outside of the "
"sequence are not taken into account for computing the loss."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:112
msgid ""
"Labels for position (index) of the end of the labelled span for computing"
" the token classification loss. Positions are clamped to the length of "
"the sequence (`sequence_length`). Position outside of the sequence are "
"not taken into account for computing the loss."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:117
msgid ""
"A "
":class:`~transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.LEDConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Total span "
"extraction loss is the sum of a Cross-Entropy for the start and end "
"positions. - **start_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Span-start scores (before "
"SoftMax). - **end_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Span-end scores (before "
"SoftMax). - **past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:117
msgid ""
"A "
":class:`~transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.LEDConfig`) and "
"inputs."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:121
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Total span extraction loss is"
" the sum of a Cross-Entropy for the start and end positions."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:122
msgid ""
"**start_logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Span-start scores (before SoftMax)."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:123
msgid ""
"**end_logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Span-end scores (before SoftMax)."
msgstr ""

#: of transformers.LEDForQuestionAnswering.forward:154
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/led.rst:140
msgid "TFLEDModel"
msgstr ""

#: of transformers.TFLEDModel:1
msgid ""
"The bare LED Model outputting raw hidden-states without any specific head"
" on top. This model inherits from "
":class:`~transformers.TFPreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:6 transformers.TFLEDModel:6
msgid ""
"This model is also a `tf.keras.Model "
"<https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. "
"Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:12 transformers.TFLEDModel:12
msgid "TF 2.0 models accepts two formats as inputs:"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:14 transformers.TFLEDModel:14
msgid "having all inputs as keyword arguments (like PyTorch models), or"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:15 transformers.TFLEDModel:15
msgid ""
"having all inputs as a list, tuple or dict in the first positional "
"arguments."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:17 transformers.TFLEDModel:17
msgid ""
"This second option is useful when using :meth:`tf.keras.Model.fit` method"
" which currently requires having all the tensors in the first argument of"
" the model call function: :obj:`model(inputs)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:20 transformers.TFLEDModel:20
msgid ""
"If you choose this second option, there are three possibilities you can "
"use to gather all the input Tensors in the first positional argument :"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:23 transformers.TFLEDModel:23
msgid ""
"a single Tensor with :obj:`input_ids` only and nothing else: "
":obj:`model(input_ids)`"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:24 transformers.TFLEDModel:24
msgid ""
"a list of varying length with one or several input Tensors IN THE ORDER "
"given in the docstring: :obj:`model([input_ids, attention_mask])` or "
":obj:`model([input_ids, attention_mask, token_type_ids])`"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:26 transformers.TFLEDModel:26
msgid ""
"a dictionary with one or several input Tensors associated to the input "
"names given in the docstring: :obj:`model({\"input_ids\": input_ids, "
"\"token_type_ids\": token_type_ids})`"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:29 transformers.TFLEDModel:29
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.TFPreTrainedModel.from_pretrained` method to load "
"the model weights."
msgstr ""

#: of transformers.TFLEDModel.call:1
msgid ""
"The :class:`~transformers.TFLEDModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:8
#: transformers.TFLEDModel.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.BertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:8
#: transformers.TFLEDModel.call:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:10
#: transformers.TFLEDModel.call:10
msgid ""
"Indices can be obtained using :class:`~transformers.BertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:35
#: transformers.TFLEDModel.call:35
msgid ""
"will be made by default and ignore pad tokens. It is not recommended to "
"set this for most use cases."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:47
#: transformers.TFLEDModel.call:47
msgid ""
"hidden states at the output of the last layer of the encoder. Used in the"
" cross-attention of the decoder. of shape :obj:`(batch_size, "
"sequence_length, hidden_size)` is a sequence of"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:50
#: transformers.TFLEDModel.call:50
msgid ""
"contains precomputed key and value hidden states of the attention blocks."
" Can be used to speed up decoding. If :obj:`past_key_values` are used, "
"the user can optionally input only the last :obj:`decoder_input_ids` "
"(those that don't have their past key value states given to this model) "
"of shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids` "
"of shape :obj:`(batch_size, sequence_length)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:55
#: transformers.TFLEDModel.call:55
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`). Set to :obj:`False` during training, :obj:`True`"
" during generation"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:58
#: transformers.TFLEDModel.call:58
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:62
#: transformers.TFLEDModel.call:62
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:66
#: transformers.TFLEDModel.call:66
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple. This argument can be used in eager mode, in "
"graph mode the value will always be set to True."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:69
#: transformers.TFLEDModel.call:69
msgid ""
"Whether or not to use the model in training mode (some modules like "
"dropout modules have different behaviors between training and "
"evaluation)."
msgstr ""

#: of transformers.TFLEDModel.call:73
msgid ""
"A "
":class:`~transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.LEDConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the decoder of the model.    If "
":obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size,   1, hidden_size)` is output. - "
"**past_key_values** (:obj:`List[tf.Tensor]`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- List "
"of :obj:`tf.Tensor` of length :obj:`config.n_layers`, with each tensor of"
" shape :obj:`(2, batch_size,   num_heads, sequence_length, "
"embed_size_per_head)`).    Contains pre-computed hidden-states (key and "
"values in the attention blocks) of the decoder that can be   used (see "
":obj:`past_key_values` input) to speed up sequential decoding. - "
"**decoder_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the decoder at the output of each layer plus the initial "
"embedding outputs. - **decoder_attentions** (:obj:`tuple(tf.Tensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the decoder, after the "
"attention softmax, used to compute the weighted average in the   self-"
"attention heads. - **cross_attentions** (:obj:`tuple(tf.Tensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the decoder's cross-attention"
" layer, after the attention softmax, used to compute the   weighted "
"average in the cross-attention heads. - **encoder_last_hidden_state** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) -- Sequence of hidden-states at the output of "
"the last layer of the encoder of the model. - **encoder_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the encoder at the output of each layer plus the initial "
"embedding outputs. - **encoder_attentions** (:obj:`tuple(tf.Tensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the encoder, after the "
"attention softmax, used to compute the weighted average in the   self-"
"attention heads. - **encoder_global_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, x)`,"
"   where ``x`` is the number of tokens with global attention mask.    "
"Global attentions weights after the attention softmax, used to compute "
"the weighted average in the   self-attention heads. Those are the "
"attention weights from every token with global attention to every token"
"   in the sequence."
msgstr ""

#: of transformers.TFLEDModel.call:73
msgid ""
"A "
":class:`~transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.LEDConfig`) and inputs."
msgstr ""

#: of transformers.TFLEDModel.call:77
msgid ""
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the decoder of the model."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:79
#: transformers.TFLEDModel.call:81
msgid ""
"**past_key_values** (:obj:`List[tf.Tensor]`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- List "
"of :obj:`tf.Tensor` of length :obj:`config.n_layers`, with each tensor of"
" shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`)."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:84
#: transformers.TFLEDModel.call:86
msgid ""
"**decoder_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:88
#: transformers.TFLEDModel.call:90
msgid ""
"**decoder_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:93
#: transformers.TFLEDModel.call:95
msgid ""
"**cross_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:98
#: transformers.TFLEDModel.call:100
msgid ""
"**encoder_last_hidden_state** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:99
#: transformers.TFLEDModel.call:101
msgid ""
"**encoder_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:103
#: transformers.TFLEDModel.call:105
msgid ""
"**encoder_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:108
#: transformers.TFLEDModel.call:110
msgid ""
"**encoder_global_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, x)`, "
"where ``x`` is the number of tokens with global attention mask."
msgstr ""

#: of transformers.TFLEDModel.call:116
msgid ""
":class:`~transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/led.rst:147
msgid "TFLEDForConditionalGeneration"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration:1
msgid ""
"The LED Model with a language modeling head. Can be used for "
"summarization. This model inherits from "
":class:`~transformers.TFPreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:1
msgid ""
"The :class:`~transformers.TFLEDForConditionalGeneration` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:73
msgid ""
"A :class:`~transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput` "
"or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.LEDConfig`) and inputs.  - "
"**loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Language modeling loss. - **logits** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax). - **past_key_values** "
"(:obj:`List[tf.Tensor]`, `optional`, returned when ``use_cache=True`` is "
"passed or when ``config.use_cache=True``) -- List of :obj:`tf.Tensor` of "
"length :obj:`config.n_layers`, with each tensor of shape :obj:`(2, "
"batch_size,   num_heads, sequence_length, embed_size_per_head)`).    "
"Contains pre-computed hidden-states (key and values in the attention "
"blocks) of the decoder that can be   used (see :obj:`past_key_values` "
"input) to speed up sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the decoder at the output of each layer plus the initial "
"embedding outputs. - **decoder_attentions** (:obj:`tuple(tf.Tensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the decoder, after the "
"attention softmax, used to compute the weighted average in the   self-"
"attention heads. - **cross_attentions** (:obj:`tuple(tf.Tensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the decoder's cross-attention"
" layer, after the attention softmax, used to compute the   weighted "
"average in the cross-attention heads. - **encoder_last_hidden_state** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) -- Sequence of hidden-states at the output of "
"the last layer of the encoder of the model. - **encoder_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the encoder at the output of each layer plus the initial "
"embedding outputs. - **encoder_attentions** (:obj:`tuple(tf.Tensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the encoder, after the "
"attention softmax, used to compute the weighted average in the   self-"
"attention heads. - **encoder_global_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, x)`,"
"   where ``x`` is the number of tokens with global attention mask.    "
"Global attentions weights after the attention softmax, used to compute "
"the weighted average in the   self-attention heads. Those are the "
"attention weights from every token with global attention to every token"
"   in the sequence.   Examples::      >>> from transformers import "
"LEDTokenizer, TFLEDForConditionalGeneration     >>> import tensorflow as "
"tf     >>> mname = 'allenai/led-base-16384'     >>> tokenizer = "
"LEDTokenizer.from_pretrained(mname)     >>> TXT = \"My friends are <mask>"
" but they eat too many carbs.\"     >>> model = "
"TFLEDForConditionalGeneration.from_pretrained(mname)     >>> batch = "
"tokenizer([TXT], return_tensors='tf')     >>> logits = "
"model(inputs=batch.input_ids).logits     >>> probs = "
"tf.nn.softmax(logits[0])     >>> # probs[5] is associated with the mask "
"token"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:73
msgid ""
"A :class:`~transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput` "
"or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.LEDConfig`) and inputs."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:77
msgid ""
"**loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:78
msgid ""
"**logits** (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length,"
" config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:116
msgid "Examples::"
msgstr ""

#: of transformers.TFLEDForConditionalGeneration.call:128
msgid ""
":class:`~transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput` or"
" :obj:`tuple(tf.Tensor)`"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/m2m_100.rst:14
msgid "M2M100"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:19
msgid ""
"The M2M100 model was proposed in `Beyond English-Centric Multilingual "
"Machine Translation <https://arxiv.org/abs/2010.11125>`__ by Angela Fan, "
"Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth "
"Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, "
"Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard "
"Grave, Michael Auli, Armand Joulin."
msgstr ""

#: ../../source/model_doc/m2m_100.rst:24
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:26
msgid ""
"*Existing work in translation demonstrated the potential of massively "
"multilingual machine translation by training a single model able to "
"translate between any pair of languages. However, much of this work is "
"English-Centric by training only on data which was translated from or to "
"English. While this is supported by large sources of training data, it "
"does not reflect translation needs worldwide. In this work, we create a "
"true Many-to-Many multilingual translation model that can translate "
"directly between any pair of 100 languages. We build and open source a "
"training dataset that covers thousands of language directions with "
"supervised data, created through large-scale mining. Then, we explore how"
" to effectively increase model capacity through a combination of dense "
"scaling and language-specific sparse parameters to create high quality "
"models. Our focus on non-English-Centric models brings gains of more than"
" 10 BLEU when directly translating between non-English directions while "
"performing competitively to the best single systems of WMT. We open-"
"source our scripts so that others may reproduce the data, evaluation, and"
" final M2M-100 model.*"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:37
msgid ""
"This model was contributed by `valhalla "
"<https://huggingface.co/valhalla>`__."
msgstr ""

#: ../../source/model_doc/m2m_100.rst:41
msgid "Training and Generation"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:43
msgid ""
"M2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily "
"intended for translation tasks. As the model is multilingual it expects "
"the sequences in a certain format: A special language id token is used as"
" prefix in both the source and target text. The source text format is "
":obj:`[lang_code] X [eos]`, where :obj:`lang_code` is source language id "
"for source text and target language id for target text, with :obj:`X` "
"being the source or target text."
msgstr ""

#: ../../source/model_doc/m2m_100.rst:48
msgid ""
"The :class:`~transformers.M2M100Tokenizer` depends on "
":obj:`sentencepiece` so be sure to install it before running the "
"examples. To install :obj:`sentencepiece` run ``pip install "
"sentencepiece``."
msgstr ""

#: ../../source/model_doc/m2m_100.rst:51
msgid "Supervised Training"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:70
msgid "Generation"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:72
msgid ""
"M2M100 uses the :obj:`eos_token_id` as the :obj:`decoder_start_token_id` "
"for generation with the target language id being forced as the first "
"generated token. To force the target language id as the first generated "
"token, pass the `forced_bos_token_id` parameter to the `generate` method."
" The following example shows how to translate between Hindi to French and"
" Chinese to English using the `facebook/m2m100_418M` checkpoint."
msgstr ""

#: ../../source/model_doc/m2m_100.rst:103
msgid "M2M100Config"
msgstr ""

#: of transformers.M2M100Config:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.M2M100Model`. It is used to instantiate an M2M100 "
"model according to the specified arguments, defining the model "
"architecture. Instantiating a configuration with the defaults will yield "
"a similar configuration to that of the M2M100 `m2m100_418M "
"<https://huggingface.co/facebook/m2m100_418M>`__ architecture."
msgstr ""

#: of transformers.M2M100Config:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.M2M100Config transformers.M2M100ForConditionalGeneration
#: transformers.M2M100ForConditionalGeneration.forward transformers.M2M100Model
#: transformers.M2M100Model.forward transformers.M2M100Tokenizer
#: transformers.M2M100Tokenizer.build_inputs_with_special_tokens
#: transformers.M2M100Tokenizer.create_token_type_ids_from_sequences
#: transformers.M2M100Tokenizer.get_special_tokens_mask
#: transformers.M2M100Tokenizer.save_vocabulary
msgid "Parameters"
msgstr ""

#: of transformers.M2M100Config:10
msgid ""
"Vocabulary size of the M2M100 model. Defines the number of different "
"tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.M2M100Model` or"
msgstr ""

#: of transformers.M2M100Config:13
msgid "Dimensionality of the layers and the pooler layer."
msgstr ""

#: of transformers.M2M100Config:15
msgid "Number of encoder layers."
msgstr ""

#: of transformers.M2M100Config:17
msgid "Number of decoder layers."
msgstr ""

#: of transformers.M2M100Config:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.M2M100Config:21
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"decoder."
msgstr ""

#: of transformers.M2M100Config:23 transformers.M2M100Config:25
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in decoder."
msgstr ""

#: of transformers.M2M100Config:27
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.M2M100Config:30
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.M2M100Config:32
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.M2M100Config:34
msgid "The dropout ratio for activations inside the fully connected layer."
msgstr ""

#: of transformers.M2M100Config:36
msgid "The dropout ratio for classifier."
msgstr ""

#: of transformers.M2M100Config:38
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.M2M100Config:41
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.M2M100Config:43
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the encoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.M2M100Config:46
msgid ""
"(:obj:`float`, `optional`, defaults to 0.0): The LayerDrop probability "
"for the decoder. See the `LayerDrop paper <see "
"https://arxiv.org/abs/1909.11556>`__ for more details."
msgstr ""

#: of transformers.M2M100Config:49
msgid ""
"Whether or not the model should return the last key/values attentions "
"(not used by all models)."
msgstr ""

#: of transformers.M2M100Config:51
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.M2M100Config:53
msgid ""
">>> from transformers import M2M100Model, M2M100Config  >>> # "
"Initializing a M2M100 facebook/m2m100_418M style configuration >>> "
"configuration = M2M100Config()  >>> # Initializing a model from the "
"facebook/m2m100_418M style configuration >>> model = "
"M2M100Model(configuration)  >>> # Accessing the model configuration >>> "
"configuration = model.config"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:110
msgid "M2M100Tokenizer"
msgstr ""

#: of transformers.M2M100Tokenizer:1
msgid ""
"Construct an M2M100 tokenizer. Based on `SentencePiece "
"<https://github.com/google/sentencepiece>`__."
msgstr ""

#: of transformers.M2M100Tokenizer:3
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods."
msgstr ""

#: of transformers.M2M100Tokenizer:6
msgid "Path to the vocabulary file."
msgstr ""

#: of transformers.M2M100Tokenizer:8
msgid ""
"Path to `SentencePiece <https://github.com/google/sentencepiece>`__ file "
"(generally has a .spm extension) that contains the vocabulary."
msgstr ""

#: of transformers.M2M100Tokenizer:11
msgid "A string representing the source language."
msgstr ""

#: of transformers.M2M100Tokenizer:13
msgid "A string representing the target language."
msgstr ""

#: of transformers.M2M100Tokenizer:15
msgid "The end of sequence token."
msgstr ""

#: of transformers.M2M100Tokenizer:17
msgid ""
"The separator token, which is used when building a sequence from multiple"
" sequences, e.g. two sequences for sequence classification or for a text "
"and a question for question answering. It is also used as the last token "
"of a sequence built with special tokens."
msgstr ""

#: of transformers.M2M100Tokenizer:21
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.M2M100Tokenizer:24
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.M2M100Tokenizer:26
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:  - ``enable_sampling``: Enable subword "
"regularization. - ``nbest_size``: Sampling parameters for unigram. "
"Invalid for BPE-Dropout.    - ``nbest_size = {0,1}``: No sampling is "
"performed.   - ``nbest_size > 1``: samples from the nbest_size results."
"   - ``nbest_size < 0``: assuming that nbest_size is infinite and samples"
" from the all hypothesis (lattice)     using forward-filtering-and-"
"backward-sampling algorithm.  - ``alpha``: Smoothing parameter for "
"unigram sampling, and dropout probability of merge operations for   BPE-"
"dropout."
msgstr ""

#: of transformers.M2M100Tokenizer:26
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:"
msgstr ""

#: of transformers.M2M100Tokenizer:29
msgid "``enable_sampling``: Enable subword regularization."
msgstr ""

#: of transformers.M2M100Tokenizer:30
msgid "``nbest_size``: Sampling parameters for unigram. Invalid for BPE-Dropout."
msgstr ""

#: of transformers.M2M100Tokenizer:32
msgid "``nbest_size = {0,1}``: No sampling is performed."
msgstr ""

#: of transformers.M2M100Tokenizer:33
msgid "``nbest_size > 1``: samples from the nbest_size results."
msgstr ""

#: of transformers.M2M100Tokenizer:34
msgid ""
"``nbest_size < 0``: assuming that nbest_size is infinite and samples from"
" the all hypothesis (lattice) using forward-filtering-and-backward-"
"sampling algorithm."
msgstr ""

#: of transformers.M2M100Tokenizer:37
msgid ""
"``alpha``: Smoothing parameter for unigram sampling, and dropout "
"probability of merge operations for BPE-dropout."
msgstr ""

#: of transformers.M2M100Tokenizer:41
msgid "Examples::"
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens. An MBART"
" sequence has the following format, where ``X`` represents the sequence:"
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:4
msgid "``input_ids`` (for encoder) ``X [eos, src_lang_code]``"
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:5
msgid "``decoder_input_ids``: (for decoder) ``X [eos, tgt_lang_code]``"
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:7
msgid ""
"BOS is never used. Pairs of sequences are not the expected use case, but "
"they will be handled without a separator."
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:10
msgid "List of IDs to which the special tokens will be added."
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:12
#: transformers.M2M100Tokenizer.get_special_tokens_mask:6
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward
#: transformers.M2M100Model.forward
#: transformers.M2M100Tokenizer.build_inputs_with_special_tokens
#: transformers.M2M100Tokenizer.create_token_type_ids_from_sequences
#: transformers.M2M100Tokenizer.get_special_tokens_mask
#: transformers.M2M100Tokenizer.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:15
msgid ""
"List of `input IDs <../glossary.html#input-ids>`__ with the appropriate "
"special tokens."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward
#: transformers.M2M100Model.forward
#: transformers.M2M100Tokenizer.build_inputs_with_special_tokens
#: transformers.M2M100Tokenizer.create_token_type_ids_from_sequences
#: transformers.M2M100Tokenizer.get_special_tokens_mask
#: transformers.M2M100Tokenizer.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.M2M100Tokenizer.build_inputs_with_special_tokens:16
#: transformers.M2M100Tokenizer.create_token_type_ids_from_sequences:12
#: transformers.M2M100Tokenizer.get_special_tokens_mask:12
msgid ":obj:`List[int]`"
msgstr ""

#: of transformers.M2M100Tokenizer.create_token_type_ids_from_sequences:1
msgid ""
"Create the token type IDs corresponding to the sequences passed. `What "
"are token type IDs? <../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.M2M100Tokenizer.create_token_type_ids_from_sequences:4
msgid ""
"Should be overridden in a subclass if the model has a special way of "
"building those."
msgstr ""

#: of transformers.M2M100Tokenizer.create_token_type_ids_from_sequences:6
msgid "The first tokenized sequence."
msgstr ""

#: of transformers.M2M100Tokenizer.create_token_type_ids_from_sequences:8
msgid "The second tokenized sequence."
msgstr ""

#: of transformers.M2M100Tokenizer.create_token_type_ids_from_sequences:11
msgid "The token type ids."
msgstr ""

#: of transformers.M2M100Tokenizer.get_special_tokens_mask:1
msgid ""
"Retrieve sequence ids from a token list that has no special tokens added."
" This method is called when adding special tokens using the tokenizer "
"``prepare_for_model`` method."
msgstr ""

#: of transformers.M2M100Tokenizer.get_special_tokens_mask:4
msgid "List of IDs."
msgstr ""

#: of transformers.M2M100Tokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model."
msgstr ""

#: of transformers.M2M100Tokenizer.get_special_tokens_mask:11
msgid ""
"A list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of transformers.M2M100Tokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.M2M100Tokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.M2M100Tokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.M2M100Tokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.M2M100Tokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.M2M100Tokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:118
msgid "M2M100Model"
msgstr ""

#: of transformers.M2M100Model:1
msgid ""
"The bare M2M100 Model outputting raw hidden-states without any specific "
"head on top. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration:6 transformers.M2M100Model:6
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration:10
#: transformers.M2M100Model:10
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.M2M100Model.forward:1
msgid ""
"The :class:`~transformers.M2M100Model` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:4
#: transformers.M2M100Model.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:8
#: transformers.M2M100Model.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. Padding will be "
"ignored by default should you provide it.  Indices can be obtained using "
":class:`~transformers.M2M100Tokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:8
#: transformers.M2M100Model.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. Padding will be "
"ignored by default should you provide it."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:11
#: transformers.M2M100ForConditionalGeneration.forward:26
#: transformers.M2M100Model.forward:11 transformers.M2M100Model.forward:26
msgid ""
"Indices can be obtained using :class:`~transformers.M2M100Tokenizer`. See"
" :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:15
#: transformers.M2M100Model.forward:15
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:17
#: transformers.M2M100Model.forward:17
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:17
#: transformers.M2M100Model.forward:17
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:19
#: transformers.M2M100Model.forward:19
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:20
#: transformers.M2M100Model.forward:20
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:22
#: transformers.M2M100Model.forward:22
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:24
#: transformers.M2M100Model.forward:24
msgid ""
"Indices of decoder input sequence tokens in the vocabulary.  Indices can "
"be obtained using :class:`~transformers.M2M100Tokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" decoder input IDs? <../glossary.html#decoder-input-ids>`__  M2M100 uses "
"the :obj:`eos_token_id` as the starting token for "
":obj:`decoder_input_ids` generation. If :obj:`past_key_values` is used, "
"optionally only the last :obj:`decoder_input_ids` have to be input (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:24
#: transformers.M2M100Model.forward:24
msgid "Indices of decoder input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:30
#: transformers.M2M100Model.forward:30
msgid "`What are decoder input IDs? <../glossary.html#decoder-input-ids>`__"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:32
#: transformers.M2M100Model.forward:32
msgid ""
"M2M100 uses the :obj:`eos_token_id` as the starting token for "
":obj:`decoder_input_ids` generation. If :obj:`past_key_values` is used, "
"optionally only the last :obj:`decoder_input_ids` have to be input (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:36
#: transformers.M2M100Model.forward:36
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:39
#: transformers.M2M100Model.forward:39
msgid ""
"Mask to nullify selected heads of the attention modules in the encoder. "
"Mask values selected in ``[0, 1]``:  - 1 indicates the head is **not "
"masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:39
#: transformers.M2M100Model.forward:39
msgid ""
"Mask to nullify selected heads of the attention modules in the encoder. "
"Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:41
#: transformers.M2M100ForConditionalGeneration.forward:46
#: transformers.M2M100ForConditionalGeneration.forward:52
#: transformers.M2M100Model.forward:41 transformers.M2M100Model.forward:46
#: transformers.M2M100Model.forward:52
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:42
#: transformers.M2M100ForConditionalGeneration.forward:47
#: transformers.M2M100ForConditionalGeneration.forward:53
#: transformers.M2M100Model.forward:42 transformers.M2M100Model.forward:47
#: transformers.M2M100Model.forward:53
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:44
#: transformers.M2M100Model.forward:44
msgid ""
"Mask to nullify selected heads of the attention modules in the decoder. "
"Mask values selected in ``[0, 1]``:  - 1 indicates the head is **not "
"masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:44
#: transformers.M2M100Model.forward:44
msgid ""
"Mask to nullify selected heads of the attention modules in the decoder. "
"Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:49
#: transformers.M2M100Model.forward:49
msgid ""
"Mask to nullify selected heads of the cross-attention modules in the "
"decoder. Mask values selected in ``[0, 1]``:  - 1 indicates the head is "
"**not masked**, - 0 indicates the head is **masked**."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:49
#: transformers.M2M100Model.forward:49
msgid ""
"Mask to nullify selected heads of the cross-attention modules in the "
"decoder. Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:55
#: transformers.M2M100Model.forward:55
msgid ""
"Tuple consists of (:obj:`last_hidden_state`, `optional`: "
":obj:`hidden_states`, `optional`: :obj:`attentions`) "
":obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) is a sequence of hidden-states at the output "
"of the last layer of the encoder. Used in the cross-attention of the "
"decoder."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:60
#: transformers.M2M100Model.forward:60
msgid ""
"Tuple of :obj:`tuple(torch.FloatTensor)` of length "
":obj:`config.n_layers`, with each tuple having 2 tensors of shape "
":obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and"
" 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`.  Contains pre-computed "
"hidden-states (key and values in the self-attention blocks and in the "
"cross-attention blocks) that can be used (see :obj:`past_key_values` "
"input) to speed up sequential decoding.  If :obj:`past_key_values` are "
"used, the user can optionally input only the last "
":obj:`decoder_input_ids` (those that don't have their past key value "
"states given to this model) of shape :obj:`(batch_size, 1)` instead of "
"all :obj:`decoder_input_ids`` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:60
#: transformers.M2M100Model.forward:60
msgid ""
"Tuple of :obj:`tuple(torch.FloatTensor)` of length "
":obj:`config.n_layers`, with each tuple having 2 tensors of shape "
":obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and"
" 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:64
#: transformers.M2M100ForConditionalGeneration.forward:109
#: transformers.M2M100Model.forward:64 transformers.M2M100Model.forward:107
msgid ""
"Contains pre-computed hidden-states (key and values in the self-attention"
" blocks and in the cross-attention blocks) that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:67
#: transformers.M2M100Model.forward:67
msgid ""
"If :obj:`past_key_values` are used, the user can optionally input only "
"the last :obj:`decoder_input_ids` (those that don't have their past key "
"value states given to this model) of shape :obj:`(batch_size, 1)` instead"
" of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size, "
"sequence_length)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:71
#: transformers.M2M100Model.forward:71
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:75
#: transformers.M2M100Model.forward:75
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. If :obj:`past_key_values` is "
"used, optionally only the last :obj:`decoder_inputs_embeds` have to be "
"input (see :obj:`past_key_values`). This is useful if you want more "
"control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix.  If"
" :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both "
"unset, :obj:`decoder_inputs_embeds` takes the value of "
":obj:`inputs_embeds`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:75
#: transformers.M2M100Model.forward:75
msgid ""
"Optionally, instead of passing :obj:`decoder_input_ids` you can choose to"
" directly pass an embedded representation. If :obj:`past_key_values` is "
"used, optionally only the last :obj:`decoder_inputs_embeds` have to be "
"input (see :obj:`past_key_values`). This is useful if you want more "
"control over how to convert :obj:`decoder_input_ids` indices into "
"associated vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:80
#: transformers.M2M100Model.forward:80
msgid ""
"If :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both "
"unset, :obj:`decoder_inputs_embeds` takes the value of "
":obj:`inputs_embeds`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:83
#: transformers.M2M100Model.forward:83
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:86
#: transformers.M2M100Model.forward:86
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:89
#: transformers.M2M100Model.forward:89
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:92
#: transformers.M2M100Model.forward:92
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.M2M100Model.forward:95
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or a tuple "
"of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.M2M100Config`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model.    If"
" :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size,   1, hidden_size)` is output. - "
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads."
msgstr ""

#: of transformers.M2M100Model.forward:95
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or a tuple "
"of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.M2M100Config`) and inputs."
msgstr ""

#: of transformers.M2M100Model.forward:99
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the decoder of the model."
msgstr ""

#: of transformers.M2M100Model.forward:101
msgid ""
"If :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size, 1, hidden_size)` is output."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:105
#: transformers.M2M100Model.forward:103
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors of shape"
" :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) "
"and 2 additional tensors of shape :obj:`(batch_size, num_heads, "
"encoder_sequence_length, embed_size_per_head)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:111
#: transformers.M2M100Model.forward:109
msgid ""
"**decoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:114
#: transformers.M2M100Model.forward:112
msgid ""
"Hidden-states of the decoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:115
#: transformers.M2M100Model.forward:113
msgid ""
"**decoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:118
#: transformers.M2M100Model.forward:116
msgid ""
"Attentions weights of the decoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:120
#: transformers.M2M100Model.forward:118
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:123
#: transformers.M2M100Model.forward:121
msgid ""
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:125
#: transformers.M2M100Model.forward:123
msgid ""
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:126
#: transformers.M2M100Model.forward:124
msgid ""
"**encoder_hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:129
#: transformers.M2M100Model.forward:127
msgid ""
"Hidden-states of the encoder at the output of each layer plus the initial"
" embedding outputs."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:130
#: transformers.M2M100Model.forward:128
msgid ""
"**encoder_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:133
#: transformers.M2M100Model.forward:131
msgid ""
"Attentions weights of the encoder, after the attention softmax, used to "
"compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.M2M100Model.forward:133
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:137
#: transformers.M2M100Model.forward:135
msgid "Example::"
msgstr ""

#: ../../source/model_doc/m2m_100.rst:125
msgid "M2M100ForConditionalGeneration"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration:1
msgid ""
"The M2M100 Model with a language modeling head. Can be used for "
"summarization. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:1
msgid ""
"The :class:`~transformers.M2M100ForConditionalGeneration` forward method,"
" overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:94
msgid ""
"Labels for computing the masked language modeling loss. Indices should "
"either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` "
"docstring). Tokens with indices set to ``-100`` are ignored (masked), the"
" loss is only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:99
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.M2M100Config`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss. - "
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
" - **past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and 2 additional tensors of   shape "
":obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and in the cross-attention   blocks) "
"that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding. - **decoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the decoder at the output of each "
"layer plus the initial embedding outputs. - **decoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the decoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"decoder's cross-attention layer, after the attention softmax, used to "
"compute the   weighted average in the cross-attention heads. - "
"**encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the encoder "
"of the model. - **encoder_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the encoder at the output of each "
"layer plus the initial embedding outputs. - **encoder_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the encoder,"
" after the attention softmax, used to compute the weighted average in the"
"   self-attention heads.   Example::      >>> from transformers import "
"M2M100Tokenizer, M2M100ForConditionalGeneration      >>> model = "
"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')"
"     >>> tokenizer = "
"M2M100Tokenizer.from_pretrained('facebook/m2m100_418M')      >>> "
"text_to_translate = \"Life is like a box of chocolates\"     >>> "
"model_inputs = tokenizer(text_to_translate, return_tensors='pt')      >>>"
" # translate to French     >>> gen_tokens = model.generate( "
"**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))     "
">>> print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:99
msgid ""
"A :class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.M2M100Config`) and inputs."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:103
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:104
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:150
msgid ""
":class:`~transformers.modeling_outputs.Seq2SeqLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.M2M100ForConditionalGeneration.forward:152
msgid "Translation example::"
msgstr ""


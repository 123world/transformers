# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/visual_bert.rst:14
msgid "VisualBERT"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:19
msgid ""
"The VisualBERT model was proposed in `VisualBERT: A Simple and Performant"
" Baseline for Vision and Language <https://arxiv.org/pdf/1908.03557>`__ "
"by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang."
" VisualBERT is a neural network trained on a variety of (image, text) "
"pairs."
msgstr ""

#: ../../source/model_doc/visual_bert.rst:23
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:25
msgid ""
"*We propose VisualBERT, a simple and flexible framework for modeling a "
"broad range of vision-and-language tasks. VisualBERT consists of a stack "
"of Transformer layers that implicitly align elements of an input text and"
" regions in an associated input image with self-attention. We further "
"propose two visually-grounded language model objectives for pre-training "
"VisualBERT on image caption data. Experiments on four vision-and-language"
" tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT "
"outperforms or rivals with state-of-the-art models while being "
"significantly simpler. Further analysis demonstrates that VisualBERT can "
"ground elements of language to image regions without any explicit "
"supervision and is even sensitive to syntactic relationships, tracking, "
"for example, associations between verbs and image regions corresponding "
"to their arguments.*"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:34
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:36
msgid ""
"Most of the checkpoints provided work with the "
":class:`~transformers.VisualBertForPreTraining` configuration. Other "
"checkpoints provided are the fine-tuned checkpoints for down-stream tasks"
" - VQA ('visualbert-vqa'), VCR ('visualbert-vcr'), NLVR2 ('visualbert-"
"nlvr2'). Hence, if you are not working on these downstream tasks, it is "
"recommended that you use the pretrained checkpoints."
msgstr ""

#: ../../source/model_doc/visual_bert.rst:41
msgid ""
"For the VCR task, the authors use a fine-tuned detector for generating "
"visual embeddings, for all the checkpoints. We do not provide the "
"detector and its weights as a part of the package, but it will be "
"available in the research projects, and the states can be loaded directly"
" into the detector provided."
msgstr ""

#: ../../source/model_doc/visual_bert.rst:46
msgid "Usage"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:48
msgid ""
"VisualBERT is a multi-modal vision and language model. It can be used for"
" visual question answering, multiple choice, visual reasoning and region-"
"to-phrase correspondence tasks. VisualBERT uses a BERT-like transformer "
"to prepare embeddings for image-text pairs. Both the text and visual "
"features are then projected to a latent space with identical dimension."
msgstr ""

#: ../../source/model_doc/visual_bert.rst:53
msgid ""
"To feed images to the model, each image is passed through a pre-trained "
"object detector and the regions and the bounding boxes are extracted. The"
" authors use the features generated after passing these regions through a"
" pre-trained CNN like ResNet as visual embeddings. They also add absolute"
" position embeddings, and feed the resulting sequence of vectors to a "
"standard BERT model. The text input is concatenated in the front of the "
"visual embeddings in the embedding layer, and is expected to be bound by "
"[CLS] and a [SEP] tokens, as in BERT. The segment IDs must also be set "
"appropriately for the textual and visual parts."
msgstr ""

#: ../../source/model_doc/visual_bert.rst:60
msgid ""
"The :class:`~transformers.BertTokenizer` is used to encode the text. A "
"custom detector/feature extractor must be used to get the visual "
"embeddings. For an example on how to generate visual embeddings, see the "
"`colab notebook "
"<https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing>`__."
" The following example shows how to get the last hidden state using "
":class:`~transformers.VisualBertModel`:"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:80
msgid ""
"This model was contributed by `gchhablani "
"<https://huggingface.co/gchhablani>`__. The original code can be found "
"`here <https://github.com/uclanlp/visualbert>`__."
msgstr ""

#: ../../source/model_doc/visual_bert.rst:84
msgid "VisualBertConfig"
msgstr ""

#: of transformers.VisualBertConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.VisualBertModel`. It is used to instantiate an "
"VisualBERT model according to the specified arguments, defining the model"
" architecture. Instantiating a configuration with the defaults will yield"
" a similar configuration to that of the VisualBERT `visualbert-vqa-coco-"
"pre <https://huggingface.co/uclanlp/visualbert-vqa-coco-pre>`__ "
"architecture."
msgstr ""

#: of transformers.VisualBertConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.VisualBertConfig transformers.VisualBertForMultipleChoice
#: transformers.VisualBertForMultipleChoice.forward
#: transformers.VisualBertForPreTraining
#: transformers.VisualBertForPreTraining.forward
#: transformers.VisualBertForQuestionAnswering
#: transformers.VisualBertForQuestionAnswering.forward
#: transformers.VisualBertForRegionToPhraseAlignment
#: transformers.VisualBertForRegionToPhraseAlignment.forward
#: transformers.VisualBertForVisualReasoning
#: transformers.VisualBertForVisualReasoning.forward
#: transformers.VisualBertModel transformers.VisualBertModel.forward
msgid "Parameters"
msgstr ""

#: of transformers.VisualBertConfig:10
msgid ""
"Vocabulary size of the VisualBERT model. Defines the number of different "
"tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.VisualBertModel`. Vocabulary size of the "
"model. Defines the different tokens that can be represented by the "
"``inputs_ids`` passed to the forward method of "
":class:`~transformers.VisualBertModel`."
msgstr ""

#: of transformers.VisualBertConfig:15
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.VisualBertConfig:17
msgid "Dimensionality of the visual embeddings to be passed to the model."
msgstr ""

#: of transformers.VisualBertConfig:19
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.VisualBertConfig:21
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.VisualBertConfig:23
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.VisualBertConfig:25
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.VisualBertConfig:28
msgid ""
"The dropout probabilitiy for all fully connected layers in the "
"embeddings, encoder, and pooler."
msgstr ""

#: of transformers.VisualBertConfig:30
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.VisualBertConfig:32
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.VisualBertConfig:35
msgid ""
"The vocabulary size of the :obj:`token_type_ids` passed when calling "
":class:`~transformers.VisualBertModel`."
msgstr ""

#: of transformers.VisualBertConfig:38
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.VisualBertConfig:40
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.VisualBertConfig:42
msgid ""
"Whether or not the model should bypass the transformer for the visual "
"embeddings. If set to :obj:`True`, the model directly concatenates the "
"visual embeddings from :class:`~transformers.VisualBertEmbeddings` with "
"text output from transformers, and then pass it to a self-attention "
"layer."
msgstr ""

#: of transformers.VisualBertConfig:46
msgid ""
"Whether or not the visual token type and position type embedding weights "
"should be initialized the same as the textual token type and positive "
"type embeddings. When set to :obj:`True`, the weights of the textual "
"token type and position type embeddings are copied to the respective "
"visual embedding layers."
msgstr ""

#: of transformers.VisualBertConfig:51
#: transformers.VisualBertForMultipleChoice.forward:94
#: transformers.VisualBertForPreTraining.forward:101
#: transformers.VisualBertForQuestionAnswering.forward:91
#: transformers.VisualBertForRegionToPhraseAlignment.forward:94
#: transformers.VisualBertForVisualReasoning.forward:91
msgid "Example::"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:90
msgid "VisualBertModel"
msgstr ""

#: of transformers.VisualBertModel:1
msgid ""
"The bare VisualBert Model transformer outputting raw hidden-states "
"without any specific head on top. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"model (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.VisualBertForMultipleChoice:8
#: transformers.VisualBertForPreTraining:8
#: transformers.VisualBertForQuestionAnswering:8
#: transformers.VisualBertForRegionToPhraseAlignment:8
#: transformers.VisualBertForVisualReasoning:8 transformers.VisualBertModel:6
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.VisualBertForMultipleChoice:12
#: transformers.VisualBertForPreTraining:12
#: transformers.VisualBertForQuestionAnswering:12
#: transformers.VisualBertForRegionToPhraseAlignment:12
#: transformers.VisualBertForVisualReasoning:12 transformers.VisualBertModel:10
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.VisualBertModel:16
msgid ""
"The model can behave as an encoder (with only self-attention) following "
"the architecture described in `Attention is all you need "
"<https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, "
"Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser "
"and Illia Polosukhin."
msgstr ""

#: of transformers.VisualBertModel.forward:1
msgid ""
"The :class:`~transformers.VisualBertModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:4
#: transformers.VisualBertForPreTraining.forward:4
#: transformers.VisualBertForQuestionAnswering.forward:4
#: transformers.VisualBertForRegionToPhraseAlignment.forward:4
#: transformers.VisualBertForVisualReasoning.forward:4
#: transformers.VisualBertModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:8
#: transformers.VisualBertForPreTraining.forward:8
#: transformers.VisualBertForQuestionAnswering.forward:8
#: transformers.VisualBertForRegionToPhraseAlignment.forward:8
#: transformers.VisualBertForVisualReasoning.forward:8
#: transformers.VisualBertModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.BertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:8
#: transformers.VisualBertForPreTraining.forward:8
#: transformers.VisualBertForQuestionAnswering.forward:8
#: transformers.VisualBertForRegionToPhraseAlignment.forward:8
#: transformers.VisualBertForVisualReasoning.forward:8
#: transformers.VisualBertModel.forward:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:10
#: transformers.VisualBertForPreTraining.forward:10
#: transformers.VisualBertForQuestionAnswering.forward:10
#: transformers.VisualBertForRegionToPhraseAlignment.forward:10
#: transformers.VisualBertForVisualReasoning.forward:10
#: transformers.VisualBertModel.forward:10
msgid ""
"Indices can be obtained using :class:`~transformers.BertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:14
#: transformers.VisualBertForPreTraining.forward:14
#: transformers.VisualBertForQuestionAnswering.forward:14
#: transformers.VisualBertForRegionToPhraseAlignment.forward:14
#: transformers.VisualBertForVisualReasoning.forward:14
#: transformers.VisualBertModel.forward:14
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:16
#: transformers.VisualBertForPreTraining.forward:16
#: transformers.VisualBertForQuestionAnswering.forward:16
#: transformers.VisualBertForRegionToPhraseAlignment.forward:16
#: transformers.VisualBertForVisualReasoning.forward:16
#: transformers.VisualBertModel.forward:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:16
#: transformers.VisualBertForPreTraining.forward:16
#: transformers.VisualBertForQuestionAnswering.forward:16
#: transformers.VisualBertForRegionToPhraseAlignment.forward:16
#: transformers.VisualBertForVisualReasoning.forward:16
#: transformers.VisualBertModel.forward:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:18
#: transformers.VisualBertForMultipleChoice.forward:49
#: transformers.VisualBertForPreTraining.forward:18
#: transformers.VisualBertForPreTraining.forward:49
#: transformers.VisualBertForQuestionAnswering.forward:18
#: transformers.VisualBertForQuestionAnswering.forward:49
#: transformers.VisualBertForRegionToPhraseAlignment.forward:18
#: transformers.VisualBertForRegionToPhraseAlignment.forward:49
#: transformers.VisualBertForVisualReasoning.forward:18
#: transformers.VisualBertForVisualReasoning.forward:49
#: transformers.VisualBertModel.forward:18
#: transformers.VisualBertModel.forward:49
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:19
#: transformers.VisualBertForMultipleChoice.forward:50
#: transformers.VisualBertForPreTraining.forward:19
#: transformers.VisualBertForPreTraining.forward:50
#: transformers.VisualBertForQuestionAnswering.forward:19
#: transformers.VisualBertForQuestionAnswering.forward:50
#: transformers.VisualBertForRegionToPhraseAlignment.forward:19
#: transformers.VisualBertForRegionToPhraseAlignment.forward:50
#: transformers.VisualBertForVisualReasoning.forward:19
#: transformers.VisualBertForVisualReasoning.forward:50
#: transformers.VisualBertModel.forward:19
#: transformers.VisualBertModel.forward:50
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:21
#: transformers.VisualBertForMultipleChoice.forward:52
#: transformers.VisualBertForPreTraining.forward:21
#: transformers.VisualBertForPreTraining.forward:52
#: transformers.VisualBertForQuestionAnswering.forward:21
#: transformers.VisualBertForQuestionAnswering.forward:52
#: transformers.VisualBertForRegionToPhraseAlignment.forward:21
#: transformers.VisualBertForRegionToPhraseAlignment.forward:52
#: transformers.VisualBertForVisualReasoning.forward:21
#: transformers.VisualBertForVisualReasoning.forward:52
#: transformers.VisualBertModel.forward:21
#: transformers.VisualBertModel.forward:52
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:23
#: transformers.VisualBertForPreTraining.forward:23
#: transformers.VisualBertForQuestionAnswering.forward:23
#: transformers.VisualBertForRegionToPhraseAlignment.forward:23
#: transformers.VisualBertForVisualReasoning.forward:23
#: transformers.VisualBertModel.forward:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:  - 0 corresponds to a "
"`sentence A` token, - 1 corresponds to a `sentence B` token.  `What are "
"token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:23
#: transformers.VisualBertForPreTraining.forward:23
#: transformers.VisualBertForQuestionAnswering.forward:23
#: transformers.VisualBertForRegionToPhraseAlignment.forward:23
#: transformers.VisualBertForVisualReasoning.forward:23
#: transformers.VisualBertModel.forward:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:26
#: transformers.VisualBertForPreTraining.forward:26
#: transformers.VisualBertForQuestionAnswering.forward:26
#: transformers.VisualBertForRegionToPhraseAlignment.forward:26
#: transformers.VisualBertForVisualReasoning.forward:26
#: transformers.VisualBertModel.forward:26
msgid "0 corresponds to a `sentence A` token,"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:27
#: transformers.VisualBertForPreTraining.forward:27
#: transformers.VisualBertForQuestionAnswering.forward:27
#: transformers.VisualBertForRegionToPhraseAlignment.forward:27
#: transformers.VisualBertForVisualReasoning.forward:27
#: transformers.VisualBertModel.forward:27
msgid "1 corresponds to a `sentence B` token."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:29
#: transformers.VisualBertForPreTraining.forward:29
#: transformers.VisualBertForQuestionAnswering.forward:29
#: transformers.VisualBertForRegionToPhraseAlignment.forward:29
#: transformers.VisualBertForVisualReasoning.forward:29
#: transformers.VisualBertModel.forward:29
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:31
#: transformers.VisualBertForPreTraining.forward:31
#: transformers.VisualBertForQuestionAnswering.forward:31
#: transformers.VisualBertForRegionToPhraseAlignment.forward:31
#: transformers.VisualBertForVisualReasoning.forward:31
#: transformers.VisualBertModel.forward:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``.  `What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:31
#: transformers.VisualBertForPreTraining.forward:31
#: transformers.VisualBertForQuestionAnswering.forward:31
#: transformers.VisualBertForRegionToPhraseAlignment.forward:31
#: transformers.VisualBertForVisualReasoning.forward:31
#: transformers.VisualBertModel.forward:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:34
#: transformers.VisualBertForPreTraining.forward:34
#: transformers.VisualBertForQuestionAnswering.forward:34
#: transformers.VisualBertForRegionToPhraseAlignment.forward:34
#: transformers.VisualBertForVisualReasoning.forward:34
#: transformers.VisualBertModel.forward:34
msgid "`What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:36
#: transformers.VisualBertForPreTraining.forward:36
#: transformers.VisualBertForQuestionAnswering.forward:36
#: transformers.VisualBertForRegionToPhraseAlignment.forward:36
#: transformers.VisualBertForVisualReasoning.forward:36
#: transformers.VisualBertModel.forward:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:36
#: transformers.VisualBertForPreTraining.forward:36
#: transformers.VisualBertForQuestionAnswering.forward:36
#: transformers.VisualBertForRegionToPhraseAlignment.forward:36
#: transformers.VisualBertForVisualReasoning.forward:36
#: transformers.VisualBertModel.forward:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:38
#: transformers.VisualBertForPreTraining.forward:38
#: transformers.VisualBertForQuestionAnswering.forward:38
#: transformers.VisualBertForRegionToPhraseAlignment.forward:38
#: transformers.VisualBertForVisualReasoning.forward:38
#: transformers.VisualBertModel.forward:38
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:39
#: transformers.VisualBertForPreTraining.forward:39
#: transformers.VisualBertForQuestionAnswering.forward:39
#: transformers.VisualBertForRegionToPhraseAlignment.forward:39
#: transformers.VisualBertForVisualReasoning.forward:39
#: transformers.VisualBertModel.forward:39
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:41
#: transformers.VisualBertForPreTraining.forward:41
#: transformers.VisualBertForQuestionAnswering.forward:41
#: transformers.VisualBertForRegionToPhraseAlignment.forward:41
#: transformers.VisualBertForVisualReasoning.forward:41
#: transformers.VisualBertModel.forward:41
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:45
#: transformers.VisualBertForPreTraining.forward:45
#: transformers.VisualBertForQuestionAnswering.forward:45
#: transformers.VisualBertForRegionToPhraseAlignment.forward:45
#: transformers.VisualBertForVisualReasoning.forward:45
#: transformers.VisualBertModel.forward:45
msgid ""
"The embedded representation of the visual inputs, generally derived using"
" using an object detector."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:47
#: transformers.VisualBertForPreTraining.forward:47
#: transformers.VisualBertForQuestionAnswering.forward:47
#: transformers.VisualBertForRegionToPhraseAlignment.forward:47
#: transformers.VisualBertForVisualReasoning.forward:47
#: transformers.VisualBertModel.forward:47
msgid ""
"Mask to avoid performing attention on visual embeddings. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:47
#: transformers.VisualBertForPreTraining.forward:47
#: transformers.VisualBertForQuestionAnswering.forward:47
#: transformers.VisualBertForRegionToPhraseAlignment.forward:47
#: transformers.VisualBertForVisualReasoning.forward:47
#: transformers.VisualBertModel.forward:47
msgid ""
"Mask to avoid performing attention on visual embeddings. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:54
#: transformers.VisualBertForPreTraining.forward:54
#: transformers.VisualBertForQuestionAnswering.forward:54
#: transformers.VisualBertForRegionToPhraseAlignment.forward:54
#: transformers.VisualBertForVisualReasoning.forward:54
#: transformers.VisualBertModel.forward:54
msgid ""
"Segment token indices to indicate different portions of the visual "
"embeds.  `What are token type IDs? <../glossary.html#token-type-ids>`_ "
"The authors of VisualBERT set the `visual_token_type_ids` to `1` for all "
"tokens."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:54
#: transformers.VisualBertForPreTraining.forward:54
#: transformers.VisualBertForQuestionAnswering.forward:54
#: transformers.VisualBertForRegionToPhraseAlignment.forward:54
#: transformers.VisualBertForVisualReasoning.forward:54
#: transformers.VisualBertModel.forward:54
msgid "Segment token indices to indicate different portions of the visual embeds."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:56
#: transformers.VisualBertForPreTraining.forward:56
#: transformers.VisualBertForQuestionAnswering.forward:56
#: transformers.VisualBertForRegionToPhraseAlignment.forward:56
#: transformers.VisualBertForVisualReasoning.forward:56
#: transformers.VisualBertModel.forward:56
msgid ""
"`What are token type IDs? <../glossary.html#token-type-ids>`_ The authors"
" of VisualBERT set the `visual_token_type_ids` to `1` for all tokens."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:59
#: transformers.VisualBertForPreTraining.forward:59
#: transformers.VisualBertForQuestionAnswering.forward:59
#: transformers.VisualBertForRegionToPhraseAlignment.forward:59
#: transformers.VisualBertForVisualReasoning.forward:59
#: transformers.VisualBertModel.forward:59
msgid ""
"Image-Text alignment uses to decide the position IDs of the visual "
"embeddings."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:61
#: transformers.VisualBertForPreTraining.forward:61
#: transformers.VisualBertForQuestionAnswering.forward:61
#: transformers.VisualBertForRegionToPhraseAlignment.forward:61
#: transformers.VisualBertForVisualReasoning.forward:61
#: transformers.VisualBertModel.forward:61
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:64
#: transformers.VisualBertForPreTraining.forward:64
#: transformers.VisualBertForQuestionAnswering.forward:64
#: transformers.VisualBertForRegionToPhraseAlignment.forward:64
#: transformers.VisualBertForVisualReasoning.forward:64
#: transformers.VisualBertModel.forward:64
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:67
#: transformers.VisualBertForPreTraining.forward:67
#: transformers.VisualBertForQuestionAnswering.forward:67
#: transformers.VisualBertForRegionToPhraseAlignment.forward:67
#: transformers.VisualBertForVisualReasoning.forward:67
#: transformers.VisualBertModel.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.VisualBertModel.forward:70
msgid ""
">>> # Assumption: `get_visual_embeddings(image)` gets the visual "
"embeddings of the image. >>> from transformers import BertTokenizer, "
"VisualBertModel >>> import torch  >>> tokenizer = "
"BertTokenizer.from_pretrained('bert-base-uncased') >>> model = "
"VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre')  >>> "
"inputs = tokenizer(\"The capital of France is Paris.\", "
"return_tensors=\"pt\") >>> visual_embeds = "
"get_visual_embeddings(image).unsqueeze(0) >>> visual_token_type_ids = "
"torch.ones(visual_embeds.shape[:-1], dtype=torch.long) #example >>> "
"visual_attention_mask = torch.ones(visual_embeds.shape[:-1], "
"dtype=torch.float)  >>> inputs.update({{ ...     \"visual_embeds\": "
"visual_embeds, ...     \"visual_token_type_ids\": visual_token_type_ids, "
"...     \"visual_attention_mask\": visual_attention_mask ... }})  >>> "
"outputs = model(**inputs)  >>> last_hidden_states = "
"outputs.last_hidden_state"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:97
msgid "VisualBertForPreTraining"
msgstr ""

#: of transformers.VisualBertForPreTraining:1
msgid ""
"VisualBert Model with two heads on top as done during the pretraining: a "
"`masked language modeling` head and a `sentence-image prediction "
"(classification)` head."
msgstr ""

#: of transformers.VisualBertForMultipleChoice:4
#: transformers.VisualBertForPreTraining:4
#: transformers.VisualBertForQuestionAnswering:4
#: transformers.VisualBertForRegionToPhraseAlignment:4
#: transformers.VisualBertForVisualReasoning:4
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:1
msgid ""
"The :class:`~transformers.VisualBertForPreTraining` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple.  labels (:obj:`torch.LongTensor` of shape "
"``(batch_size, total_sequence_length)``, `optional`):     Labels for "
"computing the masked language modeling loss. Indices should be in "
"``[-100, 0, ...,     config.vocab_size]`` (see ``input_ids`` docstring) "
"Tokens with indices set to ``-100`` are ignored     (masked), the loss is"
" only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]`` sentence_image_labels (``torch.LongTensor`` of shape"
" ``(batch_size,)``, `optional`):     Labels for computing the sentence-"
"image prediction (classification) loss. Input should be a sequence     "
"pair (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:"
"      - 0 indicates sequence B is a matching pair of sequence A for the "
"given image,     - 1 indicates sequence B is a random sequence w.r.t A "
"for the given image."
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:71
msgid ""
"labels (:obj:`torch.LongTensor` of shape ``(batch_size, "
"total_sequence_length)``, `optional`):"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:70
msgid ""
"Labels for computing the masked language modeling loss. Indices should be"
" in ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) "
"Tokens with indices set to ``-100`` are ignored (masked), the loss is "
"only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:77
msgid ""
"sentence_image_labels (``torch.LongTensor`` of shape ``(batch_size,)``, "
"`optional`):"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:74
msgid ""
"Labels for computing the sentence-image prediction (classification) loss."
" Input should be a sequence pair (see :obj:`input_ids` docstring) Indices"
" should be in ``[0, 1]``:"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:77
msgid ""
"0 indicates sequence B is a matching pair of sequence A for the given "
"image,"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:78
msgid "1 indicates sequence B is a random sequence w.r.t A for the given image."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward
#: transformers.VisualBertForPreTraining.forward
#: transformers.VisualBertForQuestionAnswering.forward
#: transformers.VisualBertForRegionToPhraseAlignment.forward
#: transformers.VisualBertForVisualReasoning.forward
msgid "Returns"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:81
msgid ""
"A "
":class:`~transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.VisualBertConfig`) "
"and inputs.  - **loss** (`optional`, returned when ``labels`` is "
"provided, ``torch.FloatTensor`` of shape :obj:`(1,)`) -- Total loss as "
"the sum of the masked language modeling loss and the sentence-image "
"prediction   (classification) loss. - **prediction_logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax). - "
"**seq_relationship_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, 2)`) -- Prediction scores of the sentence-image "
"prediction (classification) head (scores of True/False continuation   "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> # Assumption: "
"`get_visual_embeddings(image)` gets the visual embeddings of the image in"
" the batch.     >>> from transformers import BertTokenizer, "
"VisualBertForPreTraining      >>> tokenizer = "
"BertTokenizer.from_pretrained('bert-base-uncased')     >>> model = "
"VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-"
"pre')      >>> inputs = tokenizer(\"The capital of France is {mask}.\", "
"return_tensors=\"pt\")     >>> visual_embeds = "
"get_visual_embeddings(image).unsqueeze(0)     >>> visual_token_type_ids ="
" torch.ones(visual_embeds.shape[:-1], dtype=torch.long) #example     >>> "
"visual_attention_mask = torch.ones(visual_embeds.shape[:-1], "
"dtype=torch.float)      >>> inputs.update({{     ...     "
"\"visual_embeds\": visual_embeds,     ...     \"visual_token_type_ids\": "
"visual_token_type_ids,     ...     \"visual_attention_mask\": "
"visual_attention_mask     ... }})     >>> max_length  = "
"inputs[\"input_ids\"].shape[-1]+visual_embeds.shape[-2]     >>> labels = "
"tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", "
"padding=\"max_length\", max_length=max_length)[\"input_ids\"]     >>> "
"sentence_image_labels = torch.tensor(1).unsqueeze(0) # Batch_size       "
">>> outputs = model(**inputs, labels=labels, "
"sentence_image_labels=sentence_image_labels)     >>> loss = outputs.loss"
"     >>> prediction_logits = outputs.prediction_logits     >>> "
"seq_relationship_logits = outputs.seq_relationship_logits"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:81
msgid ""
"A "
":class:`~transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.VisualBertConfig`) "
"and inputs."
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:85
msgid ""
"**loss** (`optional`, returned when ``labels`` is provided, "
"``torch.FloatTensor`` of shape :obj:`(1,)`) -- Total loss as the sum of "
"the masked language modeling loss and the sentence-image prediction "
"(classification) loss."
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:87
msgid ""
"**prediction_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax)."
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:88
msgid ""
"**seq_relationship_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, 2)`) -- Prediction scores of the sentence-image "
"prediction (classification) head (scores of True/False continuation "
"before SoftMax)."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:83
#: transformers.VisualBertForPreTraining.forward:90
#: transformers.VisualBertForQuestionAnswering.forward:80
#: transformers.VisualBertForRegionToPhraseAlignment.forward:83
#: transformers.VisualBertForVisualReasoning.forward:80
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:86
#: transformers.VisualBertForPreTraining.forward:93
#: transformers.VisualBertForQuestionAnswering.forward:83
#: transformers.VisualBertForRegionToPhraseAlignment.forward:86
#: transformers.VisualBertForVisualReasoning.forward:83
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:87
#: transformers.VisualBertForPreTraining.forward:94
#: transformers.VisualBertForQuestionAnswering.forward:84
#: transformers.VisualBertForRegionToPhraseAlignment.forward:87
#: transformers.VisualBertForVisualReasoning.forward:84
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:90
#: transformers.VisualBertForPreTraining.forward:97
#: transformers.VisualBertForQuestionAnswering.forward:87
#: transformers.VisualBertForRegionToPhraseAlignment.forward:90
#: transformers.VisualBertForVisualReasoning.forward:87
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward
#: transformers.VisualBertForPreTraining.forward
#: transformers.VisualBertForQuestionAnswering.forward
#: transformers.VisualBertForRegionToPhraseAlignment.forward
#: transformers.VisualBertForVisualReasoning.forward
msgid "Return type"
msgstr ""

#: of transformers.VisualBertForPreTraining.forward:128
msgid ""
":class:`~transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:104
msgid "VisualBertForQuestionAnswering"
msgstr ""

#: of transformers.VisualBertForQuestionAnswering:1
msgid ""
"VisualBert Model with a classification/regression head on top (a dropout "
"and a linear layer on top of the pooled output) for VQA."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:1
msgid ""
"The :class:`~transformers.VisualBertForQuestionAnswering` forward method,"
" overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple.  labels (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size, total_sequence_length)`, `optional`):     Labels for "
"computing the sequence classification/regression loss. Indices should be "
"in :obj:`[0, ...,     config.num_labels - 1]`. A KLDivLoss is computed "
"between the labels and the returned logits."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:70
msgid ""
"labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, "
"total_sequence_length)`, `optional`):"
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:70
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. A KLDivLoss is "
"computed between the labels and the returned logits."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:74
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.VisualBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"(or regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> # Assumption: "
"`get_visual_embeddings(image)` gets the visual embeddings of the image in"
" the batch.     >>> from transformers import BertTokenizer, "
"VisualBertForQuestionAnswering     >>> import torch      >>> tokenizer = "
"BertTokenizer.from_pretrained('bert-base-uncased')     >>> model = "
"VisualBertForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')"
"      >>> text = \"Who is eating the apple?\"     >>> inputs = "
"tokenizer(text, return_tensors='pt')     >>> visual_embeds = "
"get_visual_embeddings(image).unsqueeze(0)     >>> visual_token_type_ids ="
" torch.ones(visual_embeds.shape[:-1], dtype=torch.long) #example     >>> "
"visual_attention_mask = torch.ones(visual_embeds.shape[:-1], "
"dtype=torch.float)      >>> inputs.update({{     ...     "
"\"visual_embeds\": visual_embeds,     ...     \"visual_token_type_ids\": "
"visual_token_type_ids,     ...     \"visual_attention_mask\": "
"visual_attention_mask     ... }})      >>> labels = "
"torch.tensor([[0.0,1.0]]).unsqueeze(0)  # Batch size 1, Num labels 2"
"      >>> outputs = model(**inputs, labels=labels)     >>> loss = "
"outputs.loss     >>> scores = outputs.logits"
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:74
#: transformers.VisualBertForRegionToPhraseAlignment.forward:77
#: transformers.VisualBertForVisualReasoning.forward:74
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.VisualBertConfig`) and "
"inputs."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:78
#: transformers.VisualBertForRegionToPhraseAlignment.forward:81
#: transformers.VisualBertForVisualReasoning.forward:78
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:79
#: transformers.VisualBertForRegionToPhraseAlignment.forward:82
#: transformers.VisualBertForVisualReasoning.forward:79
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.VisualBertForQuestionAnswering.forward:117
#: transformers.VisualBertForRegionToPhraseAlignment.forward:122
#: transformers.VisualBertForVisualReasoning.forward:117
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:111
msgid "VisualBertForMultipleChoice"
msgstr ""

#: of transformers.VisualBertForMultipleChoice:1
msgid ""
"VisualBert Model with a multiple choice classification head on top (a "
"linear layer on top of the pooled output and a softmax) e.g. for VCR "
"tasks."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:1
msgid ""
"The :class:`~transformers.VisualBertForMultipleChoice` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple.  labels (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size,)`, `optional`):     Labels for computing the multiple "
"choice classification loss. Indices should be in ``[0, ...,     "
"num_choices-1]`` where :obj:`num_choices` is the size of the second "
"dimension of the input tensors.     (See :obj:`input_ids` above)"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:71
#: transformers.VisualBertForVisualReasoning.forward:70
msgid ""
"labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, "
"`optional`):"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:70
msgid ""
"Labels for computing the multiple choice classification loss. Indices "
"should be in ``[0, ..., num_choices-1]`` where :obj:`num_choices` is the "
"size of the second dimension of the input tensors. (See :obj:`input_ids` "
"above)"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:75
msgid ""
"A :class:`~transformers.modeling_outputs.MultipleChoiceModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.VisualBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape `(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"loss. - **logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_choices)`) -- `num_choices` is the second dimension of the input "
"tensors. (see `input_ids` above).    Classification scores (before "
"SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> from transformers import "
"BertTokenizer, VisualBertForMultipleChoice     >>> import torch      >>> "
"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')     >>> "
"model = VisualBertForMultipleChoice.from_pretrained('uclanlp/visualbert-"
"vcr')      >>> prompt = \"In Italy, pizza served in formal settings, such"
" as at a restaurant, is presented unsliced.\"     >>> choice0 = \"It is "
"eaten with a fork and a knife.\"     >>> choice1 = \"It is eaten while "
"held in the hand.\"      >>> visual_embeds = get_visual_embeddings(image)"
"     >>> # (batch_size, num_choices, visual_seq_length, "
"visual_embedding_dim)     >>> visual_embeds = visual_embeds.expand(1, 2, "
"*visual_embeds.shape)     >>> visual_token_type_ids = "
"torch.ones(visual_embeds.shape[:-1], dtype=torch.long)     >>> "
"visual_attention_mask = torch.ones(visual_embeds.shape[:-1], "
"dtype=torch.float)      >>> labels = torch.tensor(0).unsqueeze(0)  # "
"choice0 is correct (according to Wikipedia ;)), batch size 1      >>> "
"encoding = tokenizer([[prompt, prompt], [choice0, choice1]], "
"return_tensors='pt', padding=True)     >>> # batch size is 1     >>> "
"inputs_dict = {{k: v.unsqueeze(0) for k,v in encoding.items()}}     >>> "
"inputs_dict.update({{     ... visual_embeds=visual_embeds,     ... "
"visual_attention_mask=visual_attention_mask,     ... "
"visual_token_type_ids=visual_token_type_ids,     ... labels=labels     "
"... }})     >>> outputs = model(**inputs_dict)      >>> loss = "
"outputs.loss     >>> logits = outputs.logits"
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:75
msgid ""
"A :class:`~transformers.modeling_outputs.MultipleChoiceModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.VisualBertConfig`) and "
"inputs."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:79
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape `(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Classification loss."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:80
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"num_choices)`) -- `num_choices` is the second dimension of the input "
"tensors. (see `input_ids` above)."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:82
msgid "Classification scores (before SoftMax)."
msgstr ""

#: of transformers.VisualBertForMultipleChoice.forward:127
msgid ""
":class:`~transformers.modeling_outputs.MultipleChoiceModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:118
msgid "VisualBertForVisualReasoning"
msgstr ""

#: of transformers.VisualBertForVisualReasoning:1
msgid ""
"VisualBert Model with a sequence classification head on top (a dropout "
"and a linear layer on top of the pooled output) for Visual Reasoning e.g."
" for NLVR task."
msgstr ""

#: of transformers.VisualBertForVisualReasoning.forward:1
msgid ""
"The :class:`~transformers.VisualBertForVisualReasoning` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.VisualBertForVisualReasoning.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple.  labels (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size,)`, `optional`):     Labels for computing the sequence "
"classification/regression loss. Indices should be in :obj:`[0, ...,     "
"config.num_labels - 1]`. A classification loss is computed (Cross-"
"Entropy) against these labels."
msgstr ""

#: of transformers.VisualBertForVisualReasoning.forward:70
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. A classification "
"loss is computed (Cross-Entropy) against these labels."
msgstr ""

#: of transformers.VisualBertForVisualReasoning.forward:74
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.VisualBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"(or regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> # Assumption: "
"`get_visual_embeddings(image)` gets the visual embeddings of the image in"
" the batch.     >>> from transformers import BertTokenizer, "
"VisualBertForVisualReasoning     >>> import torch      >>> tokenizer = "
"BertTokenizer.from_pretrained('bert-base-uncased')     >>> model = "
"VisualBertForVisualReasoning.from_pretrained('uclanlp/visualbert-nlvr2')"
"      >>> text = \"Who is eating the apple?\"     >>> inputs = "
"tokenizer(text, return_tensors='pt')     >>> visual_embeds = "
"get_visual_embeddings(image).unsqueeze(0)     >>> visual_token_type_ids ="
" torch.ones(visual_embeds.shape[:-1], dtype=torch.long) #example     >>> "
"visual_attention_mask = torch.ones(visual_embeds.shape[:-1], "
"dtype=torch.float)      >>> inputs.update({{     ...     "
"\"visual_embeds\": visual_embeds,     ...     \"visual_token_type_ids\": "
"visual_token_type_ids,     ...     \"visual_attention_mask\": "
"visual_attention_mask     ... }})      >>> labels = "
"torch.tensor(1).unsqueeze(0)  # Batch size 1, Num choices 2      >>> "
"outputs = model(**inputs, labels=labels)     >>> loss = outputs.loss     "
">>> scores = outputs.logits"
msgstr ""

#: ../../source/model_doc/visual_bert.rst:125
msgid "VisualBertForRegionToPhraseAlignment"
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment:1
msgid ""
"VisualBert Model with a Masked Language Modeling head and an attention "
"layer on top for Region-to-Phrase Alignment e.g. for Flickr30 Entities "
"task."
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:1
msgid ""
"The :class:`~transformers.VisualBertForRegionToPhraseAlignment` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple.  region_to_phrase_position "
"(:obj:`torch.LongTensor` of shape ``(batch_size, "
"total_sequence_length)``, `optional`):     The positions depicting the "
"position of the image embedding corresponding to the textual tokens.  "
"labels (:obj:`torch.LongTensor` of shape ``(batch_size, "
"total_sequence_length, visual_sequence_length)``, `optional`):     Labels"
" for computing the masked language modeling loss. KLDivLoss is computed "
"against these labels and     the outputs from the attention layer."
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:70
msgid ""
"region_to_phrase_position (:obj:`torch.LongTensor` of shape "
"``(batch_size, total_sequence_length)``, `optional`):"
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:70
msgid ""
"The positions depicting the position of the image embedding corresponding"
" to the textual tokens."
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:73
msgid ""
"labels (:obj:`torch.LongTensor` of shape ``(batch_size, "
"total_sequence_length, visual_sequence_length)``, `optional`):"
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:73
msgid ""
"Labels for computing the masked language modeling loss. KLDivLoss is "
"computed against these labels and the outputs from the attention layer."
msgstr ""

#: of transformers.VisualBertForRegionToPhraseAlignment.forward:77
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.VisualBertConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"(or regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> # Assumption: "
"`get_visual_embeddings(image)` gets the visual embeddings of the image in"
" the batch.     >>> from transformers import BertTokenizer, "
"VisualBertForRegionToPhraseAlignment     >>> import torch      >>> "
"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')     >>> "
"model = VisualBertForRegionToPhraseAlignment.from_pretrained('uclanlp"
"/visualbert-vqa-coco-pre')      >>> text = \"Who is eating the apple?\""
"     >>> inputs = tokenizer(text, return_tensors='pt')     >>> "
"visual_embeds = get_visual_embeddings(image).unsqueeze(0)     >>> "
"visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], "
"dtype=torch.long) #example     >>> visual_attention_mask = "
"torch.ones(visual_embeds.shape[:-1], dtype=torch.float)     >>> "
"region_to_phrase_position = torch.ones((1, "
"inputs[\"input_ids\"].shape[-1]+visual_embeds.shape[-2]))      >>> "
"inputs.update({{     ...     \"region_to_phrase_position\": "
"region_to_phrase_position,     ...     \"visual_embeds\": visual_embeds,"
"     ...     \"visual_token_type_ids\": visual_token_type_ids,     ..."
"     \"visual_attention_mask\": visual_attention_mask     ... }})      "
">>> labels = torch.ones((1, "
"inputs[\"input_ids\"].shape[-1]+visual_embeds.shape[-2], "
"visual_embeds.shape[-2])) # Batch size 1      >>> outputs = "
"model(**inputs, labels=labels)     >>> loss = outputs.loss     >>> scores"
" = outputs.logits"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/gpt_neo.rst:14
msgid "GPT Neo"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:19
msgid ""
"The GPTNeo model was released in the `EleutherAI/gpt-neo "
"<https://github.com/EleutherAI/gpt-neo>`__ repository by Sid Black, "
"Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like "
"causal language model trained on the `Pile <https://pile.eleuther.ai/>`__"
" dataset."
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:23
msgid ""
"The architecture is similar to GPT2 except that GPT Neo uses local "
"attention in every other layer with a window size of 256 tokens."
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:26
msgid ""
"This model was contributed by `valhalla "
"<https://huggingface.co/valhalla>`__."
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:29
msgid "Generation"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:31
msgid ""
"The :obj:`generate()` method can be used to generate text using GPT Neo "
"model."
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:50
msgid "GPTNeoConfig"
msgstr ""

#: of transformers.GPTNeoConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.GPTNeoModel`. It is used to instantiate a GPT Neo "
"model according to the specified arguments, defining the model "
"architecture. Instantiating a configuration with the defaults will yield "
"a similar configuration to that of the GPTNeo `gpt-neo-1.3B "
"<https://huggingface.co/EleutherAI/gpt-neo-1.3B>`__ architecture."
msgstr ""

#: of transformers.GPTNeoConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM
#: transformers.FlaxGPTNeoForCausalLM.__call__ transformers.FlaxGPTNeoModel
#: transformers.FlaxGPTNeoModel.__call__ transformers.GPTNeoConfig
#: transformers.GPTNeoForCausalLM transformers.GPTNeoForCausalLM.forward
#: transformers.GPTNeoForSequenceClassification
#: transformers.GPTNeoForSequenceClassification.forward
#: transformers.GPTNeoModel transformers.GPTNeoModel.forward
msgid "Parameters"
msgstr ""

#: of transformers.GPTNeoConfig:10
msgid ""
"Vocabulary size of the GPT Neo model. Defines the number of different "
"tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.GPTNeoModel`. Vocabulary size of the model."
" Defines the different tokens that can be represented by the `inputs_ids`"
" passed to the forward method of :class:`~transformers.GPTNeoModel`."
msgstr ""

#: of transformers.GPTNeoConfig:15
msgid ""
"The type of attention for each layer in a :obj:`List` of the following "
"format :obj:`[[[\"attention_type\"], num_layerss]]` e.g. for a 24 layer "
"model :obj:`[[[\"global\"], 24]]` or :obj:`[[[\"global\", \"local\"], "
"12]]` Choose the value of ``attention_type`` from :obj:`[\"global\", "
"\"local\"]`"
msgstr ""

#: of transformers.GPTNeoConfig:19
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.GPTNeoConfig:21
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.GPTNeoConfig:23
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.GPTNeoConfig:25
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.GPTNeoConfig:27
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.GPTNeoConfig:30
msgid ""
"The dropout probabilitiy for all fully connected layers in the "
"embeddings, encoder, and pooler."
msgstr ""

#: of transformers.GPTNeoConfig:32
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.GPTNeoConfig:34
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.GPTNeoConfig:37
msgid ""
"The vocabulary size of the :obj:`token_type_ids` passed when calling "
":class:`~transformers.GPTNeoModel`."
msgstr ""

#: of transformers.GPTNeoConfig:39
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.GPTNeoConfig:41
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.GPTNeoConfig:43
msgid ""
"Whether or not the model should return the last key/values attentions "
"(not used by all models). Only relevant if ``config.is_decoder=True``."
msgstr ""

#: of transformers.GPTNeoConfig:46
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.GPTNeoConfig:48
msgid ""
">>> from transformers import GPTNeoModel, GPTNeoConfig  >>> # "
"Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration >>> "
"configuration = GPTNeoConfig()  >>> # Initializing a model from the "
"EleutherAI/gpt-neo-1.3B style configuration >>> model = "
"GPTNeoModel(configuration)  >>> # Accessing the model configuration >>> "
"configuration = model.config"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:57
msgid "GPTNeoModel"
msgstr ""

#: of transformers.GPTNeoModel:1
msgid ""
"The bare GPT Neo Model transformer outputting raw hidden-states without "
"any specific head on top."
msgstr ""

#: of transformers.GPTNeoForCausalLM:5
#: transformers.GPTNeoForSequenceClassification:13 transformers.GPTNeoModel:3
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.GPTNeoForCausalLM:9
#: transformers.GPTNeoForSequenceClassification:17 transformers.GPTNeoModel:7
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.GPTNeoForCausalLM:13
#: transformers.GPTNeoForSequenceClassification:21 transformers.GPTNeoModel:11
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.GPTNeoModel.forward:1
msgid ""
"The :class:`~transformers.GPTNeoModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:4
#: transformers.FlaxGPTNeoModel.__call__:4
#: transformers.GPTNeoForCausalLM.forward:4
#: transformers.GPTNeoForSequenceClassification.forward:4
#: transformers.GPTNeoModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:8
#: transformers.GPTNeoForSequenceClassification.forward:8
#: transformers.GPTNeoModel.forward:8
msgid ""
":obj:`input_ids_length` = ``sequence_length`` if :obj:`past_key_values` "
"is ``None`` else ``past_key_values[0][0].shape[-2]`` (``sequence_length``"
" of input past key value states). Indices of input sequence tokens in the"
" vocabulary.  If :obj:`past_key_values` is used, only ``input_ids`` that "
"do not have their past calculated should be passed as ``input_ids``.  "
"Indices can be obtained using :class:`~transformers.GPTNeoTokenizer`. See"
" :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:8
#: transformers.GPTNeoForSequenceClassification.forward:8
#: transformers.GPTNeoModel.forward:8
msgid ""
":obj:`input_ids_length` = ``sequence_length`` if :obj:`past_key_values` "
"is ``None`` else ``past_key_values[0][0].shape[-2]`` (``sequence_length``"
" of input past key value states). Indices of input sequence tokens in the"
" vocabulary."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:12
#: transformers.GPTNeoForSequenceClassification.forward:12
#: transformers.GPTNeoModel.forward:12
msgid ""
"If :obj:`past_key_values` is used, only ``input_ids`` that do not have "
"their past calculated should be passed as ``input_ids``."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:10
#: transformers.FlaxGPTNeoModel.__call__:10
#: transformers.GPTNeoForCausalLM.forward:15
#: transformers.GPTNeoForSequenceClassification.forward:15
#: transformers.GPTNeoModel.forward:15
msgid ""
"Indices can be obtained using :class:`~transformers.GPTNeoTokenizer`. See"
" :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:14
#: transformers.FlaxGPTNeoModel.__call__:14
#: transformers.GPTNeoForCausalLM.forward:19
#: transformers.GPTNeoForSequenceClassification.forward:19
#: transformers.GPTNeoModel.forward:19
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:21
#: transformers.GPTNeoForSequenceClassification.forward:21
#: transformers.GPTNeoModel.forward:21
msgid ""
"Contains precomputed hidden-states (key and values in the attention "
"blocks) as computed by the model (see :obj:`past_key_values` output "
"below). Can be used to speed up sequential decoding. The ``input_ids`` "
"which have their past given to this model should not be passed as "
"``input_ids`` as they have already been computed."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:16
#: transformers.FlaxGPTNeoModel.__call__:16
#: transformers.GPTNeoForCausalLM.forward:26
#: transformers.GPTNeoForSequenceClassification.forward:26
#: transformers.GPTNeoModel.forward:26
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:16
#: transformers.FlaxGPTNeoModel.__call__:16
#: transformers.GPTNeoForCausalLM.forward:26
#: transformers.GPTNeoForSequenceClassification.forward:26
#: transformers.GPTNeoModel.forward:26
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:18
#: transformers.FlaxGPTNeoModel.__call__:18
#: transformers.GPTNeoForCausalLM.forward:28
#: transformers.GPTNeoForSequenceClassification.forward:28
#: transformers.GPTNeoModel.forward:28
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:19
#: transformers.FlaxGPTNeoModel.__call__:19
#: transformers.GPTNeoForCausalLM.forward:29
#: transformers.GPTNeoForSequenceClassification.forward:29
#: transformers.GPTNeoModel.forward:29
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:21
#: transformers.FlaxGPTNeoModel.__call__:21
#: transformers.GPTNeoForCausalLM.forward:31
#: transformers.GPTNeoForSequenceClassification.forward:31
#: transformers.GPTNeoModel.forward:31
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:33
#: transformers.GPTNeoForSequenceClassification.forward:33
#: transformers.GPTNeoModel.forward:33
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:  - 0 corresponds to a "
"`sentence A` token, - 1 corresponds to a `sentence B` token.  `What are "
"token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:33
#: transformers.GPTNeoForSequenceClassification.forward:33
#: transformers.GPTNeoModel.forward:33
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:36
#: transformers.GPTNeoForSequenceClassification.forward:36
#: transformers.GPTNeoModel.forward:36
msgid "0 corresponds to a `sentence A` token,"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:37
#: transformers.GPTNeoForSequenceClassification.forward:37
#: transformers.GPTNeoModel.forward:37
msgid "1 corresponds to a `sentence B` token."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:39
#: transformers.GPTNeoForSequenceClassification.forward:39
#: transformers.GPTNeoModel.forward:39
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:41
#: transformers.GPTNeoForSequenceClassification.forward:41
#: transformers.GPTNeoModel.forward:41
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``.  `What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:23
#: transformers.FlaxGPTNeoModel.__call__:23
#: transformers.GPTNeoForCausalLM.forward:41
#: transformers.GPTNeoForSequenceClassification.forward:41
#: transformers.GPTNeoModel.forward:41
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:44
#: transformers.GPTNeoForSequenceClassification.forward:44
#: transformers.GPTNeoModel.forward:44
msgid "`What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:46
#: transformers.GPTNeoForSequenceClassification.forward:46
#: transformers.GPTNeoModel.forward:46
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:46
#: transformers.GPTNeoForSequenceClassification.forward:46
#: transformers.GPTNeoModel.forward:46
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:48
#: transformers.GPTNeoForSequenceClassification.forward:48
#: transformers.GPTNeoModel.forward:48
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:49
#: transformers.GPTNeoForSequenceClassification.forward:49
#: transformers.GPTNeoModel.forward:49
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:51
#: transformers.GPTNeoForSequenceClassification.forward:51
#: transformers.GPTNeoModel.forward:51
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix.  If "
":obj:`past_key_values` is used, optionally only the last "
":obj:`inputs_embeds` have to be input (see :obj:`past_key_values`)."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:51
#: transformers.GPTNeoForSequenceClassification.forward:51
#: transformers.GPTNeoModel.forward:51
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:55
#: transformers.GPTNeoForSequenceClassification.forward:55
#: transformers.GPTNeoModel.forward:55
msgid ""
"If :obj:`past_key_values` is used, optionally only the last "
":obj:`inputs_embeds` have to be input (see :obj:`past_key_values`)."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:58
#: transformers.GPTNeoForSequenceClassification.forward:58
#: transformers.GPTNeoModel.forward:58
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:29
#: transformers.FlaxGPTNeoModel.__call__:29
#: transformers.GPTNeoForCausalLM.forward:61
#: transformers.GPTNeoForSequenceClassification.forward:61
#: transformers.GPTNeoModel.forward:61
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:32
#: transformers.FlaxGPTNeoModel.__call__:32
#: transformers.GPTNeoForCausalLM.forward:64
#: transformers.GPTNeoForSequenceClassification.forward:64
#: transformers.GPTNeoModel.forward:64
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:35
#: transformers.FlaxGPTNeoModel.__call__:35
#: transformers.GPTNeoForCausalLM.forward:67
#: transformers.GPTNeoForSequenceClassification.forward:67
#: transformers.GPTNeoModel.forward:67
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__
#: transformers.FlaxGPTNeoModel.__call__ transformers.GPTNeoForCausalLM.forward
#: transformers.GPTNeoForSequenceClassification.forward
#: transformers.GPTNeoModel.forward
msgid "Returns"
msgstr ""

#: of transformers.GPTNeoModel.forward:70
msgid ""
"A "
":class:`~transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.GPTNeoConfig`) and "
"inputs.  - **last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model.    If "
":obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size,   1, hidden_size)` is output. - "
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors   of "
"shape :obj:`(batch_size, num_heads, sequence_length, "
"embed_size_per_head)`) and optionally if   "
"``config.is_encoder_decoder=True`` 2 additional tensors of shape "
":obj:`(batch_size, num_heads,   encoder_sequence_length, "
"embed_size_per_head)`.    Contains pre-computed hidden-states (key and "
"values in the self-attention blocks and optionally if   "
"``config.is_encoder_decoder=True`` in the cross-attention blocks) that "
"can be used (see   :obj:`past_key_values` input) to speed up sequential "
"decoding. - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` and ``config.add_cross_attention=True`` is "
"passed or when ``config.output_attentions=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size,"
" num_heads,   sequence_length, sequence_length)`.    Attentions weights "
"of the decoder's cross-attention layer, after the attention softmax, used"
" to compute the   weighted average in the cross-attention heads."
msgstr ""

#: of transformers.GPTNeoModel.forward:70
msgid ""
"A "
":class:`~transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.GPTNeoConfig`) and "
"inputs."
msgstr ""

#: of transformers.GPTNeoModel.forward:74
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.GPTNeoModel.forward:76
msgid ""
"If :obj:`past_key_values` is used only the last hidden-state of the "
"sequences of shape :obj:`(batch_size, 1, hidden_size)` is output."
msgstr ""

#: of transformers.GPTNeoModel.forward:78
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors of shape"
" :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) "
"and optionally if ``config.is_encoder_decoder=True`` 2 additional tensors"
" of shape :obj:`(batch_size, num_heads, encoder_sequence_length, "
"embed_size_per_head)`."
msgstr ""

#: of transformers.GPTNeoModel.forward:83
msgid ""
"Contains pre-computed hidden-states (key and values in the self-attention"
" blocks and optionally if ``config.is_encoder_decoder=True`` in the "
"cross-attention blocks) that can be used (see :obj:`past_key_values` "
"input) to speed up sequential decoding."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:80
#: transformers.GPTNeoForSequenceClassification.forward:85
#: transformers.GPTNeoModel.forward:86
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:46
#: transformers.FlaxGPTNeoModel.__call__:46
#: transformers.GPTNeoForCausalLM.forward:83
#: transformers.GPTNeoForSequenceClassification.forward:88
#: transformers.GPTNeoModel.forward:89
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:84
#: transformers.GPTNeoForSequenceClassification.forward:89
#: transformers.GPTNeoModel.forward:90
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:50
#: transformers.FlaxGPTNeoModel.__call__:50
#: transformers.GPTNeoForCausalLM.forward:87
#: transformers.GPTNeoForSequenceClassification.forward:92
#: transformers.GPTNeoModel.forward:93
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.GPTNeoModel.forward:95
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` and "
"``config.add_cross_attention=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.GPTNeoModel.forward:98
msgid ""
"Attentions weights of the decoder's cross-attention layer, after the "
"attention softmax, used to compute the weighted average in the cross-"
"attention heads."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__
#: transformers.FlaxGPTNeoModel.__call__ transformers.GPTNeoForCausalLM.forward
#: transformers.GPTNeoForSequenceClassification.forward
#: transformers.GPTNeoModel.forward
msgid "Return type"
msgstr ""

#: of transformers.GPTNeoModel.forward:100
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:54
#: transformers.FlaxGPTNeoModel.__call__:54
#: transformers.GPTNeoForCausalLM.forward:102
#: transformers.GPTNeoForSequenceClassification.forward:96
#: transformers.GPTNeoModel.forward:102
msgid "Example::"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:64
msgid "GPTNeoForCausalLM"
msgstr ""

#: of transformers.GPTNeoForCausalLM:1
msgid ""
"The GPT Neo Model transformer with a language modeling head on top "
"(linear layer with weights tied to the input embeddings)."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:1
msgid ""
"The :class:`~transformers.GPTNeoForCausalLM` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:69
msgid ""
"Labels for language modeling. Note that the labels **are shifted** inside"
" the model, i.e. you can set ``labels = input_ids`` Indices are selected "
"in ``[-100, 0, ..., config.vocab_size]`` All labels set to ``-100`` are "
"ignored (masked), the loss is only computed for labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:74
msgid ""
"A "
":class:`~transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.GPTNeoConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Language modeling"
" loss (for next-token prediction). - **logits** (:obj:`torch.FloatTensor`"
" of shape :obj:`(batch_size, sequence_length, config.vocab_size)`) -- "
"Prediction scores of the language modeling head (scores for each "
"vocabulary token before SoftMax). - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads. - **cross_attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Cross attentions weights after the"
" attention softmax, used to compute the weighted average in the   cross-"
"attention heads. - **past_key_values** "
"(:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- Tuple "
"of :obj:`torch.FloatTensor` tuples of length :obj:`config.n_layers`, with"
" each tuple containing the   cached key, value states of the self-"
"attention and the cross-attention layers if model is used in   encoder-"
"decoder setting. Only relevant if ``config.is_decoder = True``.    "
"Contains pre-computed hidden-states (key and values in the attention "
"blocks) that can be used (see   :obj:`past_key_values` input) to speed up"
" sequential decoding."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:74
msgid ""
"A "
":class:`~transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.GPTNeoConfig`) and "
"inputs."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:78
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction)."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:79
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:89
msgid ""
"**cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:92
msgid ""
"Cross attentions weights after the attention softmax, used to compute the"
" weighted average in the cross-attention heads."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:94
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`torch.FloatTensor` tuples of"
" length :obj:`config.n_layers`, with each tuple containing the cached "
"key, value states of the self-attention and the cross-attention layers if"
" model is used in encoder-decoder setting. Only relevant if "
"``config.is_decoder = True``."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:98
msgid ""
"Contains pre-computed hidden-states (key and values in the attention "
"blocks) that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding."
msgstr ""

#: of transformers.GPTNeoForCausalLM.forward:100
msgid ""
":class:`~transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:70
msgid "GPTNeoForSequenceClassification"
msgstr ""

#: of transformers.GPTNeoForSequenceClassification:1
msgid ""
"The GPTNeo Model transformer with a sequence classification head on top "
"(linear layer)."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification:3
msgid ""
":class:`~transformers.GPTNeoForSequenceClassification` uses the last "
"token in order to do the classification, as other causal models (e.g. "
"GPT-1) do."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification:6
msgid ""
"Since it does classification on the last token, it requires to know the "
"position of the last token. If a :obj:`pad_token_id` is defined in the "
"configuration, it finds the last token that is not a padding token in "
"each row. If no :obj:`pad_token_id` is defined, it simply takes the last "
"value in each row of the batch. Since it cannot guess the padding tokens "
"when :obj:`inputs_embeds` are passed instead of :obj:`input_ids`, it does"
" the same (take the last value in each row of the batch)."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:1
msgid ""
"The :class:`~transformers.GPTNeoForSequenceClassification` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:69
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy)."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:74
msgid ""
"A "
":class:`~transformers.modeling_outputs.SequenceClassifierOutputWithPast` "
"or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.GPTNeoConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"(or regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **past_key_values** "
"(:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- Tuple "
"of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with"
" each tuple having 2 tensors   of shape :obj:`(batch_size, num_heads, "
"sequence_length, embed_size_per_head)`)    Contains pre-computed hidden-"
"states (key and values in the self-attention blocks) that can be used "
"(see   :obj:`past_key_values` input) to speed up sequential decoding. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:74
msgid ""
"A "
":class:`~transformers.modeling_outputs.SequenceClassifierOutputWithPast` "
"or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.GPTNeoConfig`) and "
"inputs."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:78
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:79
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:80
msgid ""
"**past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, "
"returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of"
" length :obj:`config.n_layers`, with each tuple having 2 tensors of shape"
" :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`)"
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:83
msgid ""
"Contains pre-computed hidden-states (key and values in the self-attention"
" blocks) that can be used (see :obj:`past_key_values` input) to speed up "
"sequential decoding."
msgstr ""

#: of transformers.GPTNeoForSequenceClassification.forward:94
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutputWithPast` "
"or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:76
msgid "FlaxGPTNeoModel"
msgstr ""

#: of transformers.FlaxGPTNeoModel:1
msgid ""
"The bare GPTNeo Model transformer outputting raw hidden-states without "
"any specific head on top."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:5 transformers.FlaxGPTNeoModel:3
msgid ""
"This model inherits from :class:`~transformers.FlaxPreTrainedModel`. "
"Check the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:9 transformers.FlaxGPTNeoModel:7
msgid ""
"This model is also a Flax Linen `flax.nn.Module "
"<https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html>`__"
" subclass. Use it as a regular Flax Module and refer to the Flax "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:13 transformers.FlaxGPTNeoModel:11
msgid "Finally, this model supports inherent JAX features such as:"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:15 transformers.FlaxGPTNeoModel:13
msgid ""
"`Just-In-Time (JIT) compilation "
"<https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-"
"jit>`__"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:16 transformers.FlaxGPTNeoModel:14
msgid ""
"`Automatic Differentiation <https://jax.readthedocs.io/en/latest/jax.html"
"#automatic-differentiation>`__"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:17 transformers.FlaxGPTNeoModel:15
msgid ""
"`Vectorization <https://jax.readthedocs.io/en/latest/jax.html"
"#vectorization-vmap>`__"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:18 transformers.FlaxGPTNeoModel:16
msgid ""
"`Parallelization <https://jax.readthedocs.io/en/latest/jax.html"
"#parallelization-pmap>`__"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:20 transformers.FlaxGPTNeoModel:18
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.FlaxPreTrainedModel.from_pretrained` method to load "
"the model weights."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:1
#: transformers.FlaxGPTNeoModel.__call__:1
msgid ""
"The :class:`~transformers.FlaxGPTNeoPreTrainedModel` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:8
#: transformers.FlaxGPTNeoModel.__call__:8
msgid ""
":obj:`input_ids_length` = ``sequence_length``. Indices of input sequence "
"tokens in the vocabulary.  Indices can be obtained using "
":class:`~transformers.GPTNeoTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:8
#: transformers.FlaxGPTNeoModel.__call__:8
msgid ""
":obj:`input_ids_length` = ``sequence_length``. Indices of input sequence "
"tokens in the vocabulary."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:26
#: transformers.FlaxGPTNeoModel.__call__:26
msgid ""
"Dictionary of pre-computed hidden-states (key and values in the attention"
" blocks) that can be used for fast auto-regressive decoding. Pre-computed"
" key and value hidden-states are of shape `[batch_size, max_length]`."
msgstr ""

#: of transformers.FlaxGPTNeoModel.__call__:38
msgid ""
"A :class:`~transformers.modeling_flax_outputs.FlaxBaseModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.GPTNeoConfig`) and inputs.  -"
" **last_hidden_state** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model. - **hidden_states** "
"(:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxGPTNeoModel.__call__:38
msgid ""
"A :class:`~transformers.modeling_flax_outputs.FlaxBaseModelOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.GPTNeoConfig`) and inputs."
msgstr ""

#: of transformers.FlaxGPTNeoModel.__call__:42
msgid ""
"**last_hidden_state** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:43
#: transformers.FlaxGPTNeoModel.__call__:43
msgid ""
"**hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:47
#: transformers.FlaxGPTNeoModel.__call__:47
msgid ""
"**attentions** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.FlaxGPTNeoModel.__call__:52
msgid ""
":class:`~transformers.modeling_flax_outputs.FlaxBaseModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/gpt_neo.rst:83
msgid "FlaxGPTNeoForCausalLM"
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM:1
msgid ""
"The GPTNeo Model transformer with a language modeling head on top (linear"
" layer with weights tied to the input embeddings)."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:38
msgid ""
"A :class:`~transformers.modeling_flax_outputs.FlaxMaskedLMOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.GPTNeoConfig`) and inputs.  -"
" **logits** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
" - **hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:38
msgid ""
"A :class:`~transformers.modeling_flax_outputs.FlaxMaskedLMOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.GPTNeoConfig`) and inputs."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:42
msgid ""
"**logits** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.FlaxGPTNeoForCausalLM.__call__:52
msgid ""
":class:`~transformers.modeling_flax_outputs.FlaxMaskedLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/dpr.rst:14
msgid "DPR"
msgstr ""

#: ../../source/model_doc/dpr.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/dpr.rst:19
msgid ""
"Dense Passage Retrieval (DPR) is a set of tools and models for state-of-"
"the-art open-domain Q&A research. It was introduced in `Dense Passage "
"Retrieval for Open-Domain Question Answering "
"<https://arxiv.org/abs/2004.04906>`__ by Vladimir Karpukhin, Barlas OÄŸuz,"
" Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau "
"Yih."
msgstr ""

#: ../../source/model_doc/dpr.rst:23
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/dpr.rst:25
#, python-format
msgid ""
"*Open-domain question answering relies on efficient passage retrieval to "
"select candidate contexts, where traditional sparse vector space models, "
"such as TF-IDF or BM25, are the de facto method. In this work, we show "
"that retrieval can be practically implemented using dense representations"
" alone, where embeddings are learned from a small number of questions and"
" passages by a simple dual-encoder framework. When evaluated on a wide "
"range of open-domain QA datasets, our dense retriever outperforms a "
"strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 "
"passage retrieval accuracy, and helps our end-to-end QA system establish "
"new state-of-the-art on multiple open-domain QA benchmarks.*"
msgstr ""

#: ../../source/model_doc/dpr.rst:33
msgid ""
"This model was contributed by `lhoestq "
"<https://huggingface.co/lhoestq>`__. The original code can be found `here"
" <https://github.com/facebookresearch/DPR>`__."
msgstr ""

#: ../../source/model_doc/dpr.rst:38
msgid "DPRConfig"
msgstr ""

#: of transformers.DPRConfig:1
msgid ""
":class:`~transformers.DPRConfig` is the configuration class to store the "
"configuration of a `DPRModel`."
msgstr ""

#: of transformers.DPRConfig:3
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.DPRContextEncoder`, "
":class:`~transformers.DPRQuestionEncoder`, or a "
":class:`~transformers.DPRReader`. It is used to instantiate the "
"components of the DPR model."
msgstr ""

#: of transformers.DPRConfig:7
msgid ""
"This class is a subclass of :class:`~transformers.BertConfig`. Please "
"check the superclass for the documentation of all kwargs."
msgstr ""

#: of transformers.DPRConfig transformers.DPRContextEncoder
#: transformers.DPRContextEncoder.forward transformers.DPRQuestionEncoder
#: transformers.DPRQuestionEncoder.forward transformers.DPRReader
#: transformers.DPRReader.forward transformers.DPRReaderTokenizer
#: transformers.DPRReaderTokenizerFast transformers.TFDPRContextEncoder
#: transformers.TFDPRContextEncoder.call transformers.TFDPRQuestionEncoder
#: transformers.TFDPRQuestionEncoder.call transformers.TFDPRReader
#: transformers.TFDPRReader.call
#: transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput
msgid "Parameters"
msgstr ""

#: of transformers.DPRConfig:10
msgid ""
"Vocabulary size of the DPR model. Defines the different tokens that can "
"be represented by the `inputs_ids` passed to the forward method of "
":class:`~transformers.BertModel`."
msgstr ""

#: of transformers.DPRConfig:13
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.DPRConfig:15
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.DPRConfig:17
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.DPRConfig:19
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.DPRConfig:21
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.DPRConfig:24
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.DPRConfig:26
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.DPRConfig:28
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.DPRConfig:31
msgid ""
"The vocabulary size of the `token_type_ids` passed into "
":class:`~transformers.BertModel`."
msgstr ""

#: of transformers.DPRConfig:33
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.DPRConfig:35
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.DPRConfig:37
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.DPRConfig:39
msgid ""
"Type of position embedding. Choose one of :obj:`\"absolute\"`, "
":obj:`\"relative_key\"`, :obj:`\"relative_key_query\"`. For positional "
"embeddings use :obj:`\"absolute\"`. For more information on "
":obj:`\"relative_key\"`, please refer to `Self-Attention with Relative "
"Position Representations (Shaw et al.) "
"<https://arxiv.org/abs/1803.02155>`__. For more information on "
":obj:`\"relative_key_query\"`, please refer to `Method 4` in `Improve "
"Transformer Models with Better Relative Position Embeddings (Huang et "
"al.) <https://arxiv.org/abs/2009.13658>`__."
msgstr ""

#: of transformers.DPRConfig:46
msgid ""
"Dimension of the projection for the context and question encoders. If it "
"is set to zero (default), then no projection is done."
msgstr ""

#: ../../source/model_doc/dpr.rst:45
msgid "DPRContextEncoderTokenizer"
msgstr ""

#: of transformers.DPRContextEncoderTokenizer:1
msgid "Construct a DPRContextEncoder tokenizer."
msgstr ""

#: of transformers.DPRContextEncoderTokenizer:3
msgid ""
":class:`~transformers.DPRContextEncoderTokenizer` is identical to "
":class:`~transformers.BertTokenizer` and runs end-to-end tokenization: "
"punctuation splitting and wordpiece."
msgstr ""

#: of transformers.DPRContextEncoderTokenizer:6
#: transformers.DPRQuestionEncoderTokenizer:6 transformers.DPRReaderTokenizer:7
msgid ""
"Refer to superclass :class:`~transformers.BertTokenizer` for usage "
"examples and documentation concerning parameters."
msgstr ""

#: ../../source/model_doc/dpr.rst:52
msgid "DPRContextEncoderTokenizerFast"
msgstr ""

#: of transformers.DPRContextEncoderTokenizerFast:1
msgid ""
"Construct a \"fast\" DPRContextEncoder tokenizer (backed by HuggingFace's"
" `tokenizers` library)."
msgstr ""

#: of transformers.DPRContextEncoderTokenizerFast:3
msgid ""
":class:`~transformers.DPRContextEncoderTokenizerFast` is identical to "
":class:`~transformers.BertTokenizerFast` and runs end-to-end "
"tokenization: punctuation splitting and wordpiece."
msgstr ""

#: of transformers.DPRContextEncoderTokenizerFast:6
#: transformers.DPRQuestionEncoderTokenizerFast:6
#: transformers.DPRReaderTokenizerFast:7
msgid ""
"Refer to superclass :class:`~transformers.BertTokenizerFast` for usage "
"examples and documentation concerning parameters."
msgstr ""

#: ../../source/model_doc/dpr.rst:58
msgid "DPRQuestionEncoderTokenizer"
msgstr ""

#: of transformers.DPRQuestionEncoderTokenizer:1
msgid "Constructs a DPRQuestionEncoder tokenizer."
msgstr ""

#: of transformers.DPRQuestionEncoderTokenizer:3
msgid ""
":class:`~transformers.DPRQuestionEncoderTokenizer` is identical to "
":class:`~transformers.BertTokenizer` and runs end-to-end tokenization: "
"punctuation splitting and wordpiece."
msgstr ""

#: ../../source/model_doc/dpr.rst:65
msgid "DPRQuestionEncoderTokenizerFast"
msgstr ""

#: of transformers.DPRQuestionEncoderTokenizerFast:1
msgid ""
"Constructs a \"fast\" DPRQuestionEncoder tokenizer (backed by "
"HuggingFace's `tokenizers` library)."
msgstr ""

#: of transformers.DPRQuestionEncoderTokenizerFast:3
msgid ""
":class:`~transformers.DPRQuestionEncoderTokenizerFast` is identical to "
":class:`~transformers.BertTokenizerFast` and runs end-to-end "
"tokenization: punctuation splitting and wordpiece."
msgstr ""

#: ../../source/model_doc/dpr.rst:71
msgid "DPRReaderTokenizer"
msgstr ""

#: of transformers.DPRReaderTokenizer:1
msgid "Construct a DPRReader tokenizer."
msgstr ""

#: of transformers.DPRReaderTokenizer:3
msgid ""
":class:`~transformers.DPRReaderTokenizer` is almost identical to "
":class:`~transformers.BertTokenizer` and runs end-to-end tokenization: "
"punctuation splitting and wordpiece. The difference is that is has three "
"inputs strings: question, titles and texts that are combined to be fed to"
" the :class:`~transformers.DPRReader` model."
msgstr ""

#: of transformers.DPRReaderTokenizer:10 transformers.DPRReaderTokenizerFast:11
msgid ""
"Return a dictionary with the token ids of the input strings and other "
"information to give to :obj:`.decode_best_spans`. It converts the strings"
" of a question and different passages (title and text) in a sequence of "
"IDs (integers), using the tokenizer and vocabulary. The resulting "
":obj:`input_ids` is a matrix of size :obj:`(n_passages, sequence_length)`"
" with the format:"
msgstr ""

#: of transformers.DPRReaderTokenizer:19 transformers.DPRReaderTokenizerFast:18
msgid ""
"The questions to be encoded. You can specify one question for many "
"passages. In this case, the question will be duplicated like "
":obj:`[questions] * n_passages`. Otherwise you have to specify as many "
"questions as in :obj:`titles` or :obj:`texts`."
msgstr ""

#: of transformers.DPRReaderTokenizer:23 transformers.DPRReaderTokenizerFast:22
msgid ""
"The passages titles to be encoded. This can be a string or a list of "
"strings if there are several passages."
msgstr ""

#: of transformers.DPRReaderTokenizer:25 transformers.DPRReaderTokenizerFast:24
msgid ""
"The passages texts to be encoded. This can be a string or a list of "
"strings if there are several passages."
msgstr ""

#: of transformers.DPRReaderTokenizer:27 transformers.DPRReaderTokenizerFast:26
msgid ""
"Activates and controls padding. Accepts the following values:  * "
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single   sequence if provided). * "
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the   maximum acceptable input length for the "
"model if that argument is not provided. * :obj:`False` or "
":obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with "
"sequences of   different lengths)."
msgstr ""

#: of transformers.DPRReaderTokenizer:27 transformers.DPRReaderTokenizerFast:26
msgid "Activates and controls padding. Accepts the following values:"
msgstr ""

#: of transformers.DPRReaderTokenizer:29 transformers.DPRReaderTokenizerFast:28
msgid ""
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single sequence if provided)."
msgstr ""

#: of transformers.DPRReaderTokenizer:31 transformers.DPRReaderTokenizerFast:30
msgid ""
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the maximum acceptable input length for the model"
" if that argument is not provided."
msgstr ""

#: of transformers.DPRReaderTokenizer:33 transformers.DPRReaderTokenizerFast:32
msgid ""
":obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can "
"output a batch with sequences of different lengths)."
msgstr ""

#: of transformers.DPRReaderTokenizer:36 transformers.DPRReaderTokenizerFast:35
msgid ""
"Activates and controls truncation. Accepts the following values:  * "
":obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length "
"specified with the argument   :obj:`max_length` or to the maximum "
"acceptable input length for the model if that argument is not   provided."
" This will truncate token by token, removing a token from the longest "
"sequence in the pair if a   pair of sequences (or a batch of pairs) is "
"provided. * :obj:`'only_first'`: Truncate to a maximum length specified "
"with the argument :obj:`max_length` or to the   maximum acceptable input "
"length for the model if that argument is not provided. This will only "
"truncate   the first sequence of a pair if a pair of sequences (or a "
"batch of pairs) is provided. * :obj:`'only_second'`: Truncate to a "
"maximum length specified with the argument :obj:`max_length` or to   the "
"maximum acceptable input length for the model if that argument is not "
"provided. This will only   truncate the second sequence of a pair if a "
"pair of sequences (or a batch of pairs) is provided. * :obj:`False` or "
":obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch"
" with sequence   lengths greater than the model maximum admissible input "
"size)."
msgstr ""

#: of transformers.DPRReaderTokenizer:36 transformers.DPRReaderTokenizerFast:35
msgid "Activates and controls truncation. Accepts the following values:"
msgstr ""

#: of transformers.DPRReaderTokenizer:38 transformers.DPRReaderTokenizerFast:37
msgid ""
":obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length "
"specified with the argument :obj:`max_length` or to the maximum "
"acceptable input length for the model if that argument is not provided. "
"This will truncate token by token, removing a token from the longest "
"sequence in the pair if a pair of sequences (or a batch of pairs) is "
"provided."
msgstr ""

#: of transformers.DPRReaderTokenizer:42 transformers.DPRReaderTokenizerFast:41
msgid ""
":obj:`'only_first'`: Truncate to a maximum length specified with the "
"argument :obj:`max_length` or to the maximum acceptable input length for "
"the model if that argument is not provided. This will only truncate the "
"first sequence of a pair if a pair of sequences (or a batch of pairs) is "
"provided."
msgstr ""

#: of transformers.DPRReaderTokenizer:45 transformers.DPRReaderTokenizerFast:44
msgid ""
":obj:`'only_second'`: Truncate to a maximum length specified with the "
"argument :obj:`max_length` or to the maximum acceptable input length for "
"the model if that argument is not provided. This will only truncate the "
"second sequence of a pair if a pair of sequences (or a batch of pairs) is"
" provided."
msgstr ""

#: of transformers.DPRReaderTokenizer:48 transformers.DPRReaderTokenizerFast:47
msgid ""
":obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., "
"can output batch with sequence lengths greater than the model maximum "
"admissible input size)."
msgstr ""

#: of transformers.DPRReaderTokenizer:51 transformers.DPRReaderTokenizerFast:50
msgid ""
"Controls the maximum length to use by one of the truncation/padding "
"parameters.  If left unset or set to :obj:`None`, this will use the "
"predefined model maximum length if a maximum length is required by one of"
" the truncation/padding parameters. If the model has no specific maximum "
"input length (like XLNet) truncation/padding to a maximum length will be "
"deactivated."
msgstr ""

#: of transformers.DPRReaderTokenizer:51 transformers.DPRReaderTokenizerFast:50
msgid ""
"Controls the maximum length to use by one of the truncation/padding "
"parameters."
msgstr ""

#: of transformers.DPRReaderTokenizer:53 transformers.DPRReaderTokenizerFast:52
msgid ""
"If left unset or set to :obj:`None`, this will use the predefined model "
"maximum length if a maximum length is required by one of the "
"truncation/padding parameters. If the model has no specific maximum input"
" length (like XLNet) truncation/padding to a maximum length will be "
"deactivated."
msgstr ""

#: of transformers.DPRReaderTokenizer:57 transformers.DPRReaderTokenizerFast:56
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:  * :obj:`'tf'`: Return TensorFlow "
":obj:`tf.constant` objects. * :obj:`'pt'`: Return PyTorch "
":obj:`torch.Tensor` objects. * :obj:`'np'`: Return Numpy "
":obj:`np.ndarray` objects."
msgstr ""

#: of transformers.DPRReaderTokenizer:57 transformers.DPRReaderTokenizerFast:56
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:"
msgstr ""

#: of transformers.DPRReaderTokenizer:59 transformers.DPRReaderTokenizerFast:58
msgid ":obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects."
msgstr ""

#: of transformers.DPRReaderTokenizer:60 transformers.DPRReaderTokenizerFast:59
msgid ":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.DPRReaderTokenizer:61 transformers.DPRReaderTokenizerFast:60
msgid ":obj:`'np'`: Return Numpy :obj:`np.ndarray` objects."
msgstr ""

#: of transformers.DPRReaderTokenizer:63 transformers.DPRReaderTokenizerFast:62
msgid ""
"Whether or not to return the attention mask. If not set, will return the "
"attention mask according to the specific tokenizer's default, defined by "
"the :obj:`return_outputs` attribute.  `What are attention masks? "
"<../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.DPRReaderTokenizer:63 transformers.DPRReaderTokenizerFast:62
msgid ""
"Whether or not to return the attention mask. If not set, will return the "
"attention mask according to the specific tokenizer's default, defined by "
"the :obj:`return_outputs` attribute."
msgstr ""

#: of transformers.DPRContextEncoder.forward:39
#: transformers.DPRQuestionEncoder.forward:39 transformers.DPRReader.forward:27
#: transformers.DPRReaderTokenizer:66 transformers.DPRReaderTokenizerFast:65
#: transformers.TFDPRContextEncoder.call:39
#: transformers.TFDPRQuestionEncoder.call:39 transformers.TFDPRReader.call:25
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.DPRContextEncoder.forward
#: transformers.DPRQuestionEncoder.forward transformers.DPRReader.forward
#: transformers.DPRReaderTokenizer transformers.DPRReaderTokenizerFast
#: transformers.TFDPRContextEncoder.call transformers.TFDPRQuestionEncoder.call
#: transformers.TFDPRReader.call
msgid "Returns"
msgstr ""

#: of transformers.DPRReaderTokenizer:69 transformers.DPRReaderTokenizerFast:68
msgid ""
"A dictionary with the following keys:  - ``input_ids``: List of token ids"
" to be fed to a model. - ``attention_mask``: List of indices specifying "
"which tokens should be attended to by the model."
msgstr ""

#: of transformers.DPRReaderTokenizer:69 transformers.DPRReaderTokenizerFast:68
msgid "A dictionary with the following keys:"
msgstr ""

#: of transformers.DPRReaderTokenizer:71 transformers.DPRReaderTokenizerFast:70
msgid "``input_ids``: List of token ids to be fed to a model."
msgstr ""

#: of transformers.DPRReaderTokenizer:72 transformers.DPRReaderTokenizerFast:71
msgid ""
"``attention_mask``: List of indices specifying which tokens should be "
"attended to by the model."
msgstr ""

#: of transformers.DPRContextEncoder.forward
#: transformers.DPRQuestionEncoder.forward transformers.DPRReader.forward
#: transformers.DPRReaderTokenizer transformers.DPRReaderTokenizerFast
#: transformers.TFDPRContextEncoder.call transformers.TFDPRQuestionEncoder.call
#: transformers.TFDPRReader.call
msgid "Return type"
msgstr ""

#: of transformers.DPRReaderTokenizer:73 transformers.DPRReaderTokenizerFast:72
msgid ":obj:`Dict[str, List[List[int]]]`"
msgstr ""

#: ../../source/model_doc/dpr.rst:78
msgid "DPRReaderTokenizerFast"
msgstr ""

#: of transformers.DPRReaderTokenizerFast:1
msgid ""
"Constructs a \"fast\" DPRReader tokenizer (backed by HuggingFace's "
"`tokenizers` library)."
msgstr ""

#: of transformers.DPRReaderTokenizerFast:3
msgid ""
":class:`~transformers.DPRReaderTokenizerFast` is almost identical to "
":class:`~transformers.BertTokenizerFast` and runs end-to-end "
"tokenization: punctuation splitting and wordpiece. The difference is that"
" is has three inputs strings: question, titles and texts that are "
"combined to be fed to the :class:`~transformers.DPRReader` model."
msgstr ""

#: of transformers.DPRReaderTokenizerFast:16
msgid "[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"
msgstr ""

#: ../../source/model_doc/dpr.rst:85
msgid "DPR specific outputs"
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:1
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:1
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:1
msgid "Class for outputs of :class:`~transformers.DPRQuestionEncoder`."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:3
msgid ""
"(:obj:``torch.FloatTensor`` of shape ``(batch_size, embeddings_size)``): "
"The DPR encoder outputs the `pooler_output` that corresponds to the "
"context representation. Last layer hidden-state of the first token of the"
" sequence (classification token) further processed by a Linear layer. "
"This output is to be used to embed contexts for nearest neighbors queries"
" with questions embeddings."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:7
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:7
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:10
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the model at the output"
" of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:7
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:7
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:10
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.DPRContextEncoder.forward:72
#: transformers.DPRQuestionEncoder.forward:72 transformers.DPRReader.forward:53
#: transformers.TFDPRContextEncoder.call:78
#: transformers.TFDPRQuestionEncoder.call:78 transformers.TFDPRReader.call:53
#: transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:10
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:10
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:13
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:12
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:12
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:15
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:12
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:12
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:15
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.DPRContextEncoder.forward:76
#: transformers.DPRQuestionEncoder.forward:76 transformers.DPRReader.forward:57
#: transformers.TFDPRContextEncoder.call:82
#: transformers.TFDPRQuestionEncoder.call:82 transformers.TFDPRReader.call:57
#: transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput:15
#: transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:15
#: transformers.models.dpr.modeling_dpr.DPRReaderOutput:18
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput:3
msgid ""
"(:obj:``torch.FloatTensor`` of shape ``(batch_size, embeddings_size)``): "
"The DPR encoder outputs the `pooler_output` that corresponds to the "
"question representation. Last layer hidden-state of the first token of "
"the sequence (classification token) further processed by a Linear layer. "
"This output is to be used to embed questions for nearest neighbors "
"queries with context embeddings."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRReaderOutput:3
msgid ""
"(:obj:``torch.FloatTensor`` of shape ``(n_passages, sequence_length)``): "
"Logits of the start index of the span for each passage."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRReaderOutput:5
msgid ""
"(:obj:``torch.FloatTensor`` of shape ``(n_passages, sequence_length)``): "
"Logits of the end index of the span for each passage."
msgstr ""

#: of transformers.models.dpr.modeling_dpr.DPRReaderOutput:7
msgid ""
"(:obj:`torch.FloatTensor`` of shape ``(n_passages, )``): Outputs of the "
"QA classifier of the DPRReader that corresponds to the scores of each "
"passage to answer the question, compared to all the other passages."
msgstr ""

#: ../../source/model_doc/dpr.rst:98
msgid "DPRContextEncoder"
msgstr ""

#: of transformers.DPRContextEncoder:1 transformers.TFDPRContextEncoder:1
msgid ""
"The bare DPRContextEncoder transformer outputting pooler outputs as "
"context representations."
msgstr ""

#: of transformers.DPRContextEncoder:3 transformers.DPRQuestionEncoder:3
#: transformers.DPRReader:3
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.DPRContextEncoder:7 transformers.DPRQuestionEncoder:7
#: transformers.DPRReader:7
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.DPRContextEncoder:11 transformers.DPRQuestionEncoder:11
#: transformers.DPRReader:11
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.DPRContextEncoder.forward:1
msgid ""
"The :class:`~transformers.DPRContextEncoder` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.DPRContextEncoder.forward:4
#: transformers.DPRQuestionEncoder.forward:4 transformers.DPRReader.forward:4
#: transformers.TFDPRContextEncoder.call:4
#: transformers.TFDPRQuestionEncoder.call:4 transformers.TFDPRReader.call:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.DPRContextEncoder.forward:8
#: transformers.DPRQuestionEncoder.forward:8
#: transformers.TFDPRContextEncoder.call:8
#: transformers.TFDPRQuestionEncoder.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary. To match pretraining,"
" DPR input sequence should be formatted with [CLS] and [SEP] tokens as "
"follows:  (a) For sequence pairs (for a pair title+text for example):  ::"
"      tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is "
"not . [SEP]     token_type_ids:   0   0  0    0    0     0       0   0   "
"1  1  1  1   1   1  (b) For single sequences (for a question for "
"example):  ::      tokens:         [CLS] the dog is hairy . [SEP]     "
"token_type_ids:   0   0   0   0  0     0   0  DPR is a model with "
"absolute position embeddings so it's usually advised to pad the inputs on"
" the right rather than the left.  Indices can be obtained using "
":class:`~transformers.DPRTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.DPRContextEncoder.forward:8
#: transformers.DPRQuestionEncoder.forward:8
#: transformers.TFDPRContextEncoder.call:8
#: transformers.TFDPRQuestionEncoder.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary. To match pretraining,"
" DPR input sequence should be formatted with [CLS] and [SEP] tokens as "
"follows:"
msgstr ""

#: of transformers.DPRContextEncoder.forward:11
#: transformers.DPRQuestionEncoder.forward:11
#: transformers.TFDPRContextEncoder.call:11
#: transformers.TFDPRQuestionEncoder.call:11
msgid "For sequence pairs (for a pair title+text for example):"
msgstr ""

#: of transformers.DPRContextEncoder.forward:18
#: transformers.DPRQuestionEncoder.forward:18
#: transformers.TFDPRContextEncoder.call:18
#: transformers.TFDPRQuestionEncoder.call:18
msgid "For single sequences (for a question for example):"
msgstr ""

#: of transformers.DPRContextEncoder.forward:25
#: transformers.DPRQuestionEncoder.forward:25 transformers.DPRReader.forward:15
#: transformers.TFDPRContextEncoder.call:25
#: transformers.TFDPRQuestionEncoder.call:25 transformers.TFDPRReader.call:15
msgid ""
"DPR is a model with absolute position embeddings so it's usually advised "
"to pad the inputs on the right rather than the left."
msgstr ""

#: of transformers.DPRContextEncoder.forward:28
#: transformers.DPRQuestionEncoder.forward:28
#: transformers.TFDPRContextEncoder.call:28
#: transformers.TFDPRQuestionEncoder.call:28
msgid ""
"Indices can be obtained using :class:`~transformers.DPRTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.DPRContextEncoder.forward:32
#: transformers.DPRQuestionEncoder.forward:32 transformers.DPRReader.forward:21
#: transformers.TFDPRContextEncoder.call:32
#: transformers.TFDPRQuestionEncoder.call:32
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.DPRContextEncoder.forward:34
#: transformers.DPRQuestionEncoder.forward:34 transformers.DPRReader.forward:22
#: transformers.TFDPRContextEncoder.call:34
#: transformers.TFDPRQuestionEncoder.call:34 transformers.TFDPRReader.call:20
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.DPRContextEncoder.forward:34
#: transformers.DPRQuestionEncoder.forward:34 transformers.DPRReader.forward:22
#: transformers.TFDPRContextEncoder.call:34
#: transformers.TFDPRQuestionEncoder.call:34 transformers.TFDPRReader.call:20
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.DPRContextEncoder.forward:36
#: transformers.DPRQuestionEncoder.forward:36 transformers.DPRReader.forward:24
#: transformers.TFDPRContextEncoder.call:36
#: transformers.TFDPRQuestionEncoder.call:36 transformers.TFDPRReader.call:22
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.DPRContextEncoder.forward:37
#: transformers.DPRQuestionEncoder.forward:37 transformers.DPRReader.forward:25
#: transformers.TFDPRContextEncoder.call:37
#: transformers.TFDPRQuestionEncoder.call:37 transformers.TFDPRReader.call:23
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.DPRContextEncoder.forward:41
#: transformers.DPRQuestionEncoder.forward:41
#: transformers.TFDPRContextEncoder.call:41
#: transformers.TFDPRQuestionEncoder.call:41
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:  - 0 corresponds to a "
"`sentence A` token, - 1 corresponds to a `sentence B` token.  `What are "
"token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.DPRContextEncoder.forward:41
#: transformers.DPRQuestionEncoder.forward:41
#: transformers.TFDPRContextEncoder.call:41
#: transformers.TFDPRQuestionEncoder.call:41
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:"
msgstr ""

#: of transformers.DPRContextEncoder.forward:44
#: transformers.DPRQuestionEncoder.forward:44
#: transformers.TFDPRContextEncoder.call:44
#: transformers.TFDPRQuestionEncoder.call:44
msgid "0 corresponds to a `sentence A` token,"
msgstr ""

#: of transformers.DPRContextEncoder.forward:45
#: transformers.DPRQuestionEncoder.forward:45
#: transformers.TFDPRContextEncoder.call:45
#: transformers.TFDPRQuestionEncoder.call:45
msgid "1 corresponds to a `sentence B` token."
msgstr ""

#: of transformers.DPRContextEncoder.forward:47
#: transformers.DPRQuestionEncoder.forward:47
#: transformers.TFDPRContextEncoder.call:47
#: transformers.TFDPRQuestionEncoder.call:47
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.DPRContextEncoder.forward:49
#: transformers.DPRQuestionEncoder.forward:49 transformers.DPRReader.forward:29
#: transformers.TFDPRContextEncoder.call:49
#: transformers.TFDPRQuestionEncoder.call:49 transformers.TFDPRReader.call:27
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.DPRContextEncoder.forward:53
#: transformers.DPRQuestionEncoder.forward:53 transformers.DPRReader.forward:33
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.DPRContextEncoder.forward:56
#: transformers.DPRQuestionEncoder.forward:56 transformers.DPRReader.forward:36
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.DPRContextEncoder.forward:59
#: transformers.DPRQuestionEncoder.forward:59 transformers.DPRReader.forward:39
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.DPRContextEncoder.forward:62
msgid ""
"A :class:`~transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput` "
"or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DPRConfig`) and "
"inputs.  - **pooler_output:** (:obj:``torch.FloatTensor`` of shape "
"``(batch_size, embeddings_size)``) -- The DPR encoder outputs the "
"`pooler_output` that corresponds to the context representation. Last "
"layer   hidden-state of the first token of the sequence (classification "
"token) further processed by a Linear layer.   This output is to be used "
"to embed contexts for nearest neighbors queries with questions "
"embeddings. - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"DPRContextEncoder, DPRContextEncoderTokenizer     >>> tokenizer = "
"DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-"
"single-nq-base')     >>> model = "
"DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-"
"base')     >>> input_ids = tokenizer(\"Hello, is my dog cute ?\", "
"return_tensors='pt')[\"input_ids\"]     >>> embeddings = "
"model(input_ids).pooler_output"
msgstr ""

#: of transformers.DPRContextEncoder.forward:62
msgid ""
"A :class:`~transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput` "
"or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DPRConfig`) and "
"inputs."
msgstr ""

#: of transformers.DPRContextEncoder.forward:66
msgid ""
"**pooler_output:** (:obj:``torch.FloatTensor`` of shape ``(batch_size, "
"embeddings_size)``) -- The DPR encoder outputs the `pooler_output` that "
"corresponds to the context representation. Last layer hidden-state of the"
" first token of the sequence (classification token) further processed by "
"a Linear layer. This output is to be used to embed contexts for nearest "
"neighbors queries with questions embeddings."
msgstr ""

#: of transformers.DPRContextEncoder.forward:69
#: transformers.DPRQuestionEncoder.forward:69 transformers.DPRReader.forward:50
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.DPRContextEncoder.forward:73
#: transformers.DPRQuestionEncoder.forward:73 transformers.DPRReader.forward:54
#: transformers.TFDPRReader.call:54
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.DPRContextEncoder.forward:80
#: transformers.DPRQuestionEncoder.forward:80 transformers.DPRReader.forward:61
#: transformers.TFDPRContextEncoder.call:86
#: transformers.TFDPRQuestionEncoder.call:86 transformers.TFDPRReader.call:61
msgid "Examples::"
msgstr ""

#: of transformers.DPRContextEncoder.forward:87
msgid ""
":class:`~transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput` or"
" :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/dpr.rst:104
msgid "DPRQuestionEncoder"
msgstr ""

#: of transformers.DPRQuestionEncoder:1 transformers.TFDPRQuestionEncoder:1
msgid ""
"The bare DPRQuestionEncoder transformer outputting pooler outputs as "
"question representations."
msgstr ""

#: of transformers.DPRQuestionEncoder.forward:1
msgid ""
"The :class:`~transformers.DPRQuestionEncoder` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.DPRQuestionEncoder.forward:62
msgid ""
"A :class:`~transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DPRConfig`) and "
"inputs.  - **pooler_output:** (:obj:``torch.FloatTensor`` of shape "
"``(batch_size, embeddings_size)``) -- The DPR encoder outputs the "
"`pooler_output` that corresponds to the question representation. Last "
"layer   hidden-state of the first token of the sequence (classification "
"token) further processed by a Linear layer.   This output is to be used "
"to embed questions for nearest neighbors queries with context embeddings."
" - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"DPRQuestionEncoder, DPRQuestionEncoderTokenizer     >>> tokenizer = "
"DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-"
"question_encoder-single-nq-base')     >>> model = "
"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-"
"nq-base')     >>> input_ids = tokenizer(\"Hello, is my dog cute ?\", "
"return_tensors='pt')[\"input_ids\"]     >>> embeddings = "
"model(input_ids).pooler_output"
msgstr ""

#: of transformers.DPRQuestionEncoder.forward:62
msgid ""
"A :class:`~transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.DPRConfig`) and "
"inputs."
msgstr ""

#: of transformers.DPRQuestionEncoder.forward:66
msgid ""
"**pooler_output:** (:obj:``torch.FloatTensor`` of shape ``(batch_size, "
"embeddings_size)``) -- The DPR encoder outputs the `pooler_output` that "
"corresponds to the question representation. Last layer hidden-state of "
"the first token of the sequence (classification token) further processed "
"by a Linear layer. This output is to be used to embed questions for "
"nearest neighbors queries with context embeddings."
msgstr ""

#: of transformers.DPRQuestionEncoder.forward:87
msgid ""
":class:`~transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput` "
"or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/dpr.rst:111
msgid "DPRReader"
msgstr ""

#: of transformers.DPRReader:1 transformers.TFDPRReader:1
msgid "The bare DPRReader transformer outputting span predictions."
msgstr ""

#: of transformers.DPRReader.forward:1
msgid ""
"The :class:`~transformers.DPRReader` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.DPRReader.forward:8
msgid ""
"(:obj:`Tuple[torch.LongTensor]` of shapes :obj:`(n_passages, "
"sequence_length)`): Indices of input sequence tokens in the vocabulary. "
"It has to be a sequence triplet with 1) the question and 2) the passages "
"titles and 3) the passages texts To match pretraining, DPR "
":obj:`input_ids` sequence should be formatted with [CLS] and [SEP] with "
"the format:      ``[CLS] <question token ids> [SEP] <titles ids> [SEP] "
"<texts ids>``  DPR is a model with absolute position embeddings so it's "
"usually advised to pad the inputs on the right rather than the left.  "
"Indices can be obtained using :class:`~transformers.DPRReaderTokenizer`. "
"See this class documentation for more details.  `What are input IDs? "
"<../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.DPRReader.forward:8
msgid ""
"(:obj:`Tuple[torch.LongTensor]` of shapes :obj:`(n_passages, "
"sequence_length)`): Indices of input sequence tokens in the vocabulary. "
"It has to be a sequence triplet with 1) the question and 2) the passages "
"titles and 3) the passages texts To match pretraining, DPR "
":obj:`input_ids` sequence should be formatted with [CLS] and [SEP] with "
"the format:"
msgstr ""

#: of transformers.DPRReader.forward:13 transformers.TFDPRReader.call:13
msgid "``[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>``"
msgstr ""

#: of transformers.DPRReader.forward:18 transformers.TFDPRReader.call:18
msgid ""
"Indices can be obtained using :class:`~transformers.DPRReaderTokenizer`. "
"See this class documentation for more details."
msgstr ""

#: of transformers.DPRReader.forward:42
msgid ""
"A :class:`~transformers.models.dpr.modeling_dpr.DPRReaderOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DPRConfig`) and inputs.  - "
"**start_logits:** (:obj:``torch.FloatTensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the start index of the span for each "
"passage. - **end_logits:** (:obj:``torch.FloatTensor`` of shape "
"``(n_passages, sequence_length)``) -- Logits of the end index of the span"
" for each passage. - **relevance_logits:** (:obj:`torch.FloatTensor`` of "
"shape ``(n_passages, )``) -- Outputs of the QA classifier of the "
"DPRReader that corresponds to the scores of each passage to answer the   "
"question, compared to all the other passages. - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"DPRReader, DPRReaderTokenizer     >>> tokenizer = "
"DPRReaderTokenizer.from_pretrained('facebook/dpr-reader-single-nq-base')"
"     >>> model = DPRReader.from_pretrained('facebook/dpr-reader-single-"
"nq-base')     >>> encoded_inputs = tokenizer(     ...         "
"questions=[\"What is love ?\"],     ...         titles=[\"Haddaway\"],"
"     ...         texts=[\"'What Is Love' is a song recorded by the artist"
" Haddaway\"],     ...         return_tensors='pt'     ...     )     >>> "
"outputs = model(**encoded_inputs)     >>> start_logits = "
"outputs.start_logits     >>> end_logits = outputs.end_logits     >>> "
"relevance_logits = outputs.relevance_logits"
msgstr ""

#: of transformers.DPRReader.forward:42
msgid ""
"A :class:`~transformers.models.dpr.modeling_dpr.DPRReaderOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DPRConfig`) and inputs."
msgstr ""

#: of transformers.DPRReader.forward:46
msgid ""
"**start_logits:** (:obj:``torch.FloatTensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the start index of the span for each "
"passage."
msgstr ""

#: of transformers.DPRReader.forward:47
msgid ""
"**end_logits:** (:obj:``torch.FloatTensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the end index of the span for each "
"passage."
msgstr ""

#: of transformers.DPRReader.forward:48
msgid ""
"**relevance_logits:** (:obj:`torch.FloatTensor`` of shape ``(n_passages, "
")``) -- Outputs of the QA classifier of the DPRReader that corresponds to"
" the scores of each passage to answer the question, compared to all the "
"other passages."
msgstr ""

#: of transformers.DPRReader.forward:76
msgid ""
":class:`~transformers.models.dpr.modeling_dpr.DPRReaderOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/dpr.rst:117
msgid "TFDPRContextEncoder"
msgstr ""

#: of transformers.TFDPRContextEncoder:3 transformers.TFDPRQuestionEncoder:3
#: transformers.TFDPRReader:3
msgid ""
"This model inherits from :class:`~transformers.TFPreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TFDPRContextEncoder:7 transformers.TFDPRQuestionEncoder:7
#: transformers.TFDPRReader:7
msgid ""
"This model is also a Tensorflow `tf.keras.Model "
"<https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. "
"Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFDPRContextEncoder:13 transformers.TFDPRQuestionEncoder:13
#: transformers.TFDPRReader:13
msgid "TF 2.0 models accepts two formats as inputs:"
msgstr ""

#: of transformers.TFDPRContextEncoder:15 transformers.TFDPRQuestionEncoder:15
#: transformers.TFDPRReader:15
msgid "having all inputs as keyword arguments (like PyTorch models), or"
msgstr ""

#: of transformers.TFDPRContextEncoder:16 transformers.TFDPRQuestionEncoder:16
#: transformers.TFDPRReader:16
msgid ""
"having all inputs as a list, tuple or dict in the first positional "
"arguments."
msgstr ""

#: of transformers.TFDPRContextEncoder:18 transformers.TFDPRQuestionEncoder:18
#: transformers.TFDPRReader:18
msgid ""
"This second option is useful when using :meth:`tf.keras.Model.fit` method"
" which currently requires having all the tensors in the first argument of"
" the model call function: :obj:`model(inputs)`."
msgstr ""

#: of transformers.TFDPRContextEncoder:21 transformers.TFDPRQuestionEncoder:21
#: transformers.TFDPRReader:21
msgid ""
"If you choose this second option, there are three possibilities you can "
"use to gather all the input Tensors in the first positional argument :"
msgstr ""

#: of transformers.TFDPRContextEncoder:24 transformers.TFDPRQuestionEncoder:24
#: transformers.TFDPRReader:24
msgid ""
"a single Tensor with :obj:`input_ids` only and nothing else: "
":obj:`model(inputs_ids)`"
msgstr ""

#: of transformers.TFDPRContextEncoder:25 transformers.TFDPRQuestionEncoder:25
#: transformers.TFDPRReader:25
msgid ""
"a list of varying length with one or several input Tensors IN THE ORDER "
"given in the docstring: :obj:`model([input_ids, attention_mask])` or "
":obj:`model([input_ids, attention_mask, token_type_ids])`"
msgstr ""

#: of transformers.TFDPRContextEncoder:27 transformers.TFDPRQuestionEncoder:27
#: transformers.TFDPRReader:27
msgid ""
"a dictionary with one or several input Tensors associated to the input "
"names given in the docstring: :obj:`model({\"input_ids\": input_ids, "
"\"token_type_ids\": token_type_ids})`"
msgstr ""

#: of transformers.TFDPRContextEncoder:30 transformers.TFDPRQuestionEncoder:30
#: transformers.TFDPRReader:30
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.TFPreTrainedModel.from_pretrained` method to load "
"the model weights."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:1
msgid ""
"The :class:`~transformers.TFDPRContextEncoder` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:53
#: transformers.TFDPRQuestionEncoder.call:53
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:57
#: transformers.TFDPRQuestionEncoder.call:57 transformers.TFDPRReader.call:31
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:61
#: transformers.TFDPRQuestionEncoder.call:61 transformers.TFDPRReader.call:35
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple. This argument can be used in eager mode, in "
"graph mode the value will always be set to True."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:64
#: transformers.TFDPRQuestionEncoder.call:64 transformers.TFDPRReader.call:38
msgid ""
"Whether or not to use the model in training mode (some modules like "
"dropout modules have different behaviors between training and "
"evaluation)."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:68
msgid ""
"A "
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DPRConfig`) and inputs.  - "
"**pooler_output:** (:obj:``tf.Tensor`` of shape ``(batch_size, "
"embeddings_size)``) -- The DPR encoder outputs the `pooler_output` that "
"corresponds to the context representation. Last layer   hidden-state of "
"the first token of the sequence (classification token) further processed "
"by a Linear layer.   This output is to be used to embed contexts for "
"nearest neighbors queries with questions embeddings. - **hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads.   "
"Examples::      >>> from transformers import TFDPRContextEncoder, "
"DPRContextEncoderTokenizer     >>> tokenizer = "
"DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-"
"single-nq-base')     >>> model = "
"TFDPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-"
"base', from_pt=True)     >>> input_ids = tokenizer(\"Hello, is my dog "
"cute ?\", return_tensors='tf')[\"input_ids\"]     >>> embeddings = "
"model(input_ids).pooler_output"
msgstr ""

#: of transformers.TFDPRContextEncoder.call:68
msgid ""
"A "
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DPRConfig`) and inputs."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:72
msgid ""
"**pooler_output:** (:obj:``tf.Tensor`` of shape ``(batch_size, "
"embeddings_size)``) -- The DPR encoder outputs the `pooler_output` that "
"corresponds to the context representation. Last layer hidden-state of the"
" first token of the sequence (classification token) further processed by "
"a Linear layer. This output is to be used to embed contexts for nearest "
"neighbors queries with questions embeddings."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:75
#: transformers.TFDPRQuestionEncoder.call:75 transformers.TFDPRReader.call:50
msgid ""
"**hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:79
#: transformers.TFDPRQuestionEncoder.call:79
msgid ""
"**attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFDPRContextEncoder.call:93
msgid ""
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/dpr.rst:123
msgid "TFDPRQuestionEncoder"
msgstr ""

#: of transformers.TFDPRQuestionEncoder.call:1
msgid ""
"The :class:`~transformers.TFDPRQuestionEncoder` forward method, overrides"
" the :func:`__call__` special method."
msgstr ""

#: of transformers.TFDPRQuestionEncoder.call:68
msgid ""
"A "
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DPRConfig`) and inputs.  - "
"**pooler_output:** (:obj:``tf.Tensor`` of shape ``(batch_size, "
"embeddings_size)``) -- The DPR encoder outputs the `pooler_output` that "
"corresponds to the question representation. Last layer   hidden-state of "
"the first token of the sequence (classification token) further processed "
"by a Linear layer.   This output is to be used to embed questions for "
"nearest neighbors queries with context embeddings. - **hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads.   "
"Examples::      >>> from transformers import TFDPRQuestionEncoder, "
"DPRQuestionEncoderTokenizer     >>> tokenizer = "
"DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-"
"question_encoder-single-nq-base')     >>> model = "
"TFDPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-"
"single-nq-base', from_pt=True)     >>> input_ids = tokenizer(\"Hello, is "
"my dog cute ?\", return_tensors='tf')[\"input_ids\"]     >>> embeddings ="
" model(input_ids).pooler_output"
msgstr ""

#: of transformers.TFDPRQuestionEncoder.call:68
msgid ""
"A "
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.DPRConfig`) and inputs."
msgstr ""

#: of transformers.TFDPRQuestionEncoder.call:72
msgid ""
"**pooler_output:** (:obj:``tf.Tensor`` of shape ``(batch_size, "
"embeddings_size)``) -- The DPR encoder outputs the `pooler_output` that "
"corresponds to the question representation. Last layer hidden-state of "
"the first token of the sequence (classification token) further processed "
"by a Linear layer. This output is to be used to embed questions for "
"nearest neighbors queries with context embeddings."
msgstr ""

#: of transformers.TFDPRQuestionEncoder.call:93
msgid ""
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/dpr.rst:130
msgid "TFDPRReader"
msgstr ""

#: of transformers.TFDPRReader.call:1
msgid ""
"The :class:`~transformers.TFDPRReader` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFDPRReader.call:8
msgid ""
"(:obj:`Numpy array` or :obj:`tf.Tensor` of shapes :obj:`(n_passages, "
"sequence_length)`): Indices of input sequence tokens in the vocabulary. "
"It has to be a sequence triplet with 1) the question and 2) the passages "
"titles and 3) the passages texts To match pretraining, DPR "
":obj:`input_ids` sequence should be formatted with [CLS] and [SEP] with "
"the format:      ``[CLS] <question token ids> [SEP] <titles ids> [SEP] "
"<texts ids>``  DPR is a model with absolute position embeddings so it's "
"usually advised to pad the inputs on the right rather than the left.  "
"Indices can be obtained using :class:`~transformers.DPRReaderTokenizer`. "
"See this class documentation for more details."
msgstr ""

#: of transformers.TFDPRReader.call:8
msgid ""
"(:obj:`Numpy array` or :obj:`tf.Tensor` of shapes :obj:`(n_passages, "
"sequence_length)`): Indices of input sequence tokens in the vocabulary. "
"It has to be a sequence triplet with 1) the question and 2) the passages "
"titles and 3) the passages texts To match pretraining, DPR "
":obj:`input_ids` sequence should be formatted with [CLS] and [SEP] with "
"the format:"
msgstr ""

#: of transformers.TFDPRReader.call:42
msgid ""
"A :class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput` or "
"a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.DPRConfig`) and inputs.  - "
"**start_logits:** (:obj:``tf.Tensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the start index of the span for each "
"passage. - **end_logits:** (:obj:``tf.Tensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the end index of the span for each "
"passage. - **relevance_logits:** (:obj:`tf.Tensor`` of shape "
"``(n_passages, )``) -- Outputs of the QA classifier of the DPRReader that"
" corresponds to the scores of each passage to answer the   question, "
"compared to all the other passages. - **hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"TFDPRReader, DPRReaderTokenizer     >>> tokenizer = "
"DPRReaderTokenizer.from_pretrained('facebook/dpr-reader-single-nq-base')"
"     >>> model = TFDPRReader.from_pretrained('facebook/dpr-reader-single-"
"nq-base', from_pt=True)     >>> encoded_inputs = tokenizer(     ..."
"         questions=[\"What is love ?\"],     ...         "
"titles=[\"Haddaway\"],     ...         texts=[\"'What Is Love' is a song "
"recorded by the artist Haddaway\"],     ...         return_tensors='tf'"
"     ...     )     >>> outputs = model(encoded_inputs)     >>> "
"start_logits = outputs.start_logits     >>> end_logits = "
"outputs.end_logits     >>> relevance_logits = outputs.relevance_logits"
msgstr ""

#: of transformers.TFDPRReader.call:42
msgid ""
"A :class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput` or "
"a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.DPRConfig`) and inputs."
msgstr ""

#: of transformers.TFDPRReader.call:46
msgid ""
"**start_logits:** (:obj:``tf.Tensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the start index of the span for each "
"passage."
msgstr ""

#: of transformers.TFDPRReader.call:47
msgid ""
"**end_logits:** (:obj:``tf.Tensor`` of shape ``(n_passages, "
"sequence_length)``) -- Logits of the end index of the span for each "
"passage."
msgstr ""

#: of transformers.TFDPRReader.call:48
msgid ""
"**relevance_logits:** (:obj:`tf.Tensor`` of shape ``(n_passages, )``) -- "
"Outputs of the QA classifier of the DPRReader that corresponds to the "
"scores of each passage to answer the question, compared to all the other "
"passages."
msgstr ""

#: of transformers.TFDPRReader.call:76
msgid ""
":class:`~transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput` or "
":obj:`tuple(tf.Tensor)`"
msgstr ""


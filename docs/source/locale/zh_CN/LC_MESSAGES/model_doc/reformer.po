# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/reformer.rst:14
msgid "Reformer"
msgstr ""

#: ../../source/model_doc/reformer.rst:16
msgid ""
"**DISCLAIMER:** This model is still a work in progress, if you see "
"something strange, file a `Github Issue "
"<https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template"
"=bug-report.md&title>`__."
msgstr ""

#: ../../source/model_doc/reformer.rst:20
msgid "Overview"
msgstr ""

#: ../../source/model_doc/reformer.rst:22
msgid ""
"The Reformer model was proposed in the paper `Reformer: The Efficient "
"Transformer <https://arxiv.org/abs/2001.04451.pdf>`__ by Nikita Kitaev, "
"≈Åukasz Kaiser, Anselm Levskaya."
msgstr ""

#: ../../source/model_doc/reformer.rst:25
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/reformer.rst:27
msgid ""
"*Large Transformer models routinely achieve state-of-the-art results on a"
" number of tasks but training these models can be prohibitively costly, "
"especially on long sequences. We introduce two techniques to improve the "
"efficiency of Transformers. For one, we replace dot-product attention by "
"one that uses locality-sensitive hashing, changing its complexity from "
"O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore,"
" we use reversible residual layers instead of the standard residuals, "
"which allows storing activations only once in the training process "
"instead of N times, where N is the number of layers. The resulting model,"
" the Reformer, performs on par with Transformer models while being much "
"more memory-efficient and much faster on long sequences.*"
msgstr ""

#: ../../source/model_doc/reformer.rst:35
msgid ""
"This model was contributed by `patrickvonplaten "
"<https://huggingface.co/patrickvonplaten>`__. The Authors' code can be "
"found `here "
"<https://github.com/google/trax/tree/master/trax/models/reformer>`__."
msgstr ""

#: ../../source/model_doc/reformer.rst:39
msgid "Axial Positional Encodings"
msgstr ""

#: ../../source/model_doc/reformer.rst:41
msgid ""
"Axial Positional Encodings were first implemented in Google's `trax "
"library "
"<https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29>`__"
" and developed by the authors of this model's paper. In models that are "
"treating very long input sequences, the conventional position id "
"encodings store an embedings vector of size :math:`d` being the "
":obj:`config.hidden_size` for every position :math:`i, \\ldots, n_s`, "
"with :math:`n_s` being :obj:`config.max_embedding_size`. This means that "
"having a sequence length of :math:`n_s = 2^{19} \\approx 0.5M` and a "
"``config.hidden_size`` of :math:`d = 2^{10} \\approx 1000` would result "
"in a position encoding matrix:"
msgstr ""

#: ../../source/model_doc/reformer.rst:49
msgid ""
"X_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d\\right] \\text{ and } "
"j \\in \\left[1,\\ldots, n_s\\right]\n"
"\n"
msgstr ""

#: ../../source/model_doc/reformer.rst:52
msgid ""
"which alone has over 500M parameters to store. Axial positional encodings"
" factorize :math:`X_{i,j}` into two matrices:"
msgstr ""

#: ../../source/model_doc/reformer.rst:54
msgid ""
"X^{1}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^1\\right] \\text{ "
"and } j \\in \\left[1,\\ldots, n_s^1\\right]\n"
"\n"
msgstr ""

#: ../../source/model_doc/reformer.rst:57
msgid "and"
msgstr ""

#: ../../source/model_doc/reformer.rst:59
msgid ""
"X^{2}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^2\\right] \\text{ "
"and } j \\in \\left[1,\\ldots, n_s^2\\right]\n"
"\n"
msgstr ""

#: ../../source/model_doc/reformer.rst:62
msgid "with:"
msgstr ""

#: ../../source/model_doc/reformer.rst:64
msgid ""
"d = d^1 + d^2 \\text{ and } n_s = n_s^1 \\times n_s^2 .\n"
"\n"
msgstr ""

#: ../../source/model_doc/reformer.rst:67
msgid "Therefore the following holds:"
msgstr ""

#: ../../source/model_doc/reformer.rst:69
msgid ""
"X_{i,j} = \\begin{cases}\n"
"            X^{1}_{i, k}, & \\text{if }\\ i < d^1 \\text{ with } k = j "
"\\mod n_s^1 \\\\\n"
"            X^{2}_{i - d^1, l}, & \\text{if } i \\ge d^1 \\text{ with } l"
" = \\lfloor\\frac{j}{n_s^1}\\rfloor\n"
"          \\end{cases}\n"
"\n"
msgstr ""

#: ../../source/model_doc/reformer.rst:75
msgid ""
"Intuitively, this means that a position embedding vector :math:`x_j \\in "
"\\mathbb{R}^{d}` is now the composition of two factorized embedding "
"vectors: :math:`x^1_{k, l} + x^2_{l, k}`, where as the "
":obj:`config.max_embedding_size` dimension :math:`j` is factorized into "
":math:`k \\text{ and } l`. This design ensures that each position "
"embedding vector :math:`x_j` is unique."
msgstr ""

#: ../../source/model_doc/reformer.rst:80
msgid ""
"Using the above example again, axial position encoding with :math:`d^1 = "
"2^5, d^2 = 2^5, n_s^1 = 2^9, n_s^2 = 2^{10}` can drastically reduced the "
"number of parameters to :math:`2^{14} + 2^{15} \\approx 49000` "
"parameters."
msgstr ""

#: ../../source/model_doc/reformer.rst:83
msgid ""
"In practice, the parameter :obj:`config.axial_pos_embds_dim` is set to a "
"tuple :math:`(d^1, d^2)` which sum has to be equal to "
":obj:`config.hidden_size` and :obj:`config.axial_pos_shape` is set to a "
"tuple :math:`(n_s^1, n_s^2)` which product has to be equal to "
":obj:`config.max_embedding_size`, which during training has to be equal "
"to the `sequence length` of the :obj:`input_ids`."
msgstr ""

#: ../../source/model_doc/reformer.rst:90
msgid "LSH Self Attention"
msgstr ""

#: ../../source/model_doc/reformer.rst:92
msgid ""
"In Locality sensitive hashing (LSH) self attention the key and query "
"projection weights are tied. Therefore, the key query embedding vectors "
"are also tied. LSH self attention uses the locality sensitive hashing "
"mechanism proposed in `Practical and Optimal LSH for Angular Distance "
"<https://arxiv.org/abs/1509.02897>`__ to assign each of the tied key "
"query embedding vectors to one of :obj:`config.num_buckets` possible "
"buckets. The premise is that the more \"similar\" key query embedding "
"vectors (in terms of *cosine similarity*) are to each other, the more "
"likely they are assigned to the same bucket."
msgstr ""

#: ../../source/model_doc/reformer.rst:99
msgid ""
"The accuracy of the LSH mechanism can be improved by increasing "
":obj:`config.num_hashes` or directly the argument :obj:`num_hashes` of "
"the forward function so that the output of the LSH self attention better "
"approximates the output of the \"normal\" full self attention. The "
"buckets are then sorted and chunked into query key embedding vector "
"chunks each of length :obj:`config.lsh_chunk_length`. For each chunk, the"
" query embedding vectors attend to its key vectors (which are tied to "
"themselves) and to the key embedding vectors of "
":obj:`config.lsh_num_chunks_before` previous neighboring chunks and "
":obj:`config.lsh_num_chunks_after` following neighboring chunks."
msgstr ""

#: ../../source/model_doc/reformer.rst:106
msgid ""
"For more information, see the `original Paper "
"<https://arxiv.org/abs/2001.04451>`__ or this great `blog post "
"<https://www.pragmatic.ml/reformer-deep-dive/>`__."
msgstr ""

#: ../../source/model_doc/reformer.rst:109
msgid ""
"Note that :obj:`config.num_buckets` can also be factorized into a list "
":math:`(n_{\\text{buckets}}^1, n_{\\text{buckets}}^2)`. This way instead "
"of assigning the query key embedding vectors to one of :math:`(1,\\ldots,"
" n_{\\text{buckets}})` they are assigned to one of :math:`(1-1,\\ldots, "
"n_{\\text{buckets}}^1-1, \\ldots, 1-n_{\\text{buckets}}^2, \\ldots, "
"n_{\\text{buckets}}^1-n_{\\text{buckets}}^2)`. This is crucial for very "
"long sequences to save memory."
msgstr ""

#: ../../source/model_doc/reformer.rst:115
msgid ""
"When training a model from scratch, it is recommended to leave "
":obj:`config.num_buckets=None`, so that depending on the sequence length "
"a good value for :obj:`num_buckets` is calculated on the fly. This value "
"will then automatically be saved in the config and should be reused for "
"inference."
msgstr ""

#: ../../source/model_doc/reformer.rst:119
msgid ""
"Using LSH self attention, the memory and time complexity of the query-key"
" matmul operation can be reduced from :math:`\\mathcal{O}(n_s \\times "
"n_s)` to :math:`\\mathcal{O}(n_s \\times \\log(n_s))`, which usually "
"represents the memory and time bottleneck in a transformer model, with "
":math:`n_s` being the sequence length."
msgstr ""

#: ../../source/model_doc/reformer.rst:125
msgid "Local Self Attention"
msgstr ""

#: ../../source/model_doc/reformer.rst:127
msgid ""
"Local self attention is essentially a \"normal\" self attention layer "
"with key, query and value projections, but is chunked so that in each "
"chunk of length :obj:`config.local_chunk_length` the query embedding "
"vectors only attends to the key embedding vectors in its chunk and to the"
" key embedding vectors of :obj:`config.local_num_chunks_before` previous "
"neighboring chunks and :obj:`config.local_num_chunks_after` following "
"neighboring chunks."
msgstr ""

#: ../../source/model_doc/reformer.rst:132
msgid ""
"Using Local self attention, the memory and time complexity of the query-"
"key matmul operation can be reduced from :math:`\\mathcal{O}(n_s \\times "
"n_s)` to :math:`\\mathcal{O}(n_s \\times \\log(n_s))`, which usually "
"represents the memory and time bottleneck in a transformer model, with "
":math:`n_s` being the sequence length."
msgstr ""

#: ../../source/model_doc/reformer.rst:138
msgid "Training"
msgstr ""

#: ../../source/model_doc/reformer.rst:140
msgid ""
"During training, we must ensure that the sequence length is set to a "
"value that can be divided by the least common multiple of "
":obj:`config.lsh_chunk_length` and :obj:`config.local_chunk_length` and "
"that the parameters of the Axial Positional Encodings are correctly set "
"as described above. Reformer is very memory efficient so that the model "
"can easily be trained on sequences as long as 64000 tokens."
msgstr ""

#: ../../source/model_doc/reformer.rst:145
msgid ""
"For training, the :class:`~transformers.ReformerModelWithLMHead` should "
"be used as follows:"
msgstr ""

#: ../../source/model_doc/reformer.rst:154
msgid "ReformerConfig"
msgstr ""

#: of transformers.ReformerConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.ReformerModel`. It is used to instantiate a "
"Reformer model according to the specified arguments, defining the model "
"architecture."
msgstr ""

#: of transformers.ReformerConfig:4
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.ReformerConfig transformers.ReformerForMaskedLM
#: transformers.ReformerForMaskedLM.forward
#: transformers.ReformerForQuestionAnswering
#: transformers.ReformerForQuestionAnswering.forward
#: transformers.ReformerForSequenceClassification
#: transformers.ReformerForSequenceClassification.forward
#: transformers.ReformerModel transformers.ReformerModel.forward
#: transformers.ReformerModelWithLMHead
#: transformers.ReformerModelWithLMHead.forward transformers.ReformerTokenizer
#: transformers.ReformerTokenizer.save_vocabulary
#: transformers.ReformerTokenizerFast
#: transformers.ReformerTokenizerFast.save_vocabulary
msgid "Parameters"
msgstr ""

#: of transformers.ReformerConfig:7
msgid "Dimensionality of the projected key, query and value vectors"
msgstr ""

#: of transformers.ReformerConfig:9
msgid ""
"List of attention layer types in ascending order. It can be chosen "
"between a LSHSelfAttention layer (:obj:`\"lsh\"`) and a "
"LocalSelfAttention layer (:obj:`\"local\"`).  For more information on "
"LSHSelfAttention layer, see `LSH Self Attention <reformer.html#lsh-self-"
"attention>`__. For more information on LocalSelfAttention layer, see "
"`Local Self Attention <reformer.html#local-self-attention>`__."
msgstr ""

#: of transformers.ReformerConfig:9
msgid ""
"List of attention layer types in ascending order. It can be chosen "
"between a LSHSelfAttention layer (:obj:`\"lsh\"`) and a "
"LocalSelfAttention layer (:obj:`\"local\"`)."
msgstr ""

#: of transformers.ReformerConfig:12
msgid ""
"For more information on LSHSelfAttention layer, see `LSH Self Attention "
"<reformer.html#lsh-self-attention>`__. For more information on "
"LocalSelfAttention layer, see `Local Self Attention <reformer.html#local-"
"self-attention>`__."
msgstr ""

#: of transformers.ReformerConfig:16
msgid ""
"Whether or not to use axial position embeddings. For more information on "
"how axial position embeddings work, see `Axial Position Encodings "
"<reformer.html#axial-positional-encodings>`__."
msgstr ""

#: of transformers.ReformerConfig:19
msgid ""
"The standard deviation of the normal_initializer for initializing the "
"weight matrices of the axial positional encodings."
msgstr ""

#: of transformers.ReformerConfig:22
msgid ""
"The position dims of the axial position encodings. During training, the "
"product of the position dims has to be equal to the sequence length.  For"
" more information on how axial position embeddings work, see `Axial "
"Position Encodings <reformer.html#axial-positional-encodings>`__."
msgstr ""

#: of transformers.ReformerConfig:22
msgid ""
"The position dims of the axial position encodings. During training, the "
"product of the position dims has to be equal to the sequence length."
msgstr ""

#: of transformers.ReformerConfig:25 transformers.ReformerConfig:31
msgid ""
"For more information on how axial position embeddings work, see `Axial "
"Position Encodings <reformer.html#axial-positional-encodings>`__."
msgstr ""

#: of transformers.ReformerConfig:28
msgid ""
"The embedding dims of the axial position encodings. The sum of the "
"embedding dims has to be equal to the hidden size.  For more information "
"on how axial position embeddings work, see `Axial Position Encodings "
"<reformer.html#axial-positional-encodings>`__."
msgstr ""

#: of transformers.ReformerConfig:28
msgid ""
"The embedding dims of the axial position encodings. The sum of the "
"embedding dims has to be equal to the hidden size."
msgstr ""

#: of transformers.ReformerConfig:34
msgid ""
"The chunk size of the final language model feed forward head layer. A "
"chunk size of 0 means that the feed forward layer is not chunked. A chunk"
" size of n means that the feed forward layer processes n < "
"sequence_length embeddings at a time.  For more information on feed "
"forward chunking, see `How does Feed Forward Chunking work? "
"<../glossary.html#feed-forward-chunking>`__."
msgstr ""

#: of transformers.ReformerConfig:34
msgid ""
"The chunk size of the final language model feed forward head layer. A "
"chunk size of 0 means that the feed forward layer is not chunked. A chunk"
" size of n means that the feed forward layer processes n < "
"sequence_length embeddings at a time."
msgstr ""

#: of transformers.ReformerConfig:38
msgid ""
"For more information on feed forward chunking, see `How does Feed Forward"
" Chunking work? <../glossary.html#feed-forward-chunking>`__."
msgstr ""

#: of transformers.ReformerConfig:41
msgid "The token id for the end-of-sentence token."
msgstr ""

#: of transformers.ReformerConfig:43
msgid "Dimensionality of the feed_forward layer in the residual attention block."
msgstr ""

#: of transformers.ReformerConfig:45
msgid ""
"Seed that can be used to make local sensitive hashing in "
":obj:`LSHSelfAttention` deterministic. This should only be set for "
"testing purposed. For evaluation and training purposes :obj:`hash_seed` "
"should be left as :obj:`None` to ensure fully random rotations in local "
"sensitive hashing scheme."
msgstr ""

#: of transformers.ReformerConfig:49
msgid ""
"The non-linear activation function (function or string) in the feed "
"forward layer in the residual attention block. If string, "
":obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"`"
" are supported."
msgstr ""

#: of transformers.ReformerConfig:52
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.ReformerConfig:54
msgid ""
"Dimensionality of the output hidden states of the residual attention "
"blocks."
msgstr ""

#: of transformers.ReformerConfig:56
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.ReformerConfig:58
msgid ""
"Whether or not to use a causal mask in addition to the "
":obj:`attention_mask` passed to :class:`~transformers.ReformerModel`. "
"When using the Reformer for causal language modeling, this argument "
"should be set to :obj:`True`."
msgstr ""

#: of transformers.ReformerConfig:62
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.ReformerConfig:64
msgid ""
"Length of chunk which attends to itself in :obj:`LocalSelfAttention`. "
"Chunking reduces memory complexity from sequence length x sequence length"
" (self attention) to chunk length x chunk length x sequence length / "
"chunk length (chunked self attention)."
msgstr ""

#: of transformers.ReformerConfig:68
msgid ""
"Number of previous neighbouring chunks to attend to in "
":obj:`LocalSelfAttention` layer to itself."
msgstr ""

#: of transformers.ReformerConfig:70
msgid ""
"Number of following neighbouring chunks to attend to in "
":obj:`LocalSelfAttention` layer in addition to itself."
msgstr ""

#: of transformers.ReformerConfig:73
msgid ""
"The dropout ratio for the attention probabilities in "
":obj:`LocalSelfAttention`."
msgstr ""

#: of transformers.ReformerConfig:75
msgid ""
"Length of chunk which attends to itself in :obj:`LSHSelfAttention`. "
"Chunking reduces memory complexity from sequence length x sequence length"
" (self attention) to chunk length x chunk length x sequence length / "
"chunk length (chunked self attention)."
msgstr ""

#: of transformers.ReformerConfig:79
msgid ""
"Number of previous neighbouring chunks to attend to in "
":obj:`LSHSelfAttention` layer to itself."
msgstr ""

#: of transformers.ReformerConfig:81
msgid ""
"Number of following neighbouring chunks to attend to in "
":obj:`LSHSelfAttention` layer to itself."
msgstr ""

#: of transformers.ReformerConfig:83
msgid ""
"The dropout ratio for the attention probabilities in "
":obj:`LSHSelfAttention`."
msgstr ""

#: of transformers.ReformerConfig:85
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.ReformerConfig:88
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.ReformerConfig:90
msgid ""
"Number of buckets, the key query vectors can be \"hashed into\" using the"
" locality sensitive hashing scheme. Each query key vector is hashed into "
"a hash in :obj:`1, ..., num_buckets`. The number of buckets can also be "
"factorized into a list for improved memory complexity. In this case, each"
" query key vector is hashed into a hash in :obj:`1-1, 1-2, ..., "
"num_buckets[0]-1, ..., num_buckets[0]-num_buckets[1]` if "
":obj:`num_buckets` is factorized into two factors. The number of buckets "
"(or the product the factors) should approximately equal sequence length /"
" lsh_chunk_length. If :obj:`num_buckets` not set, a good value is "
"calculated on the fly."
msgstr ""

#: of transformers.ReformerConfig:98
msgid ""
"Number of hashing rounds (e.g., number of random rotations) in Local "
"Sensitive Hashing scheme. The higher :obj:`num_hashes`, the more accurate"
" the :obj:`LSHSelfAttention` becomes, but also the more memory and time "
"intensive the hashing becomes."
msgstr ""

#: of transformers.ReformerConfig:102
msgid "The token id for the padding token."
msgstr ""

#: of transformers.ReformerConfig:104
msgid ""
"\\ Vocabulary size of the Reformer model. Defines the number of different"
" tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.ReformerModel`."
msgstr ""

#: of transformers.ReformerConfig:108
msgid "Whether to tie input and output embeddings."
msgstr ""

#: of transformers.ReformerConfig:110
msgid ""
"Whether or not the model should return the last key/values attentions "
"(not used by all models)."
msgstr ""

#: of transformers.ReformerConfig:112
msgid "The dropout ratio for the classification head."
msgstr ""

#: of transformers.ReformerConfig:115
msgid "Examples::"
msgstr ""

#: ../../source/model_doc/reformer.rst:161
msgid "ReformerTokenizer"
msgstr ""

#: of transformers.ReformerTokenizer:1
msgid ""
"Construct a Reformer tokenizer. Based on `SentencePiece "
"<https://github.com/google/sentencepiece>`__ ."
msgstr ""

#: of transformers.ReformerTokenizer:3
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods."
msgstr ""

#: of transformers.ReformerTokenizer:6 transformers.ReformerTokenizerFast:7
msgid ""
"`SentencePiece <https://github.com/google/sentencepiece>`__ file "
"(generally has a `.spm` extension) that contains the vocabulary necessary"
" to instantiate a tokenizer."
msgstr ""

#: of transformers.ReformerTokenizer:9 transformers.ReformerTokenizerFast:10
msgid ""
"The end of sequence token.  .. note::      When building a sequence using"
" special tokens, this is not the token that is used for the end of     "
"sequence. The token used is the :obj:`sep_token`."
msgstr ""

#: of transformers.ReformerTokenizer:9 transformers.ReformerTokenizerFast:10
msgid "The end of sequence token."
msgstr ""

#: of transformers.ReformerTokenizer:13 transformers.ReformerTokenizerFast:14
msgid ""
"When building a sequence using special tokens, this is not the token that"
" is used for the end of sequence. The token used is the :obj:`sep_token`."
msgstr ""

#: of transformers.ReformerTokenizer:16 transformers.ReformerTokenizerFast:17
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.ReformerTokenizer:19 transformers.ReformerTokenizerFast:20
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.ReformerTokenizer:21 transformers.ReformerTokenizerFast:22
msgid "Additional special tokens used by the tokenizer."
msgstr ""

#: of transformers.ReformerTokenizer:23
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:  - ``enable_sampling``: Enable subword "
"regularization. - ``nbest_size``: Sampling parameters for unigram. "
"Invalid for BPE-Dropout.    - ``nbest_size = {0,1}``: No sampling is "
"performed.   - ``nbest_size > 1``: samples from the nbest_size results."
"   - ``nbest_size < 0``: assuming that nbest_size is infinite and samples"
" from the all hypothesis (lattice)     using forward-filtering-and-"
"backward-sampling algorithm.  - ``alpha``: Smoothing parameter for "
"unigram sampling, and dropout probability of merge operations for   BPE-"
"dropout."
msgstr ""

#: of transformers.ReformerTokenizer:23
msgid ""
"Will be passed to the ``SentencePieceProcessor.__init__()`` method. The "
"`Python wrapper for SentencePiece "
"<https://github.com/google/sentencepiece/tree/master/python>`__ can be "
"used, among other things, to set:"
msgstr ""

#: of transformers.ReformerTokenizer:26
msgid "``enable_sampling``: Enable subword regularization."
msgstr ""

#: of transformers.ReformerTokenizer:27
msgid "``nbest_size``: Sampling parameters for unigram. Invalid for BPE-Dropout."
msgstr ""

#: of transformers.ReformerTokenizer:29
msgid "``nbest_size = {0,1}``: No sampling is performed."
msgstr ""

#: of transformers.ReformerTokenizer:30
msgid "``nbest_size > 1``: samples from the nbest_size results."
msgstr ""

#: of transformers.ReformerTokenizer:31
msgid ""
"``nbest_size < 0``: assuming that nbest_size is infinite and samples from"
" the all hypothesis (lattice) using forward-filtering-and-backward-"
"sampling algorithm."
msgstr ""

#: of transformers.ReformerTokenizer:34
msgid ""
"``alpha``: Smoothing parameter for unigram sampling, and dropout "
"probability of merge operations for BPE-dropout."
msgstr ""

#: of transformers.ReformerTokenizer.save_vocabulary:1
#: transformers.ReformerTokenizerFast.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.ReformerTokenizer.save_vocabulary:3
#: transformers.ReformerTokenizerFast.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.ReformerTokenizer.save_vocabulary:6
#: transformers.ReformerTokenizerFast.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.ReformerTokenizer.save_vocabulary:8
#: transformers.ReformerTokenizerFast.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward
#: transformers.ReformerForQuestionAnswering.forward
#: transformers.ReformerForSequenceClassification.forward
#: transformers.ReformerModel.forward
#: transformers.ReformerModelWithLMHead.forward
#: transformers.ReformerTokenizer.save_vocabulary
#: transformers.ReformerTokenizerFast.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.ReformerTokenizer.save_vocabulary:11
#: transformers.ReformerTokenizerFast.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward
#: transformers.ReformerForQuestionAnswering.forward
#: transformers.ReformerForSequenceClassification.forward
#: transformers.ReformerModel.forward
#: transformers.ReformerModelWithLMHead.forward
#: transformers.ReformerTokenizer.save_vocabulary
#: transformers.ReformerTokenizerFast.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.ReformerTokenizer.save_vocabulary:12
#: transformers.ReformerTokenizerFast.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/reformer.rst:168
msgid "ReformerTokenizerFast"
msgstr ""

#: of transformers.ReformerTokenizerFast:1
msgid ""
"Construct a \"fast\" Reformer tokenizer (backed by HuggingFace's "
"`tokenizers` library). Based on `Unigram "
"<https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models>`__."
msgstr ""

#: of transformers.ReformerTokenizerFast:4
msgid ""
"This tokenizer inherits from "
":class:`~transformers.PreTrainedTokenizerFast` which contains most of the"
" main methods. Users should refer to this superclass for more information"
" regarding those methods."
msgstr ""

#: ../../source/model_doc/reformer.rst:175
msgid "ReformerModel"
msgstr ""

#: of transformers.ReformerModel:1
msgid ""
"The bare Reformer Model transformer outputting raw hidden-stateswithout "
"any specific head on top. Reformer was proposed in `Reformer: The "
"Efficient Transformer <https://arxiv.org/abs/2001.04451>`__ by Nikita "
"Kitaev, ≈Åukasz Kaiser, Anselm Levskaya."
msgstr ""

#: of transformers.ReformerForMaskedLM:5
#: transformers.ReformerForQuestionAnswering:7
#: transformers.ReformerForSequenceClassification:7
#: transformers.ReformerModel:5 transformers.ReformerModelWithLMHead:5
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.ReformerForMaskedLM:9
#: transformers.ReformerForQuestionAnswering:11
#: transformers.ReformerForSequenceClassification:11
#: transformers.ReformerModel:9 transformers.ReformerModelWithLMHead:9
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.ReformerForMaskedLM:13
#: transformers.ReformerForQuestionAnswering:15
#: transformers.ReformerForSequenceClassification:15
#: transformers.ReformerModel:13 transformers.ReformerModelWithLMHead:13
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.ReformerModel.forward:1
msgid ""
"The :class:`~transformers.ReformerModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:4
#: transformers.ReformerForQuestionAnswering.forward:4
#: transformers.ReformerForSequenceClassification.forward:4
#: transformers.ReformerModel.forward:4
#: transformers.ReformerModelWithLMHead.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:8
#: transformers.ReformerForQuestionAnswering.forward:8
#: transformers.ReformerForSequenceClassification.forward:8
#: transformers.ReformerModel.forward:8
#: transformers.ReformerModelWithLMHead.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. During training the "
"input_ids sequence_length has to be a multiple of the relevant model's "
"chunk lengths (lsh's, local's or both). During evaluation, the indices "
"are automatically padded to be a multiple of the chunk length.  Indices "
"can be obtained using :class:`~transformers.ReformerTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:8
#: transformers.ReformerForQuestionAnswering.forward:8
#: transformers.ReformerForSequenceClassification.forward:8
#: transformers.ReformerModel.forward:8
#: transformers.ReformerModelWithLMHead.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. During training the "
"input_ids sequence_length has to be a multiple of the relevant model's "
"chunk lengths (lsh's, local's or both). During evaluation, the indices "
"are automatically padded to be a multiple of the chunk length."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:12
#: transformers.ReformerForQuestionAnswering.forward:12
#: transformers.ReformerForSequenceClassification.forward:12
#: transformers.ReformerModel.forward:12
#: transformers.ReformerModelWithLMHead.forward:12
msgid ""
"Indices can be obtained using :class:`~transformers.ReformerTokenizer`. "
"See :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:16
#: transformers.ReformerForQuestionAnswering.forward:16
#: transformers.ReformerForSequenceClassification.forward:16
#: transformers.ReformerModel.forward:16
#: transformers.ReformerModelWithLMHead.forward:16
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:18
#: transformers.ReformerForQuestionAnswering.forward:18
#: transformers.ReformerForSequenceClassification.forward:18
#: transformers.ReformerModel.forward:18
#: transformers.ReformerModelWithLMHead.forward:18
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:18
#: transformers.ReformerForQuestionAnswering.forward:18
#: transformers.ReformerForSequenceClassification.forward:18
#: transformers.ReformerModel.forward:18
#: transformers.ReformerModelWithLMHead.forward:18
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:20
#: transformers.ReformerForQuestionAnswering.forward:20
#: transformers.ReformerForSequenceClassification.forward:20
#: transformers.ReformerModel.forward:20
#: transformers.ReformerModelWithLMHead.forward:20
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:21
#: transformers.ReformerForQuestionAnswering.forward:21
#: transformers.ReformerForSequenceClassification.forward:21
#: transformers.ReformerModel.forward:21
#: transformers.ReformerModelWithLMHead.forward:21
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:23
#: transformers.ReformerForQuestionAnswering.forward:23
#: transformers.ReformerForSequenceClassification.forward:23
#: transformers.ReformerModel.forward:23
#: transformers.ReformerModelWithLMHead.forward:23
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:25
#: transformers.ReformerForQuestionAnswering.forward:25
#: transformers.ReformerForSequenceClassification.forward:25
#: transformers.ReformerModel.forward:25
#: transformers.ReformerModelWithLMHead.forward:25
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``.  `What are position IDs? <../glossary.html#position-ids>`__"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:25
#: transformers.ReformerForQuestionAnswering.forward:25
#: transformers.ReformerForSequenceClassification.forward:25
#: transformers.ReformerModel.forward:25
#: transformers.ReformerModelWithLMHead.forward:25
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:28
#: transformers.ReformerForQuestionAnswering.forward:28
#: transformers.ReformerForSequenceClassification.forward:28
#: transformers.ReformerModel.forward:28
#: transformers.ReformerModelWithLMHead.forward:28
msgid "`What are position IDs? <../glossary.html#position-ids>`__"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:30
#: transformers.ReformerForQuestionAnswering.forward:30
#: transformers.ReformerForSequenceClassification.forward:30
#: transformers.ReformerModel.forward:30
#: transformers.ReformerModelWithLMHead.forward:30
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:30
#: transformers.ReformerForQuestionAnswering.forward:30
#: transformers.ReformerForSequenceClassification.forward:30
#: transformers.ReformerModel.forward:30
#: transformers.ReformerModelWithLMHead.forward:30
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:32
#: transformers.ReformerForQuestionAnswering.forward:32
#: transformers.ReformerForSequenceClassification.forward:32
#: transformers.ReformerModel.forward:32
#: transformers.ReformerModelWithLMHead.forward:32
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:33
#: transformers.ReformerForQuestionAnswering.forward:33
#: transformers.ReformerForSequenceClassification.forward:33
#: transformers.ReformerModel.forward:33
#: transformers.ReformerModelWithLMHead.forward:33
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:35
#: transformers.ReformerForQuestionAnswering.forward:35
#: transformers.ReformerForSequenceClassification.forward:35
#: transformers.ReformerModel.forward:35
#: transformers.ReformerModelWithLMHead.forward:35
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:39
#: transformers.ReformerForQuestionAnswering.forward:39
#: transformers.ReformerForSequenceClassification.forward:39
#: transformers.ReformerModel.forward:39
#: transformers.ReformerModelWithLMHead.forward:39
msgid ""
"The number of hashing rounds that should be performed during bucketing. "
"Setting this argument overwrites the default defined in "
":obj:`config.num_hashes`.  For more information, see :obj:`num_hashes` in"
" :class:`~transformers.ReformerConfig`."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:39
#: transformers.ReformerForQuestionAnswering.forward:39
#: transformers.ReformerForSequenceClassification.forward:39
#: transformers.ReformerModel.forward:39
#: transformers.ReformerModelWithLMHead.forward:39
msgid ""
"The number of hashing rounds that should be performed during bucketing. "
"Setting this argument overwrites the default defined in "
":obj:`config.num_hashes`."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:42
#: transformers.ReformerForQuestionAnswering.forward:42
#: transformers.ReformerForSequenceClassification.forward:42
#: transformers.ReformerModel.forward:42
#: transformers.ReformerModelWithLMHead.forward:42
msgid ""
"For more information, see :obj:`num_hashes` in "
":class:`~transformers.ReformerConfig`."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:44
#: transformers.ReformerForQuestionAnswering.forward:44
#: transformers.ReformerForSequenceClassification.forward:44
#: transformers.ReformerModel.forward:44
#: transformers.ReformerModelWithLMHead.forward:44
msgid ""
"List of :obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length "
":obj:`config.n_layers`, with the first element being the previous "
"`buckets` of shape :obj:`(batch_size, num_heads, num_hashes, "
"sequence_length)`) and the second being the previous `hidden_states` of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`).  Contains "
"precomputed hidden-states and buckets (only relevant for LSH Self-"
"Attention). Can be used to speed up sequential decoding."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:44
#: transformers.ReformerForQuestionAnswering.forward:44
#: transformers.ReformerForSequenceClassification.forward:44
#: transformers.ReformerModel.forward:44
#: transformers.ReformerModelWithLMHead.forward:44
msgid ""
"List of :obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length "
":obj:`config.n_layers`, with the first element being the previous "
"`buckets` of shape :obj:`(batch_size, num_heads, num_hashes, "
"sequence_length)`) and the second being the previous `hidden_states` of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`)."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:49
#: transformers.ReformerForQuestionAnswering.forward:49
#: transformers.ReformerForSequenceClassification.forward:49
#: transformers.ReformerModel.forward:49
#: transformers.ReformerModelWithLMHead.forward:49
msgid ""
"Contains precomputed hidden-states and buckets (only relevant for LSH "
"Self-Attention). Can be used to speed up sequential decoding."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:52
#: transformers.ReformerForQuestionAnswering.forward:52
#: transformers.ReformerForSequenceClassification.forward:52
#: transformers.ReformerModel.forward:52
#: transformers.ReformerModelWithLMHead.forward:52
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:55
#: transformers.ReformerForQuestionAnswering.forward:55
#: transformers.ReformerForSequenceClassification.forward:55
#: transformers.ReformerModel.forward:55
#: transformers.ReformerModelWithLMHead.forward:55
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:58
#: transformers.ReformerForQuestionAnswering.forward:58
#: transformers.ReformerForSequenceClassification.forward:58
#: transformers.ReformerModel.forward:58
#: transformers.ReformerModelWithLMHead.forward:58
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:61
#: transformers.ReformerForQuestionAnswering.forward:61
#: transformers.ReformerForSequenceClassification.forward:61
#: transformers.ReformerModel.forward:61
#: transformers.ReformerModelWithLMHead.forward:61
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.ReformerModel.forward:64
msgid ""
"A "
":class:`~transformers.models.reformer.modeling_reformer.ReformerModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.ReformerConfig`) "
"and inputs.  - **last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, num_predict, hidden_size)`) -- Sequence of hidden-"
"states at the last layer of the model.    ``num_predict`` corresponds to "
"``target_mapping.shape[1]``. If ``target_mapping`` is ``None``, then   "
"``num_predict`` corresponds to ``sequence_length``. - "
"**past_buckets_states** (:obj:`List[Tuple(torch.LongTensor, "
"torch.FloatTensor)]`, `optional`, returned when ``use_cache=True`` is "
"passed or when ``config.use_cache=True``) -- List of "
":obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length "
":obj:`config.n_layers`, with the first   element being the previous "
"`buckets` of shape :obj:`(batch_size, num_heads, num_hashes, "
"sequence_length)`)   and the second being the previous `hidden_states` of"
" shape :obj:`(batch_size, sequence_length,   hidden_size)`).    Contains "
"precomputed buckets and hidden-states that can be used (see "
"``past_buckets_states`` input) to   speed up sequential decoding. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden-states of the model at the "
"output of each layer plus the initial embedding outputs. - **attentions**"
" (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.ReformerModel.forward:64
msgid ""
"A "
":class:`~transformers.models.reformer.modeling_reformer.ReformerModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.ReformerConfig`) "
"and inputs."
msgstr ""

#: of transformers.ReformerModel.forward:68
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, num_predict, hidden_size)`) -- Sequence of hidden-"
"states at the last layer of the model."
msgstr ""

#: of transformers.ReformerModel.forward:70
msgid ""
"``num_predict`` corresponds to ``target_mapping.shape[1]``. If "
"``target_mapping`` is ``None``, then ``num_predict`` corresponds to "
"``sequence_length``."
msgstr ""

#: of transformers.ReformerModel.forward:72
msgid ""
"**past_buckets_states** (:obj:`List[Tuple(torch.LongTensor, "
"torch.FloatTensor)]`, `optional`, returned when ``use_cache=True`` is "
"passed or when ``config.use_cache=True``) -- List of "
":obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length "
":obj:`config.n_layers`, with the first element being the previous "
"`buckets` of shape :obj:`(batch_size, num_heads, num_hashes, "
"sequence_length)`) and the second being the previous `hidden_states` of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`)."
msgstr ""

#: of transformers.ReformerModel.forward:77
msgid ""
"Contains precomputed buckets and hidden-states that can be used (see "
"``past_buckets_states`` input) to speed up sequential decoding."
msgstr ""

#: of transformers.ReformerModel.forward:79
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:77
#: transformers.ReformerForQuestionAnswering.forward:82
#: transformers.ReformerForSequenceClassification.forward:77
#: transformers.ReformerModel.forward:82
#: transformers.ReformerModelWithLMHead.forward:77
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:78
#: transformers.ReformerForQuestionAnswering.forward:83
#: transformers.ReformerForSequenceClassification.forward:78
#: transformers.ReformerModel.forward:83
#: transformers.ReformerModelWithLMHead.forward:78
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:81
#: transformers.ReformerForQuestionAnswering.forward:86
#: transformers.ReformerForSequenceClassification.forward:81
#: transformers.ReformerModel.forward:86
#: transformers.ReformerModelWithLMHead.forward:81
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.ReformerModel.forward:88
msgid ""
":class:`~transformers.models.reformer.modeling_reformer.ReformerModelOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:85
#: transformers.ReformerForQuestionAnswering.forward:90
#: transformers.ReformerForSequenceClassification.forward:85
#: transformers.ReformerModel.forward:90
#: transformers.ReformerModelWithLMHead.forward:85
msgid "Example::"
msgstr ""

#: ../../source/model_doc/reformer.rst:182
msgid "ReformerModelWithLMHead"
msgstr ""

#: of transformers.ReformerForMaskedLM:1 transformers.ReformerModelWithLMHead:1
msgid ""
"Reformer Model with a `language modeling` head on top. Reformer was "
"proposed in `Reformer: The Efficient Transformer "
"<https://arxiv.org/abs/2001.04451>`__ by Nikita Kitaev, ≈Åukasz Kaiser, "
"Anselm Levskaya."
msgstr ""

#: of transformers.ReformerModelWithLMHead.forward:1
msgid ""
"The :class:`~transformers.ReformerModelWithLMHead` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.ReformerModelWithLMHead.forward:63
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[-100, 0, ..., config.vocab_size - 1]`. All labels "
"set to ``-100`` are ignored (masked), the loss is only computed for "
"labels in ``[0, ..., config.vocab_size]``"
msgstr ""

#: of transformers.ReformerModelWithLMHead.forward:68
msgid ""
"A :class:`~transformers.modeling_outputs.CausalLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.ReformerConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction). - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.ReformerModelWithLMHead.forward:68
msgid ""
"A :class:`~transformers.modeling_outputs.CausalLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.ReformerConfig`) and inputs."
msgstr ""

#: of transformers.ReformerModelWithLMHead.forward:72
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction)."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:73
#: transformers.ReformerModelWithLMHead.forward:73
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:74
#: transformers.ReformerForQuestionAnswering.forward:79
#: transformers.ReformerForSequenceClassification.forward:74
#: transformers.ReformerModelWithLMHead.forward:74
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.ReformerModelWithLMHead.forward:83
msgid ""
":class:`~transformers.modeling_outputs.CausalLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/reformer.rst:189
msgid "ReformerForMaskedLM"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:1
msgid ""
"The :class:`~transformers.ReformerForMaskedLM` forward method, overrides "
"the :func:`__call__` special method."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:63
msgid ""
"Labels for computing the masked language modeling loss. Indices should be"
" in ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) "
"Tokens with indices set to ``-100`` are ignored (masked), the loss is "
"only computed for the tokens with labels"
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:68
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.ReformerConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss. - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:68
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.ReformerConfig`) and inputs."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:72
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss."
msgstr ""

#: of transformers.ReformerForMaskedLM.forward:83
msgid ""
":class:`~transformers.modeling_outputs.MaskedLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/reformer.rst:196
msgid "ReformerForSequenceClassification"
msgstr ""

#: of transformers.ReformerForSequenceClassification:1
msgid ""
"Reformer Model transformer with a sequence classification/regression head"
" on top (a linear layer on top of the pooled output) e.g. for GLUE tasks."
msgstr ""

#: of transformers.ReformerForQuestionAnswering:4
#: transformers.ReformerForSequenceClassification:4
msgid ""
"Reformer was proposed in `Reformer: The Efficient Transformer "
"<https://arxiv.org/abs/2001.04451>`__ by Nikita Kitaev, ≈Åukasz Kaiser, "
"Anselm Levskaya."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:1
msgid ""
"The :class:`~transformers.ReformerForSequenceClassification` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:63
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy)."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:68
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.ReformerConfig`) and inputs."
"  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss. - **logits** (:obj:`torch.FloatTensor` of"
" shape :obj:`(batch_size, config.num_labels)`) -- Classification (or "
"regression if config.num_labels==1) scores (before SoftMax). - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:68
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.ReformerConfig`) and inputs."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:72
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:73
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.ReformerForSequenceClassification.forward:83
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/reformer.rst:203
msgid "ReformerForQuestionAnswering"
msgstr ""

#: of transformers.ReformerForQuestionAnswering:1
msgid ""
"Reformer Model with a span classification head on top for extractive "
"question-answering tasks like SQuAD / TriviaQA ( a linear layer on top of"
" hidden-states output to compute `span start logits` and `span end "
"logits`."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:1
msgid ""
"The :class:`~transformers.ReformerForQuestionAnswering` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:63
msgid ""
"Labels for position (index) of the start of the labelled span for "
"computing the token classification loss. Positions are clamped to the "
"length of the sequence (:obj:`sequence_length`). Position outside of the "
"sequence are not taken into account for computing the loss."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:67
msgid ""
"Labels for position (index) of the end of the labelled span for computing"
" the token classification loss. Positions are clamped to the length of "
"the sequence (:obj:`sequence_length`). Position outside of the sequence "
"are not taken into account for computing the loss."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:72
msgid ""
"A :class:`~transformers.modeling_outputs.QuestionAnsweringModelOutput` or"
" a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed "
"or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.ReformerConfig`) "
"and inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Total span "
"extraction loss is the sum of a Cross-Entropy for the start and end "
"positions. - **start_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Span-start scores (before "
"SoftMax). - **end_logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Span-end scores (before "
"SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:72
msgid ""
"A :class:`~transformers.modeling_outputs.QuestionAnsweringModelOutput` or"
" a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed "
"or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.ReformerConfig`) "
"and inputs."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:76
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Total span extraction loss is"
" the sum of a Cross-Entropy for the start and end positions."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:77
msgid ""
"**start_logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Span-start scores (before SoftMax)."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:78
msgid ""
"**end_logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Span-end scores (before SoftMax)."
msgstr ""

#: of transformers.ReformerForQuestionAnswering.forward:88
msgid ""
":class:`~transformers.modeling_outputs.QuestionAnsweringModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""


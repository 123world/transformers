# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/wav2vec2.rst:14
msgid "Wav2Vec2"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:19
msgid ""
"The Wav2Vec2 model was proposed in `wav2vec 2.0: A Framework for Self-"
"Supervised Learning of Speech Representations "
"<https://arxiv.org/abs/2006.11477>`__ by Alexei Baevski, Henry Zhou, "
"Abdelrahman Mohamed, Michael Auli."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:22
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:24
msgid ""
"*We show for the first time that learning powerful representations from "
"speech audio alone followed by fine-tuning on transcribed speech can "
"outperform the best semi-supervised methods while being conceptually "
"simpler. wav2vec 2.0 masks the speech input in the latent space and "
"solves a contrastive task defined over a quantization of the latent "
"representations which are jointly learned. Experiments using all labeled "
"data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. "
"When lowering the amount of labeled data to one hour, wav2vec 2.0 "
"outperforms the previous state of the art on the 100 hour subset while "
"using 100 times less labeled data. Using just ten minutes of labeled data"
" and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 "
"WER. This demonstrates the feasibility of speech recognition with limited"
" amounts of labeled data.*"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:33
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:35
msgid ""
"Wav2Vec2 is a speech model that accepts a float array corresponding to "
"the raw waveform of the speech signal."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:36
msgid ""
"Wav2Vec2 model was trained using connectionist temporal classification "
"(CTC) so the model output has to be decoded using "
":class:`~transformers.Wav2Vec2CTCTokenizer`."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:39
msgid ""
"This model was contributed by `patrickvonplaten "
"<https://huggingface.co/patrickvonplaten>`__."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:43
msgid "Wav2Vec2Config"
msgstr ""

#: of transformers.Wav2Vec2Config:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.Wav2Vec2Model`. It is used to instantiate an "
"Wav2Vec2 model according to the specified arguments, defining the model "
"architecture. Instantiating a configuration with the defaults will yield "
"a similar configuration to that of the Wav2Vec2 `facebook/wav2vec2-base-"
"960h <https://huggingface.co/facebook/wav2vec2-base-960h>`__ "
"architecture."
msgstr ""

#: of transformers.Wav2Vec2Config:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC transformers.FlaxWav2Vec2ForCTC.__call__
#: transformers.FlaxWav2Vec2ForPreTraining
#: transformers.FlaxWav2Vec2ForPreTraining.__call__
#: transformers.FlaxWav2Vec2Model transformers.FlaxWav2Vec2Model.__call__
#: transformers.TFWav2Vec2ForCTC transformers.TFWav2Vec2ForCTC.call
#: transformers.TFWav2Vec2Model transformers.TFWav2Vec2Model.call
#: transformers.Wav2Vec2CTCTokenizer transformers.Wav2Vec2CTCTokenizer.__call__
#: transformers.Wav2Vec2CTCTokenizer.save_vocabulary
#: transformers.Wav2Vec2Config transformers.Wav2Vec2FeatureExtractor
#: transformers.Wav2Vec2FeatureExtractor.__call__ transformers.Wav2Vec2ForCTC
#: transformers.Wav2Vec2ForCTC.forward transformers.Wav2Vec2ForPreTraining
#: transformers.Wav2Vec2ForPreTraining.forward transformers.Wav2Vec2Model
#: transformers.Wav2Vec2Model.forward transformers.Wav2Vec2Processor
#: transformers.Wav2Vec2Processor.from_pretrained
#: transformers.Wav2Vec2Processor.save_pretrained
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput
msgid "Parameters"
msgstr ""

#: of transformers.Wav2Vec2Config:10
msgid ""
"Vocabulary size of the Wav2Vec2 model. Defines the number of different "
"tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.Wav2Vec2Model` or "
":class:`~transformers.TFWav2Vec2Model`. Vocabulary size of the model. "
"Defines the different tokens that can be represented by the `inputs_ids` "
"passed to the forward method of :class:`~transformers.Wav2Vec2Model`."
msgstr ""

#: of transformers.Wav2Vec2Config:15
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.Wav2Vec2Config:17
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.Wav2Vec2Config:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.Wav2Vec2Config:21
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.Wav2Vec2Config:23
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.Wav2Vec2Config:26
msgid ""
"The dropout probabilitiy for all fully connected layers in the "
"embeddings, encoder, and pooler."
msgstr ""

#: of transformers.Wav2Vec2Config:28
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.Wav2Vec2Config:30
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.Wav2Vec2Config:32
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.Wav2Vec2Config:34
msgid ""
"The norm to be applied to 1D convolutional layers in feature extractor. "
"One of :obj:`\"group\"` for group normalization of only the first 1D "
"convolutional layer or :obj:`\"layer\"` for layer normalization of all 1D"
" convolutional layers."
msgstr ""

#: of transformers.Wav2Vec2Config:38
msgid ""
"The dropout probabilitiy for all 1D convolutional layers in feature "
"extractor."
msgstr ""

#: of transformers.Wav2Vec2Config:40
msgid ""
"The non-linear activation function (function or string) in the 1D "
"convolutional layers of the feature extractor. If string, "
":obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` and :obj:`\"gelu_new\"`"
" are supported."
msgstr ""

#: of transformers.Wav2Vec2Config:43
msgid ""
"`float`, `optional`, defaults to 0.0): The dropout probabilitiy for "
"quantized feature extractor states."
msgstr ""

#: of transformers.Wav2Vec2Config:45
msgid ""
"A tuple of integers defining the number of input and output channels of "
"each 1D convolutional layer in the feature extractor. The length of "
"`conv_dim` defines the number of 1D convolutional layers."
msgstr ""

#: of transformers.Wav2Vec2Config:48
msgid ""
"A tuple of integers defining the stride of each 1D convolutional layer in"
" the feature extractor. The length of `conv_stride` defines the number of"
" convolutional layers and has to match the the length of `conv_dim`."
msgstr ""

#: of transformers.Wav2Vec2Config:51
msgid ""
"A tuple of integers defining the kernel size of each 1D convolutional "
"layer in the feature extractor. The length of `conv_kernel` defines the "
"number of convolutional layers and has to match the the length of "
"`conv_dim`."
msgstr ""

#: of transformers.Wav2Vec2Config:55
msgid "Whether the 1D convolutional layers have a bias."
msgstr ""

#: of transformers.Wav2Vec2Config:57
msgid ""
"Number of convolutional positional embeddings. Defines the kernel size of"
" 1D convolutional positional embeddings layer."
msgstr ""

#: of transformers.Wav2Vec2Config:60
msgid "Number of groups of 1D convolutional positional embeddings layer."
msgstr ""

#: of transformers.Wav2Vec2Config:62
msgid ""
"Whether do apply `stable` layer norm architecture of the Transformer "
"encoder. ``do_stable_layer_norm is True`` corresponds to applying layer "
"norm before the attention layer, whereas ``do_stable_layer_norm is "
"False`` corresponds to applying layer norm after the attention layer."
msgstr ""

#: of transformers.Wav2Vec2Config:66
msgid ""
"Whether to apply *SpecAugment* data augmentation to the outputs of the "
"feature extractor. For reference see `SpecAugment: A Simple Data "
"Augmentation Method for Automatic Speech Recognition "
"<https://arxiv.org/abs/1904.08779>`__."
msgstr ""

#: of transformers.Wav2Vec2Config:70
msgid ""
"Propability of each feature vector along the time axis to be chosen as "
"the start of the vector span to be masked. Approximately ``mask_time_prob"
" * sequence_length // mask_time_length`` feature vectors will be masked "
"along the time axis. This is only relevant if ``apply_spec_augment is "
"True``."
msgstr ""

#: of transformers.Wav2Vec2Config:74
msgid "Length of vector span along the time axis."
msgstr ""

#: of transformers.Wav2Vec2Config:76
msgid ""
"Propability of each feature vector along the feature axis to be chosen as"
" the start of the vector span to be masked. Approximately "
"``mask_time_prob * hidden_size // mask_time_length`` feature vectors will"
" be masked along the time axis. This is only relevant if "
"``apply_spec_augment is True``."
msgstr ""

#: of transformers.Wav2Vec2Config:80
msgid "Length of vector span along the feature axis."
msgstr ""

#: of transformers.Wav2Vec2Config:82
msgid "Number of entries in each quantization codebook (group)."
msgstr ""

#: of transformers.Wav2Vec2Config:84
msgid "Number of codevector groups for product codevector quantization."
msgstr ""

#: of transformers.Wav2Vec2Config:86
msgid "The temperature `kappa` in the contrastive loss."
msgstr ""

#: of transformers.Wav2Vec2Config:88
msgid ""
"The dropout probabilitiy for the output of the feature extractor that's "
"used by the quantizer."
msgstr ""

#: of transformers.Wav2Vec2Config:90
msgid "Number of negative samples for the contrastive loss."
msgstr ""

#: of transformers.Wav2Vec2Config:92
msgid "Dimensionality of the quantized feature vectors."
msgstr ""

#: of transformers.Wav2Vec2Config:94
msgid ""
"Dimensionality of the final projection of both the quantized and the "
"transformer features."
msgstr ""

#: of transformers.Wav2Vec2Config:96
msgid "The weight of the codebook diversity loss component."
msgstr ""

#: of transformers.Wav2Vec2Config:98
msgid ""
"Specifies the reduction to apply to the output of ``torch.nn.CTCLoss``. "
"Only relevant when training an instance of "
":class:`~transformers.Wav2Vec2ForCTC`."
msgstr ""

#: of transformers.Wav2Vec2Config:101
msgid ""
"Whether to zero infinite losses and the associated gradients of "
"``torch.nn.CTCLoss``. Infinite losses mainly occur when the inputs are "
"too short to be aligned to the targets. Only relevant when training an "
"instance of :class:`~transformers.Wav2Vec2ForCTC`."
msgstr ""

#: of transformers.Wav2Vec2Config:105
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:56
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:61
#: transformers.FlaxWav2Vec2Model.__call__:58
#: transformers.TFWav2Vec2ForCTC.call:81 transformers.TFWav2Vec2Model.call:76
#: transformers.Wav2Vec2Config:108 transformers.Wav2Vec2ForCTC.forward:62
#: transformers.Wav2Vec2ForPreTraining.forward:64
#: transformers.Wav2Vec2Model.forward:56
msgid "Example::"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:50
msgid "Wav2Vec2CTCTokenizer"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:1
msgid "Constructs a Wav2Vec2CTC tokenizer."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:3
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains some of the main methods. Users should refer to the "
"superclass for more information regarding such methods."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:6
msgid "File containing the vocabulary."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:8
msgid "The beginning of sentence token."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:10
msgid "The end of sentence token."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:12
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:15
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:17
msgid "The token used for defining the end of a word."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:19
msgid ""
"Whether or not to accept lowercase input and lowercase the output when "
"decoding."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer:21
msgid ""
"Additional keyword arguments passed along to "
":class:`~transformers.PreTrainedTokenizer`"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:1
msgid ""
"Main method to tokenize and prepare for the model one or several "
"sequence(s) or one or several pair(s) of sequences."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:4
#: transformers.Wav2Vec2CTCTokenizer.__call__:8
msgid ""
"The sequence or batch of sequences to be encoded. Each sequence can be a "
"string or a list of strings (pretokenized string). If the sequences are "
"provided as list of strings (pretokenized), you must set "
":obj:`is_split_into_words=True` (to lift the ambiguity with a batch of "
"sequences)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:12
msgid ""
"Whether or not to encode the sequences with the special tokens relative "
"to their model."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:14
msgid ""
"Activates and controls padding. Accepts the following values:  * "
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a   single sequence if provided). * "
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the   maximum acceptable input length for the "
"model if that argument is not provided. * :obj:`False` or "
":obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with "
"sequences of   different lengths)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:14
msgid "Activates and controls padding. Accepts the following values:"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:16
#: transformers.Wav2Vec2FeatureExtractor.__call__:9
msgid ""
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single sequence if provided)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:18
#: transformers.Wav2Vec2FeatureExtractor.__call__:11
msgid ""
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the maximum acceptable input length for the model"
" if that argument is not provided."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:20
#: transformers.Wav2Vec2FeatureExtractor.__call__:13
msgid ""
":obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can "
"output a batch with sequences of different lengths)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:23
msgid ""
"Activates and controls truncation. Accepts the following values:  * "
":obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length "
"specified with the argument   :obj:`max_length` or to the maximum "
"acceptable input length for the model if that argument is not   provided."
" This will truncate token by token, removing a token from the longest "
"sequence in the pair   if a pair of sequences (or a batch of pairs) is "
"provided. * :obj:`'only_first'`: Truncate to a maximum length specified "
"with the argument :obj:`max_length` or to   the maximum acceptable input "
"length for the model if that argument is not provided. This will only   "
"truncate the first sequence of a pair if a pair of sequences (or a batch "
"of pairs) is provided. * :obj:`'only_second'`: Truncate to a maximum "
"length specified with the argument :obj:`max_length` or   to the maximum "
"acceptable input length for the model if that argument is not provided. "
"This will only   truncate the second sequence of a pair if a pair of "
"sequences (or a batch of pairs) is provided. * :obj:`False` or "
":obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch"
" with   sequence lengths greater than the model maximum admissible input "
"size)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:23
msgid "Activates and controls truncation. Accepts the following values:"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:25
msgid ""
":obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length "
"specified with the argument :obj:`max_length` or to the maximum "
"acceptable input length for the model if that argument is not provided. "
"This will truncate token by token, removing a token from the longest "
"sequence in the pair if a pair of sequences (or a batch of pairs) is "
"provided."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:29
msgid ""
":obj:`'only_first'`: Truncate to a maximum length specified with the "
"argument :obj:`max_length` or to the maximum acceptable input length for "
"the model if that argument is not provided. This will only truncate the "
"first sequence of a pair if a pair of sequences (or a batch of pairs) is "
"provided."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:32
msgid ""
":obj:`'only_second'`: Truncate to a maximum length specified with the "
"argument :obj:`max_length` or to the maximum acceptable input length for "
"the model if that argument is not provided. This will only truncate the "
"second sequence of a pair if a pair of sequences (or a batch of pairs) is"
" provided."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:35
msgid ""
":obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., "
"can output batch with sequence lengths greater than the model maximum "
"admissible input size)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:38
msgid ""
"Controls the maximum length to use by one of the truncation/padding "
"parameters.  If left unset or set to :obj:`None`, this will use the "
"predefined model maximum length if a maximum length is required by one of"
" the truncation/padding parameters. If the model has no specific maximum "
"input length (like XLNet) truncation/padding to a maximum length will be "
"deactivated."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:38
msgid ""
"Controls the maximum length to use by one of the truncation/padding "
"parameters."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:40
msgid ""
"If left unset or set to :obj:`None`, this will use the predefined model "
"maximum length if a maximum length is required by one of the "
"truncation/padding parameters. If the model has no specific maximum input"
" length (like XLNet) truncation/padding to a maximum length will be "
"deactivated."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:44
msgid ""
"If set to a number along with :obj:`max_length`, the overflowing tokens "
"returned when :obj:`return_overflowing_tokens=True` will contain some "
"tokens from the end of the truncated sequence returned to provide some "
"overlap between truncated and overflowing sequences. The value of this "
"argument defines the number of overlapping tokens."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:49
msgid ""
"Whether or not the input is already pre-tokenized (e.g., split into "
"words). If set to :obj:`True`, the tokenizer assumes the input is already"
" split into words (for instance, by splitting it on whitespace) which it "
"will tokenize. This is useful for NER or token classification."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:53
msgid ""
"If set will pad the sequence to a multiple of the provided value. This is"
" especially useful to enable the use of Tensor Cores on NVIDIA hardware "
"with compute capability >= 7.5 (Volta)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:56
#: transformers.Wav2Vec2FeatureExtractor.__call__:41
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:  * :obj:`'tf'`: Return TensorFlow "
":obj:`tf.constant` objects. * :obj:`'pt'`: Return PyTorch "
":obj:`torch.Tensor` objects. * :obj:`'np'`: Return Numpy "
":obj:`np.ndarray` objects."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:56
#: transformers.Wav2Vec2FeatureExtractor.__call__:41
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:58
#: transformers.Wav2Vec2FeatureExtractor.__call__:43
msgid ":obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:59
#: transformers.Wav2Vec2FeatureExtractor.__call__:44
msgid ":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:60
#: transformers.Wav2Vec2FeatureExtractor.__call__:45
msgid ":obj:`'np'`: Return Numpy :obj:`np.ndarray` objects."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:62
msgid ""
"Whether to return token type IDs. If left to the default, will return the"
" token type IDs according to the specific tokenizer's default, defined by"
" the :obj:`return_outputs` attribute.  `What are token type IDs? "
"<../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:62
msgid ""
"Whether to return token type IDs. If left to the default, will return the"
" token type IDs according to the specific tokenizer's default, defined by"
" the :obj:`return_outputs` attribute."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:29
#: transformers.TFWav2Vec2Model.call:29
#: transformers.Wav2Vec2CTCTokenizer.__call__:65
#: transformers.Wav2Vec2CTCTokenizer.__call__:97
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:67
msgid ""
"Whether to return the attention mask. If left to the default, will return"
" the attention mask according to the specific tokenizer's default, "
"defined by the :obj:`return_outputs` attribute.  `What are attention "
"masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:67
msgid ""
"Whether to return the attention mask. If left to the default, will return"
" the attention mask according to the specific tokenizer's default, "
"defined by the :obj:`return_outputs` attribute."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:21
#: transformers.TFWav2Vec2Model.call:21
#: transformers.Wav2Vec2CTCTokenizer.__call__:70
#: transformers.Wav2Vec2CTCTokenizer.__call__:102
#: transformers.Wav2Vec2FeatureExtractor.__call__:28
#: transformers.Wav2Vec2ForCTC.forward:20
#: transformers.Wav2Vec2ForPreTraining.forward:20
#: transformers.Wav2Vec2Model.forward:20
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:72
msgid "Whether or not to return overflowing token sequences."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:74
msgid "Whether or not to return special tokens mask information."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:76
msgid ""
"Whether or not to return :obj:`(char_start, char_end)` for each token.  "
"This is only available on fast tokenizers inheriting from "
":class:`~transformers.PreTrainedTokenizerFast`, if using Python's "
"tokenizer, this method will raise :obj:`NotImplementedError`."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:76
msgid "Whether or not to return :obj:`(char_start, char_end)` for each token."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:78
msgid ""
"This is only available on fast tokenizers inheriting from "
":class:`~transformers.PreTrainedTokenizerFast`, if using Python's "
"tokenizer, this method will raise :obj:`NotImplementedError`."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:82
msgid "Whether or not to return the lengths of the encoded inputs."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:84
msgid "Whether or not to print more information and warnings."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:86
msgid "passed to the :obj:`self.tokenize()` method"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__
#: transformers.FlaxWav2Vec2ForPreTraining.__call__
#: transformers.FlaxWav2Vec2Model.__call__ transformers.TFWav2Vec2ForCTC.call
#: transformers.TFWav2Vec2Model.call transformers.Wav2Vec2CTCTokenizer.__call__
#: transformers.Wav2Vec2CTCTokenizer.save_vocabulary
#: transformers.Wav2Vec2ForCTC.forward
#: transformers.Wav2Vec2ForPreTraining.forward
#: transformers.Wav2Vec2Model.forward
msgid "Returns"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:88
msgid ""
"A :class:`~transformers.BatchEncoding` with the following fields:  - "
"**input_ids** -- List of token ids to be fed to a model.    `What are "
"input IDs? <../glossary.html#input-ids>`__  - **token_type_ids** -- List "
"of token type ids to be fed to a model (when "
":obj:`return_token_type_ids=True`   or if `\"token_type_ids\"` is in "
":obj:`self.model_input_names`).    `What are token type IDs? "
"<../glossary.html#token-type-ids>`__  - **attention_mask** -- List of "
"indices specifying which tokens should be attended to by the model (when"
"   :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in "
":obj:`self.model_input_names`).    `What are attention masks? "
"<../glossary.html#attention-mask>`__  - **overflowing_tokens** -- List of"
" overflowing tokens sequences (when a :obj:`max_length` is specified and"
"   :obj:`return_overflowing_tokens=True`). - **num_truncated_tokens** -- "
"Number of tokens truncated (when a :obj:`max_length` is specified and   "
":obj:`return_overflowing_tokens=True`). - **special_tokens_mask** -- List"
" of 0s and 1s, with 1 specifying added special tokens and 0 specifying   "
"regular sequence tokens (when :obj:`add_special_tokens=True` and "
":obj:`return_special_tokens_mask=True`). - **length** -- The length of "
"the inputs (when :obj:`return_length=True`)"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:88
msgid "A :class:`~transformers.BatchEncoding` with the following fields:"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:90
msgid "**input_ids** -- List of token ids to be fed to a model."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:14
#: transformers.TFWav2Vec2Model.call:14
#: transformers.Wav2Vec2CTCTokenizer.__call__:92
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:94
msgid ""
"**token_type_ids** -- List of token type ids to be fed to a model (when "
":obj:`return_token_type_ids=True` or if `\"token_type_ids\"` is in "
":obj:`self.model_input_names`)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:99
msgid ""
"**attention_mask** -- List of indices specifying which tokens should be "
"attended to by the model (when :obj:`return_attention_mask=True` or if "
"`\"attention_mask\"` is in :obj:`self.model_input_names`)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:104
msgid ""
"**overflowing_tokens** -- List of overflowing tokens sequences (when a "
":obj:`max_length` is specified and "
":obj:`return_overflowing_tokens=True`)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:106
msgid ""
"**num_truncated_tokens** -- Number of tokens truncated (when a "
":obj:`max_length` is specified and "
":obj:`return_overflowing_tokens=True`)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:108
msgid ""
"**special_tokens_mask** -- List of 0s and 1s, with 1 specifying added "
"special tokens and 0 specifying regular sequence tokens (when "
":obj:`add_special_tokens=True` and "
":obj:`return_special_tokens_mask=True`)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:110
msgid "**length** -- The length of the inputs (when :obj:`return_length=True`)"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__
#: transformers.FlaxWav2Vec2ForPreTraining.__call__
#: transformers.FlaxWav2Vec2Model.__call__ transformers.TFWav2Vec2ForCTC.call
#: transformers.TFWav2Vec2Model.call transformers.Wav2Vec2CTCTokenizer.__call__
#: transformers.Wav2Vec2CTCTokenizer.save_vocabulary
#: transformers.Wav2Vec2ForCTC.forward
#: transformers.Wav2Vec2ForPreTraining.forward
#: transformers.Wav2Vec2Model.forward
msgid "Return type"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.__call__:111
msgid ":class:`~transformers.BatchEncoding`"
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.Wav2Vec2CTCTokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:57
msgid "Wav2Vec2FeatureExtractor"
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:1
msgid "Constructs a Wav2Vec2 feature extractor."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:3
msgid ""
"This feature extractor inherits from "
":class:`~transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor`"
" which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:7
msgid "The feature dimension of the extracted features."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:9
msgid ""
"The sampling rate at which the audio files should be digitalized "
"expressed in Hertz per second (Hz)."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:11
msgid "The value that is used to fill the padding values."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:13
msgid ""
"Whether or not to zero-mean unit-variance normalize the input. "
"Normalizing can help to significantly improve the performance for some "
"models, *e.g.*, `wav2vec2-lv60 "
"<https://huggingface.co/models?search=lv60>`__."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:17
msgid ""
"Whether or not :meth:`~transformers.Wav2Vec2FeatureExtractor.__call__` "
"should return :obj:`attention_mask`.  .. note::      Wav2Vec2 models that"
" have set ``config.feat_extract_norm == \"group\"``, such as "
"`wav2vec2-base     <https://huggingface.co/facebook/wav2vec2-base-"
"960h>`__, have **not** been trained using     :obj:`attention_mask`. For "
"such models, :obj:`input_values` should simply be padded with 0 and no"
"     :obj:`attention_mask` should be passed.      For Wav2Vec2 models "
"that have set ``config.feat_extract_norm == \"layer\"``, such as "
"`wav2vec2-lv60     <https://huggingface.co/facebook/wav2vec2-large-960h-"
"lv60-self>`__, :obj:`attention_mask` should be     passed for batched "
"inference."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:17
msgid ""
"Whether or not :meth:`~transformers.Wav2Vec2FeatureExtractor.__call__` "
"should return :obj:`attention_mask`."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:21
#: transformers.Wav2Vec2FeatureExtractor.__call__:32
msgid ""
"Wav2Vec2 models that have set ``config.feat_extract_norm == \"group\"``, "
"such as `wav2vec2-base <https://huggingface.co/facebook/wav2vec2-base-"
"960h>`__, have **not** been trained using :obj:`attention_mask`. For such"
" models, :obj:`input_values` should simply be padded with 0 and no "
":obj:`attention_mask` should be passed."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor:26
#: transformers.Wav2Vec2FeatureExtractor.__call__:37
msgid ""
"For Wav2Vec2 models that have set ``config.feat_extract_norm == "
"\"layer\"``, such as `wav2vec2-lv60 "
"<https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self>`__, "
":obj:`attention_mask` should be passed for batched inference."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:1
msgid ""
"Main method to featurize and prepare for the model one or several "
"sequence(s). sequences."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:3
msgid ""
"The sequence or batch of sequences to be padded. Each sequence can be a "
"numpy array, a list of float values, a list of numpy arrays or a list of "
"list of float values."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:6
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:  * :obj:`True` or "
":obj:`'longest'`: Pad to the longest sequence in the batch (or no padding"
" if only a   single sequence if provided). * :obj:`'max_length'`: Pad to "
"a maximum length specified with the argument :obj:`max_length` or to the"
"   maximum acceptable input length for the model if that argument is not "
"provided. * :obj:`False` or :obj:`'do_not_pad'` (default): No padding "
"(i.e., can output a batch with sequences of   different lengths)."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:6
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:"
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:16
msgid ""
"Maximum length of the returned list and optionally padding length (see "
"above)."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:18
msgid ""
"Activates truncation to cut input sequences longer than `max_length` to "
"`max_length`."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:20
msgid ""
"If set will pad the sequence to a multiple of the provided value.  This "
"is especially useful to enable the use of Tensor Cores on NVIDIA hardware"
" with compute capability >= 7.5 (Volta), or on TPUs which benefit from "
"having sequence lengths be a multiple of 128."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:20
msgid "If set will pad the sequence to a multiple of the provided value."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:22
msgid ""
"This is especially useful to enable the use of Tensor Cores on NVIDIA "
"hardware with compute capability >= 7.5 (Volta), or on TPUs which benefit"
" from having sequence lengths be a multiple of 128."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:25
msgid ""
"Whether to return the attention mask. If left to the default, will return"
" the attention mask according to the specific feature_extractor's "
"default.  `What are attention masks? <../glossary.html#attention-mask>`__"
"  .. note::      Wav2Vec2 models that have set ``config.feat_extract_norm"
" == \"group\"``, such as `wav2vec2-base     "
"<https://huggingface.co/facebook/wav2vec2-base-960h>`__, have **not** "
"been trained using     :obj:`attention_mask`. For such models, "
":obj:`input_values` should simply be padded with 0 and no     "
":obj:`attention_mask` should be passed.      For Wav2Vec2 models that "
"have set ``config.feat_extract_norm == \"layer\"``, such as "
"`wav2vec2-lv60     <https://huggingface.co/facebook/wav2vec2-large-960h-"
"lv60-self>`__, :obj:`attention_mask` should be     passed for batched "
"inference."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:25
msgid ""
"Whether to return the attention mask. If left to the default, will return"
" the attention mask according to the specific feature_extractor's "
"default."
msgstr ""

#: of transformers.Wav2Vec2FeatureExtractor.__call__:47
msgid ""
"The sampling rate at which the ``raw_speech`` input was sampled. It is "
"strongly recommended to pass ``sampling_rate`` at the forward call to "
"prevent silent errors."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:64
msgid "Wav2Vec2Processor"
msgstr ""

#: of transformers.Wav2Vec2Processor:1
msgid ""
"Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor "
"and a Wav2Vec2 CTC tokenizer into a single processor."
msgstr ""

#: of transformers.Wav2Vec2Processor:4
msgid ""
":class:`~transformers.Wav2Vec2Processor` offers all the functionalities "
"of :class:`~transformers.Wav2Vec2FeatureExtractor` and "
":class:`~transformers.Wav2Vec2CTCTokenizer`. See the docstring of "
":meth:`~transformers.Wav2Vec2Processor.__call__` and "
":meth:`~transformers.Wav2Vec2Processor.decode` for more information."
msgstr ""

#: of transformers.Wav2Vec2Processor:9
msgid ""
"An instance of :class:`~transformers.Wav2Vec2FeatureExtractor`. The "
"feature extractor is a required input."
msgstr ""

#: of transformers.Wav2Vec2Processor:11
msgid ""
"An instance of :class:`~transformers.Wav2Vec2CTCTokenizer`. The tokenizer"
" is a required input."
msgstr ""

#: of transformers.Wav2Vec2Processor.__call__:1
msgid ""
"When used in normal mode, this method forwards all its arguments to "
"Wav2Vec2FeatureExtractor's "
":meth:`~transformers.Wav2Vec2FeatureExtractor.__call__` and returns its "
"output. If used in the context "
":meth:`~transformers.Wav2Vec2Processor.as_target_processor` this method "
"forwards all its arguments to Wav2Vec2CTCTokenizer's "
":meth:`~transformers.Wav2Vec2CTCTokenizer.__call__`. Please refer to the "
"docstring of the above two methods for more information."
msgstr ""

#: of transformers.Wav2Vec2Processor.as_target_processor:1
msgid ""
"Temporarily sets the tokenizer for processing the input. Useful for "
"encoding the labels when fine-tuning Wav2Vec2."
msgstr ""

#: of transformers.Wav2Vec2Processor.batch_decode:1
msgid ""
"This method forwards all its arguments to Wav2Vec2CTCTokenizer's "
":meth:`~transformers.PreTrainedTokenizer.batch_decode`. Please refer to "
"the docstring of this method for more information."
msgstr ""

#: of transformers.Wav2Vec2Processor.decode:1
msgid ""
"This method forwards all its arguments to Wav2Vec2CTCTokenizer's "
":meth:`~transformers.PreTrainedTokenizer.decode`. Please refer to the "
"docstring of this method for more information."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:1
msgid ""
"Instantiate a :class:`~transformers.Wav2Vec2Processor` from a pretrained "
"Wav2Vec2 processor."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:5
msgid ""
"This class method is simply calling Wav2Vec2FeatureExtractor's "
":meth:`~transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained`"
" and Wav2Vec2CTCTokenizer's "
":meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained`."
" Please refer to the docstrings of the methods above for more "
"information."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:10
msgid ""
"This can be either:  - a string, the `model id` of a pretrained "
"feature_extractor hosted inside a model repo on   huggingface.co. Valid "
"model ids can be located at the root-level, like ``bert-base-uncased``, "
"or   namespaced under a user or organization name, like ``dbmdz/bert-"
"base-german-cased``. - a path to a `directory` containing a feature "
"extractor file saved using the   "
":meth:`~transformers.SequenceFeatureExtractor.save_pretrained` method, "
"e.g.,   ``./my_model_directory/``. - a path or url to a saved feature "
"extractor JSON `file`, e.g.,   "
"``./my_model_directory/preprocessor_config.json``."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:10
msgid "This can be either:"
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:12
msgid ""
"a string, the `model id` of a pretrained feature_extractor hosted inside "
"a model repo on huggingface.co. Valid model ids can be located at the "
"root-level, like ``bert-base-uncased``, or namespaced under a user or "
"organization name, like ``dbmdz/bert-base-german-cased``."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:15
msgid ""
"a path to a `directory` containing a feature extractor file saved using "
"the :meth:`~transformers.SequenceFeatureExtractor.save_pretrained` "
"method, e.g., ``./my_model_directory/``."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:18
msgid ""
"a path or url to a saved feature extractor JSON `file`, e.g., "
"``./my_model_directory/preprocessor_config.json``."
msgstr ""

#: of transformers.Wav2Vec2Processor.from_pretrained:21
msgid ""
"Additional keyword arguments passed along to both "
":class:`~transformers.SequenceFeatureExtractor` and "
":class:`~transformers.PreTrainedTokenizer`"
msgstr ""

#: of transformers.Wav2Vec2Processor.pad:1
msgid ""
"When used in normal mode, this method forwards all its arguments to "
"Wav2Vec2FeatureExtractor's "
":meth:`~transformers.Wav2Vec2FeatureExtractor.pad` and returns its "
"output. If used in the context "
":meth:`~transformers.Wav2Vec2Processor.as_target_processor` this method "
"forwards all its arguments to Wav2Vec2CTCTokenizer's "
":meth:`~transformers.Wav2Vec2CTCTokenizer.pad`. Please refer to the "
"docstring of the above two methods for more information."
msgstr ""

#: of transformers.Wav2Vec2Processor.save_pretrained:1
msgid ""
"Save a Wav2Vec2 feature_extractor object and Wav2Vec2 tokenizer object to"
" the directory ``save_directory``, so that it can be re-loaded using the "
":func:`~transformers.Wav2Vec2Processor.from_pretrained` class method."
msgstr ""

#: of transformers.Wav2Vec2Processor.save_pretrained:6
msgid ""
"This class method is simply calling "
":meth:`~transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained`"
" and "
":meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.save_pretrained`."
" Please refer to the docstrings of the methods above for more "
"information."
msgstr ""

#: of transformers.Wav2Vec2Processor.save_pretrained:11
msgid ""
"Directory where the feature extractor JSON file and the tokenizer files "
"will be saved (directory will be created if it does not exist)."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:71
msgid "Wav2Vec2 specific outputs"
msgstr ""

#: of transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:1
msgid ""
"Output type of :class:`~transformers.Wav2Vec2BaseModelOutput`, with "
"potential hidden states and attentions."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:3
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:3
msgid "Sequence of hidden-states at the output of the last layer of the model."
msgstr ""

#: of transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:5
msgid ""
"Sequence of extracted feature vectors of the last convolutional layer of "
"the model."
msgstr ""

#: of transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:7
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:12
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the model at the output"
" of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:7
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:12
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:48
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:53
#: transformers.FlaxWav2Vec2Model.__call__:50
#: transformers.TFWav2Vec2ForCTC.call:73 transformers.TFWav2Vec2Model.call:68
#: transformers.Wav2Vec2ForCTC.forward:54
#: transformers.Wav2Vec2ForPreTraining.forward:56
#: transformers.Wav2Vec2Model.forward:48
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:11
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:16
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:10
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:15
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:12
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:17
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:12
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:17
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:52
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:57
#: transformers.FlaxWav2Vec2Model.__call__:54
#: transformers.TFWav2Vec2ForCTC.call:77 transformers.TFWav2Vec2Model.call:72
#: transformers.Wav2Vec2ForCTC.forward:58
#: transformers.Wav2Vec2ForPreTraining.forward:60
#: transformers.Wav2Vec2Model.forward:52
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:16
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:21
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput:15
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:20
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:1
msgid ""
"Output type of :class:`~transformers.Wav2Vec2ForPreTrainingOutput`, with "
"potential hidden states and attentions."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:4
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:3
msgid ""
"Total loss as the sum of the contrastive loss (L_m) and the diversity "
"loss (L_d) as stated in the `official paper "
"<https://arxiv.org/pdf/2006.11477.pdf>`__ . (classification) loss."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:7
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:6
msgid ""
"Hidden-states of the model projected to `config.proj_codevector_dim` that"
" can be used to predict the masked projected quantized states."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:10
#: transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput:9
msgid ""
"Quantized extracted feature vectors projected to "
"`config.proj_codevector_dim` representing the positive target vectors for"
" contrastive loss."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:1
msgid ""
"Output type of :class:`~transformers.FlaxWav2Vec2BaseModelOutput`, with "
"potential hidden states and attentions."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:5
msgid ""
"Sequence of extracted feature vectors of the last convolutional layer of "
"the model with ``last_conv_dim`` being the dimension of the last "
"convolutional layer."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:8
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:13
msgid ""
"Tuple of :obj:`jnp.ndarray` (one for the output of the embeddings + one "
"for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the model at the output"
" of each layer plus the initial embedding outputs."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:8
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:13
msgid ""
"Tuple of :obj:`jnp.ndarray` (one for the output of the embeddings + one "
"for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:13
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:18
msgid ""
"Tuple of :obj:`jnp.ndarray` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput:13
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:18
msgid ""
"Tuple of :obj:`jnp.ndarray` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput.replace:1
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput.replace:1
msgid "\"Returns a new object replacing the specified fields with new values."
msgstr ""

#: of
#: transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput:1
msgid ""
"Output type of :class:`~transformers.FlaxWav2Vec2ForPreTrainingOutput`, "
"with potential hidden states and attentions."
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:87
msgid "Wav2Vec2Model"
msgstr ""

#: of transformers.FlaxWav2Vec2Model:1 transformers.Wav2Vec2Model:1
msgid ""
"The bare Wav2Vec2 Model transformer outputting raw hidden-states without "
"any specific head on top. Wav2Vec2 was proposed in `wav2vec 2.0: A "
"Framework for Self-Supervised Learning of Speech Representations "
"<https://arxiv.org/abs/2006.11477>`__ by Alexei Baevski, Henry Zhou, "
"Abdelrahman Mohamed, Michael Auli."
msgstr ""

#: of transformers.Wav2Vec2ForCTC:5 transformers.Wav2Vec2ForPreTraining:5
#: transformers.Wav2Vec2Model:5
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving etc.)."
msgstr ""

#: of transformers.Wav2Vec2ForCTC:8 transformers.Wav2Vec2ForPreTraining:8
#: transformers.Wav2Vec2Model:8
msgid ""
"This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:30 transformers.TFWav2Vec2Model:30
#: transformers.Wav2Vec2ForCTC:12 transformers.Wav2Vec2ForPreTraining:12
#: transformers.Wav2Vec2Model:12
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.Wav2Vec2Model.forward:1
msgid ""
"The :class:`~transformers.Wav2Vec2Model` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:4
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:4
#: transformers.FlaxWav2Vec2Model.__call__:4
#: transformers.TFWav2Vec2ForCTC.call:4 transformers.TFWav2Vec2Model.call:4
#: transformers.Wav2Vec2ForCTC.forward:4
#: transformers.Wav2Vec2ForPreTraining.forward:4
#: transformers.Wav2Vec2Model.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:8
#: transformers.Wav2Vec2ForPreTraining.forward:8
#: transformers.Wav2Vec2Model.forward:8
msgid ""
"Float values of input raw speech waveform. Values can be obtained by "
"loading a `.flac` or `.wav` audio file into an array of type "
"`List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library "
"(`pip install soundfile`). To prepare the array into `input_values`, the "
":class:`~transformers.Wav2Vec2Processor` should be used for padding and "
"conversion into a tensor of type `torch.FloatTensor`. See "
":meth:`transformers.Wav2Vec2Processor.__call__` for details."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:14
#: transformers.Wav2Vec2ForPreTraining.forward:14
#: transformers.Wav2Vec2Model.forward:14
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:  - 1 for tokens that are "
"**not masked**, - 0 for tokens that are **masked**.  `What are attention "
"masks? <../glossary.html#attention-mask>`__  .. warning::     "
":obj:`attention_mask` should only be passed if the corresponding "
"processor has     ``config.return_attention_mask == True``. For all "
"models whose processor has     ``config.return_attention_mask == False``,"
" such as `wav2vec2-base     <https://huggingface.co/facebook/wav2vec2"
"-base-960h>`__, :obj:`attention_mask` should **not** be passed     to "
"avoid degraded performance when doing batched inference. For such models "
":obj:`input_values` should     simply be padded with 0 and passed without"
" :obj:`attention_mask`. Be aware that these models also yield     "
"slightly different results depending on whether :obj:`input_values` is "
"padded or not."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:14
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:14
#: transformers.FlaxWav2Vec2Model.__call__:14
#: transformers.Wav2Vec2ForCTC.forward:14
#: transformers.Wav2Vec2ForPreTraining.forward:14
#: transformers.Wav2Vec2Model.forward:14
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:17
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:17
#: transformers.FlaxWav2Vec2Model.__call__:17
#: transformers.TFWav2Vec2ForCTC.call:18 transformers.TFWav2Vec2Model.call:18
#: transformers.Wav2Vec2ForCTC.forward:17
#: transformers.Wav2Vec2ForPreTraining.forward:17
#: transformers.Wav2Vec2Model.forward:17
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:18
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:18
#: transformers.FlaxWav2Vec2Model.__call__:18
#: transformers.TFWav2Vec2ForCTC.call:19 transformers.TFWav2Vec2Model.call:19
#: transformers.Wav2Vec2ForCTC.forward:18
#: transformers.Wav2Vec2ForPreTraining.forward:18
#: transformers.Wav2Vec2Model.forward:18
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:23
#: transformers.Wav2Vec2ForPreTraining.forward:23
#: transformers.Wav2Vec2Model.forward:23
msgid ""
":obj:`attention_mask` should only be passed if the corresponding "
"processor has ``config.return_attention_mask == True``. For all models "
"whose processor has ``config.return_attention_mask == False``, such as "
"`wav2vec2-base <https://huggingface.co/facebook/wav2vec2-base-960h>`__, "
":obj:`attention_mask` should **not** be passed to avoid degraded "
"performance when doing batched inference. For such models "
":obj:`input_values` should simply be padded with 0 and passed without "
":obj:`attention_mask`. Be aware that these models also yield slightly "
"different results depending on whether :obj:`input_values` is padded or "
"not."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:31
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:31
#: transformers.FlaxWav2Vec2Model.__call__:31
#: transformers.Wav2Vec2ForCTC.forward:31
#: transformers.Wav2Vec2ForPreTraining.forward:31
#: transformers.Wav2Vec2Model.forward:31
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:34
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:34
#: transformers.FlaxWav2Vec2Model.__call__:34
#: transformers.Wav2Vec2ForCTC.forward:34
#: transformers.Wav2Vec2ForPreTraining.forward:34
#: transformers.Wav2Vec2Model.forward:34
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:37
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:37
#: transformers.FlaxWav2Vec2Model.__call__:37
#: transformers.Wav2Vec2ForCTC.forward:37
#: transformers.Wav2Vec2ForPreTraining.forward:37
#: transformers.Wav2Vec2Model.forward:37
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.Wav2Vec2Model.forward:40
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> from transformers import "
"Wav2Vec2Processor, Wav2Vec2Model     >>> from datasets import "
"load_dataset     >>> import soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")     "
">>> model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-"
"960h\")      >>> def map_to_array(batch):     >>>     speech, _ = "
"sf.read(batch[\"file\"])     >>>     batch[\"speech\"] = speech     >>>"
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"pt\").input_values  # Batch size 1     >>> hidden_states"
" = model(input_values).last_hidden_state"
msgstr ""

#: of transformers.Wav2Vec2Model.forward:40
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs."
msgstr ""

#: of transformers.Wav2Vec2Model.forward:44
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:51
#: transformers.Wav2Vec2ForPreTraining.forward:53
#: transformers.Wav2Vec2Model.forward:45
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:55
#: transformers.Wav2Vec2ForPreTraining.forward:57
#: transformers.Wav2Vec2Model.forward:49
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.Wav2Vec2Model.forward:75
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:94
msgid "Wav2Vec2ForCTC"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:1 transformers.Wav2Vec2ForCTC:1
msgid ""
"Wav2Vec2 Model with a `language modeling` head on top for Connectionist "
"Temporal Classification (CTC). Wav2Vec2 was proposed in `wav2vec 2.0: A "
"Framework for Self-Supervised Learning of Speech Representations "
"<https://arxiv.org/abs/2006.11477>`__ by Alexei Baevski, Henry Zhou, "
"Abdelrahman Mohamed, Michael Auli."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:1
msgid ""
"The :class:`~transformers.Wav2Vec2ForCTC` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:39
msgid ""
"Labels for connectionist temporal classification. Note that "
"``target_length`` has to be smaller or equal to the sequence length of "
"the output logits. Indices are selected in ``[-100, 0, ..., "
"config.vocab_size - 1]``. All labels set to ``-100`` are ignored "
"(masked), the loss is only computed for labels in ``[0, ..., "
"config.vocab_size - 1]``."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:45
msgid ""
"A :class:`~transformers.modeling_outputs.CausalLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction). - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> import torch     >>> from "
"transformers import Wav2Vec2Processor, Wav2Vec2ForCTC     >>> from "
"datasets import load_dataset     >>> import soundfile as sf      >>> "
"processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-"
"960h\")     >>> model = "
"Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")      >>> "
"def map_to_array(batch):     >>>     speech, _ = sf.read(batch[\"file\"])"
"     >>>     batch[\"speech\"] = speech     >>>     return batch      >>>"
" ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\","
" split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"pt\").input_values  # Batch size 1     >>> logits = "
"model(input_values).logits     >>> predicted_ids = torch.argmax(logits, "
"dim=-1)      >>> transcription = processor.decode(predicted_ids[0])      "
">>> # compute loss     >>> target_transcription = \"A MAN SAID TO THE "
"UNIVERSE SIR I EXIST\"      >>> # wrap processor as target processor to "
"encode labels     >>> with processor.as_target_processor():     >>>     "
"labels = processor(target_transcription, return_tensors=\"pt\").input_ids"
"      >>> loss = model(input_values, labels=labels).loss"
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:45
msgid ""
"A :class:`~transformers.modeling_outputs.CausalLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:49
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss (for "
"next-token prediction)."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:50
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.Wav2Vec2ForCTC.forward:94
msgid ""
":class:`~transformers.modeling_outputs.CausalLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:100
msgid "Wav2Vec2ForPreTraining"
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining:1
#: transformers.Wav2Vec2ForPreTraining:1
msgid ""
"Wav2Vec2 Model with a quantizer and `VQ` head on top. Wav2Vec2 was "
"proposed in `wav2vec 2.0: A Framework for Self-Supervised Learning of "
"Speech Representations <https://arxiv.org/abs/2006.11477>`__ by Alexei "
"Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli."
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:1
msgid ""
"The :class:`~transformers.Wav2Vec2ForPreTraining` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:28
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:28
#: transformers.FlaxWav2Vec2Model.__call__:28
#: transformers.Wav2Vec2ForPreTraining.forward:39
msgid ""
"Indices to mask extracted features for contrastive loss. When in training"
" mode, model learns to predict masked extracted features in "
"`config.proj_codevector_dim` space."
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:43
msgid ""
"A "
":class:`~transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.Wav2Vec2Config`) "
"and inputs.  - **loss** (`optional`, returned when model is in train "
"mode, ``torch.FloatTensor`` of shape :obj:`(1,)`) -- Total loss as the "
"sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated "
"in the `official   paper <https://arxiv.org/pdf/2006.11477.pdf>`__ . "
"(classification) loss. - **projected_states** (:obj:`torch.FloatTensor` "
"of shape :obj:`(batch_size, sequence_length, "
"config.proj_codevector_dim)`) -- Hidden-states of the model projected to "
"`config.proj_codevector_dim` that can be used to predict the masked   "
"projected quantized states. - **projected_quantized_states** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, "
"config.proj_codevector_dim)`) -- Quantized extracted feature vectors "
"projected to `config.proj_codevector_dim` representing the positive   "
"target vectors for contrastive loss. - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Example::      >>> import torch     >>> from "
"transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining     "
">>> from transformers.models.wav2vec2.modeling_wav2vec2 import "
"_compute_mask_indices     >>> from datasets import load_dataset     >>> "
"import soundfile as sf      >>> feature_extractor = "
"Wav2Vec2FeatureExtractor.from_pretrained(\"patrickvonplaten/wav2vec2-base\")"
"     >>> model = "
"Wav2Vec2ForPreTraining.from_pretrained(\"patrickvonplaten/wav2vec2-base\")"
"       >>> def map_to_array(batch):     ...     speech, _ = "
"sf.read(batch[\"file\"])     ...     batch[\"speech\"] = speech     ..."
"     return batch       >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = feature_extractor(ds[\"speech\"][0], "
"return_tensors=\"pt\").input_values  # Batch size 1      >>> # compute "
"masked indices     >>> batch_size, raw_sequence_length = "
"input_values.shape     >>> sequence_length = "
"model._get_feat_extract_output_lengths(raw_sequence_length)     >>> "
"mask_time_indices = _compute_mask_indices((batch_size, sequence_length), "
"mask_prob=0.2, mask_length=2, device=model.device)      >>> with "
"torch.no_grad():     ...     outputs = model(input_values, "
"mask_time_indices=mask_time_indices)      >>> # compute cosine similarity"
" between predicted (=projected_states) and target "
"(=projected_quantized_states)     >>> cosine_sim = "
"torch.cosine_similarity(     ...     outputs.projected_states, "
"outputs.projected_quantized_states, dim=-1     ... )      >>> # show that"
" cosine similarity is much higher than random     >>> assert "
"cosine_sim[mask_time_indices].mean() > 0.5      >>> # for contrastive "
"loss training model should be put into train mode     >>> model.train()"
"     >>> loss = model(input_values, "
"mask_time_indices=mask_time_indices).loss"
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:43
msgid ""
"A "
":class:`~transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.Wav2Vec2Config`) "
"and inputs."
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:47
msgid ""
"**loss** (`optional`, returned when model is in train mode, "
"``torch.FloatTensor`` of shape :obj:`(1,)`) -- Total loss as the sum of "
"the contrastive loss (L_m) and the diversity loss (L_d) as stated in the "
"`official paper <https://arxiv.org/pdf/2006.11477.pdf>`__ . "
"(classification) loss."
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:49
msgid ""
"**projected_states** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.proj_codevector_dim)`) -- "
"Hidden-states of the model projected to `config.proj_codevector_dim` that"
" can be used to predict the masked projected quantized states."
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:51
msgid ""
"**projected_quantized_states** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.proj_codevector_dim)`) -- "
"Quantized extracted feature vectors projected to "
"`config.proj_codevector_dim` representing the positive target vectors for"
" contrastive loss."
msgstr ""

#: of transformers.Wav2Vec2ForPreTraining.forward:106
msgid ""
":class:`~transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:107
msgid "TFWav2Vec2Model"
msgstr ""

#: of transformers.TFWav2Vec2Model:1
msgid ""
"The bare TFWav2Vec2 Model transformer outputing raw hidden-states without"
" any specific head on top."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:3 transformers.TFWav2Vec2Model:3
msgid ""
"This model inherits from :class:`~transformers.TFPreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:7 transformers.TFWav2Vec2Model:7
msgid ""
"This model is also a `tf.keras.Model "
"<https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. "
"Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:13 transformers.TFWav2Vec2Model:13
msgid "TF 2.0 models accepts two formats as inputs:"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:15 transformers.TFWav2Vec2Model:15
msgid "having all inputs as keyword arguments (like PyTorch models), or"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:16 transformers.TFWav2Vec2Model:16
msgid ""
"having all inputs as a list, tuple or dict in the first positional "
"arguments."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:18 transformers.TFWav2Vec2Model:18
msgid ""
"This second option is useful when using :meth:`tf.keras.Model.fit` method"
" which currently requires having all the tensors in the first argument of"
" the model call function: :obj:`model(inputs)`."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:21 transformers.TFWav2Vec2Model:21
msgid ""
"If you choose this second option, there are three possibilities you can "
"use to gather all the input Tensors in the first positional argument :"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:24 transformers.TFWav2Vec2Model:24
msgid ""
"a single Tensor with :obj:`input_values` only and nothing else: "
":obj:`model(inputs_ids)`"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:25 transformers.TFWav2Vec2Model:25
msgid ""
"a list of varying length with one or several input Tensors IN THE ORDER "
"given in the docstring: :obj:`model([input_values, attention_mask])` or "
":obj:`model([input_values, attention_mask, token_type_ids])`"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:27 transformers.TFWav2Vec2Model:27
msgid ""
"a dictionary with one or several input Tensors associated to the input "
"names given in the docstring: :obj:`model({\"input_values\": "
"input_values, \"token_type_ids\": token_type_ids})`"
msgstr ""

#: of transformers.TFWav2Vec2Model.call:1
msgid ""
"The :class:`~transformers.TFWav2Vec2Model` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:8 transformers.TFWav2Vec2Model.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.BertTokenizer`. See "
":func:`transformers.PreTrainedTokenizer.__call__` and "
":func:`transformers.PreTrainedTokenizer.encode` for details.  `What are "
"input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:8 transformers.TFWav2Vec2Model.call:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:10
#: transformers.TFWav2Vec2Model.call:10
msgid ""
"Indices can be obtained using :class:`~transformers.BertTokenizer`. See "
":func:`transformers.PreTrainedTokenizer.__call__` and "
":func:`transformers.PreTrainedTokenizer.encode` for details."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:16
#: transformers.TFWav2Vec2Model.call:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:16
#: transformers.TFWav2Vec2Model.call:16
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:23
#: transformers.TFWav2Vec2Model.call:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:  - 0 corresponds to a "
"`sentence A` token, - 1 corresponds to a `sentence B` token.  `What are "
"token type IDs? <../glossary.html#token-type-ids>`__"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:23
#: transformers.TFWav2Vec2Model.call:23
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices are selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:26
#: transformers.TFWav2Vec2Model.call:26
msgid "0 corresponds to a `sentence A` token,"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:27
#: transformers.TFWav2Vec2Model.call:27
msgid "1 corresponds to a `sentence B` token."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:31
#: transformers.TFWav2Vec2Model.call:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``.  `What are position IDs? <../glossary.html#position-ids>`__"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:31
#: transformers.TFWav2Vec2Model.call:31
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:34
#: transformers.TFWav2Vec2Model.call:34
msgid "`What are position IDs? <../glossary.html#position-ids>`__"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:36
#: transformers.TFWav2Vec2Model.call:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:36
#: transformers.TFWav2Vec2Model.call:36
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:38
#: transformers.TFWav2Vec2Model.call:38
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:39
#: transformers.TFWav2Vec2Model.call:39
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:41
#: transformers.TFWav2Vec2Model.call:41
msgid ""
"Optionally, instead of passing :obj:`input_values` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_values` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:45
#: transformers.TFWav2Vec2Model.call:45
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:49
#: transformers.TFWav2Vec2Model.call:49
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:53
#: transformers.TFWav2Vec2Model.call:53
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple. This argument can be used in eager mode, in "
"graph mode the value will always be set to True."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:56
#: transformers.TFWav2Vec2Model.call:56
msgid ""
"Whether or not to use the model in training mode (some modules like "
"dropout modules have different behaviors between training and "
"evaluation)."
msgstr ""

#: of transformers.TFWav2Vec2Model.call:60
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFBaseModelOutput` or a tuple"
" of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs.  - "
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model. - **hidden_states** "
"(:obj:`tuple(tf.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads.   "
"Example::      >>> from transformers import Wav2Vec2Processor, "
"TFWav2Vec2Model     >>> from datasets import load_dataset     >>> import "
"soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")     "
">>> model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-"
"960h\")      >>> def map_to_array(batch):     >>>     speech, _ = "
"sf.read(batch[\"file\"])     >>>     batch[\"speech\"] = speech     >>>"
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"tf\").input_values  # Batch size 1     >>> hidden_states"
" = model(input_values).last_hidden_state"
msgstr ""

#: of transformers.TFWav2Vec2Model.call:60
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFBaseModelOutput` or a tuple"
" of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs."
msgstr ""

#: of transformers.TFWav2Vec2Model.call:64
msgid ""
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model."
msgstr ""

#: of transformers.TFWav2Vec2Model.call:65
msgid ""
"**hidden_states** (:obj:`tuple(tf.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:74
#: transformers.TFWav2Vec2Model.call:69
msgid ""
"**attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFWav2Vec2Model.call:95
msgid ""
":class:`~transformers.modeling_tf_outputs.TFBaseModelOutput` or "
":obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:114
msgid "TFWav2Vec2ForCTC"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC:1
msgid ""
"TFWav2Vec2 Model with a `language modeling` head on top for Connectionist"
" Temporal Classification (CTC)."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:1
msgid ""
"The :class:`~transformers.TFWav2Vec2ForCTC` forward method, overrides the"
" :func:`__call__` special method."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:59
msgid ""
"Labels for computing the masked language modeling loss. Indices should be"
" in ``[-100, 0, ..., config.vocab_size]`` (see ``input_values`` "
"docstring) Tokens with indices set to ``-100`` are ignored (masked), the "
"loss is only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:64
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFCausalLMOutput` or a tuple "
"of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs.  - "
"**loss** (:obj:`tf.Tensor` of shape :obj:`(n,)`, `optional`, where n is "
"the number of non-masked labels, returned when :obj:`labels` is provided)"
" -- Language modeling loss (for next-token prediction). - **logits** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax). - **hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads.   "
"Example::      >>> import tensorflow as tf     >>> from transformers "
"import Wav2Vec2Processor, TFWav2Vec2ForCTC     >>> from datasets import "
"load_dataset     >>> import soundfile as sf      >>> processor = "
"Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")     "
">>> model = TFWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-"
"960h\")      >>> def map_to_array(batch):     >>>     speech, _ = "
"sf.read(batch[\"file\"])     >>>     batch[\"speech\"] = speech     >>>"
"     return batch      >>> ds = "
"load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", "
"split=\"validation\")     >>> ds = ds.map(map_to_array)      >>> "
"input_values = processor(ds[\"speech\"][0], "
"return_tensors=\"tf\").input_values # Batch size 1     >>> logits = "
"model(input_values).logits     >>> predicted_ids = tf.argmax(logits, "
"axis=-1)      >>> transcription = processor.decode(predicted_ids[0])"
"      >>> # compute loss     >>> target_transcription = \"A MAN SAID TO "
"THE UNIVERSE SIR I EXIST\"      >>> # wrap processor as target processor "
"to encode labels     >>> with processor.as_target_processor():     >>>"
"     labels = processor(transcription, return_tensors=\"tf\").input_ids"
"      >>> loss = model(input_values, labels=labels).loss"
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:64
msgid ""
"A :class:`~transformers.modeling_tf_outputs.TFCausalLMOutput` or a tuple "
"of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.Wav2Vec2Config`) and inputs."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:68
msgid ""
"**loss** (:obj:`tf.Tensor` of shape :obj:`(n,)`, `optional`, where n is "
"the number of non-masked labels, returned when :obj:`labels` is provided)"
" -- Language modeling loss (for next-token prediction)."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:69
msgid ""
"**logits** (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length,"
" config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:70
msgid ""
"**hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFWav2Vec2ForCTC.call:113
msgid ""
":class:`~transformers.modeling_tf_outputs.TFCausalLMOutput` or "
":obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:121
msgid "FlaxWav2Vec2Model"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:5
#: transformers.FlaxWav2Vec2ForPreTraining:5 transformers.FlaxWav2Vec2Model:5
msgid ""
"This model inherits from :class:`~transformers.FlaxPreTrainedModel`. "
"Check the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:9
#: transformers.FlaxWav2Vec2ForPreTraining:9 transformers.FlaxWav2Vec2Model:9
msgid ""
"This model is also a Flax Linen `flax.nn.Module "
"<https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html>`__"
" subclass. Use it as a regular Flax Module and refer to the Flax "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:13
#: transformers.FlaxWav2Vec2ForPreTraining:13 transformers.FlaxWav2Vec2Model:13
msgid "Finally, this model supports inherent JAX features such as:"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:15
#: transformers.FlaxWav2Vec2ForPreTraining:15 transformers.FlaxWav2Vec2Model:15
msgid ""
"`Just-In-Time (JIT) compilation "
"<https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-"
"jit>`__"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:16
#: transformers.FlaxWav2Vec2ForPreTraining:16 transformers.FlaxWav2Vec2Model:16
msgid ""
"`Automatic Differentiation <https://jax.readthedocs.io/en/latest/jax.html"
"#automatic-differentiation>`__"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:17
#: transformers.FlaxWav2Vec2ForPreTraining:17 transformers.FlaxWav2Vec2Model:17
msgid ""
"`Vectorization <https://jax.readthedocs.io/en/latest/jax.html"
"#vectorization-vmap>`__"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:18
#: transformers.FlaxWav2Vec2ForPreTraining:18 transformers.FlaxWav2Vec2Model:18
msgid ""
"`Parallelization <https://jax.readthedocs.io/en/latest/jax.html"
"#parallelization-pmap>`__"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC:20
#: transformers.FlaxWav2Vec2ForPreTraining:20 transformers.FlaxWav2Vec2Model:20
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.FlaxPreTrainedModel.from_pretrained` method to load "
"the model weights."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:1
#: transformers.FlaxWav2Vec2Model.__call__:1
msgid ""
"The :class:`~transformers.FlaxWav2Vec2PreTrainedModel` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:8
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:8
#: transformers.FlaxWav2Vec2Model.__call__:8
msgid ""
"Float values of input raw speech waveform. Values can be obtained by "
"loading a `.flac` or `.wav` audio file into an array of type "
"`List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library "
"(`pip install soundfile`). To prepare the array into `input_values`, the "
":class:`~transformers.Wav2Vec2Processor` should be used for padding and "
"conversion into a tensor of type `jnp.ndarray`. See "
":meth:`transformers.Wav2Vec2Processor.__call__` for details."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:14
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:14
#: transformers.FlaxWav2Vec2Model.__call__:14
msgid ""
"Mask to avoid performing convolution and attention on padding token "
"indices. Mask values selected in ``[0, 1]``:  - 1 for tokens that are "
"**not masked**, - 0 for tokens that are **masked**.  `What are attention "
"masks? <../glossary.html#attention-mask>`__ .. warning:: "
":obj:`attention_mask` should only be passed if the corresponding "
"processor has ``config.return_attention_mask == True``. For all models "
"whose processor has ``config.return_attention_mask == False``, such as "
"`wav2vec2-base <https://huggingface.co/facebook/wav2vec2-base-960h>`__, "
":obj:`attention_mask` should **not** be passed to avoid degraded "
"performance when doing batched inference. For such models "
":obj:`input_values` should simply be padded with 0 and passed without "
":obj:`attention_mask`. Be aware that these models also yield slightly "
"different results depending on whether :obj:`input_values` is padded or "
"not."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:20
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:20
#: transformers.FlaxWav2Vec2Model.__call__:20
msgid ""
"`What are attention masks? <../glossary.html#attention-mask>`__ .. "
"warning:: :obj:`attention_mask` should only be passed if the "
"corresponding processor has ``config.return_attention_mask == True``. For"
" all models whose processor has ``config.return_attention_mask == "
"False``, such as `wav2vec2-base <https://huggingface.co/facebook/wav2vec2"
"-base-960h>`__, :obj:`attention_mask` should **not** be passed to avoid "
"degraded performance when doing batched inference. For such models "
":obj:`input_values` should simply be padded with 0 and passed without "
":obj:`attention_mask`. Be aware that these models also yield slightly "
"different results depending on whether :obj:`input_values` is padded or "
"not."
msgstr ""

#: of transformers.FlaxWav2Vec2Model.__call__:40
msgid ""
"A "
":class:`~transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`) "
"and inputs.  - **last_hidden_state** (:obj:`jnp.ndarray` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - "
"**extract_features** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, last_conv_dim)`) -- Sequence of extracted feature "
"vectors of the last convolutional layer of the model with "
"``last_conv_dim``   being the dimension of the last convolutional layer. "
"- **hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when"
" ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxWav2Vec2Model.__call__:40
msgid ""
"A "
":class:`~transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`) "
"and inputs."
msgstr ""

#: of transformers.FlaxWav2Vec2Model.__call__:44
msgid ""
"**last_hidden_state** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model."
msgstr ""

#: of transformers.FlaxWav2Vec2Model.__call__:45
msgid ""
"**extract_features** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, last_conv_dim)`) -- Sequence of extracted feature "
"vectors of the last convolutional layer of the model with "
"``last_conv_dim`` being the dimension of the last convolutional layer."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:45
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:50
#: transformers.FlaxWav2Vec2Model.__call__:47
msgid ""
"**hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:49
#: transformers.FlaxWav2Vec2ForPreTraining.__call__:54
#: transformers.FlaxWav2Vec2Model.__call__:51
msgid ""
"**attentions** (:obj:`tuple(jnp.ndarray)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.FlaxWav2Vec2Model.__call__:56
msgid ""
":class:`~transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:128
msgid "FlaxWav2Vec2ForCTC"
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:40
msgid ""
"A :class:`~transformers.modeling_flax_outputs.FlaxMaskedLMOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.<class "
"'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`) "
"and inputs.  - **logits** (:obj:`jnp.ndarray` of shape :obj:`(batch_size,"
" sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
" - **hidden_states** (:obj:`tuple(jnp.ndarray)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` (one"
" for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:40
msgid ""
"A :class:`~transformers.modeling_flax_outputs.FlaxMaskedLMOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.<class "
"'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`) "
"and inputs."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:44
msgid ""
"**logits** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.FlaxWav2Vec2ForCTC.__call__:54
msgid ""
":class:`~transformers.modeling_flax_outputs.FlaxMaskedLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/wav2vec2.rst:134
msgid "FlaxWav2Vec2ForPreTraining"
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:1
msgid ""
"The :class:`~transformers.FlaxWav2Vec2ForPreTraining` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:40
msgid ""
"A "
":class:`~transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`) "
"and inputs.  - **loss** (`optional`, returned when model is in train "
"mode, ``jnp.ndarray`` of shape :obj:`(1,)`) -- Total loss as the sum of "
"the contrastive loss (L_m) and the diversity loss (L_d) as stated in the "
"`official   paper <https://arxiv.org/pdf/2006.11477.pdf>`__ . "
"(classification) loss. - **projected_states** (:obj:`jnp.ndarray` of "
"shape :obj:`(batch_size, sequence_length, config.proj_codevector_dim)`) "
"-- Hidden-states of the model projected to `config.proj_codevector_dim` "
"that can be used to predict the masked   projected quantized states. - "
"**projected_quantized_states** (:obj:`jnp.ndarray` of shape "
":obj:`(batch_size, sequence_length, config.proj_codevector_dim)`) -- "
"Quantized extracted feature vectors projected to "
"`config.proj_codevector_dim` representing the positive   target vectors "
"for contrastive loss. - **hidden_states** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of :obj:`jnp.ndarray` "
"(one for the output of the embeddings + one for the output of each layer)"
" of   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(jnp.ndarray)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`jnp.ndarray` (one "
"for each layer) of shape :obj:`(batch_size, num_heads, sequence_length,"
"   sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:40
msgid ""
"A "
":class:`~transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.<class "
"'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`) "
"and inputs."
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:44
msgid ""
"**loss** (`optional`, returned when model is in train mode, "
"``jnp.ndarray`` of shape :obj:`(1,)`) -- Total loss as the sum of the "
"contrastive loss (L_m) and the diversity loss (L_d) as stated in the "
"`official paper <https://arxiv.org/pdf/2006.11477.pdf>`__ . "
"(classification) loss."
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:46
msgid ""
"**projected_states** (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length, config.proj_codevector_dim)`) -- Hidden-states of the "
"model projected to `config.proj_codevector_dim` that can be used to "
"predict the masked projected quantized states."
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:48
msgid ""
"**projected_quantized_states** (:obj:`jnp.ndarray` of shape "
":obj:`(batch_size, sequence_length, config.proj_codevector_dim)`) -- "
"Quantized extracted feature vectors projected to "
"`config.proj_codevector_dim` representing the positive target vectors for"
" contrastive loss."
msgstr ""

#: of transformers.FlaxWav2Vec2ForPreTraining.__call__:59
msgid ""
":class:`~transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""


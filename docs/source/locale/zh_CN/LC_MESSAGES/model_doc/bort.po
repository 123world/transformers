# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/bort.rst:14
msgid "BORT"
msgstr ""

#: ../../source/model_doc/bort.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/bort.rst:19
msgid ""
"The BORT model was proposed in `Optimal Subarchitecture Extraction for "
"BERT <https://arxiv.org/abs/2010.10499>`__ by Adrian de Wynter and Daniel"
" J. Perry. It is an optimal subset of architectural parameters for the "
"BERT, which the authors refer to as \"Bort\"."
msgstr ""

#: ../../source/model_doc/bort.rst:23
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/bort.rst:25
#, python-format
msgid ""
"*We extract an optimal subset of architectural parameters for the BERT "
"architecture from Devlin et al. (2018) by applying recent breakthroughs "
"in algorithms for neural architecture search. This optimal subset, which "
"we refer to as \"Bort\", is demonstrably smaller, having an effective "
"(that is, not counting the embedding layer) size of 5.5% the original "
"BERT-large architecture, and 16% of the net size. Bort is also able to be"
" pretrained in 288 GPU hours, which is 1.2% of the time required to "
"pretrain the highest-performing BERT parametric architectural variant, "
"RoBERTa-large (Liu et al., 2019), and about 33% of that of the world-"
"record, in GPU hours, required to train BERT-large on the same hardware. "
"It is also 7.9x faster on a CPU, as well as being better performing than "
"other compressed variants of the architecture, and some of the non-"
"compressed variants: it obtains performance improvements of between 0.3% "
"and 31%, absolute, with respect to BERT-large, on multiple public natural"
" language understanding (NLU) benchmarks.*"
msgstr ""

#: ../../source/model_doc/bort.rst:35
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/bort.rst:37
msgid ""
"BORT's model architecture is based on BERT, so one can refer to "
":doc:`BERT's documentation page <bert>` for the model's API as well as "
"usage examples."
msgstr ""

#: ../../source/model_doc/bort.rst:39
msgid ""
"BORT uses the RoBERTa tokenizer instead of the BERT tokenizer, so one can"
" refer to :doc:`RoBERTa's documentation page <roberta>` for the "
"tokenizer's API as well as usage examples."
msgstr ""

#: ../../source/model_doc/bort.rst:41
msgid ""
"BORT requires a specific fine-tuning algorithm, called `Agora "
"<https://adewynter.github.io/notes/bort_algorithms_and_applications.html"
"#fine-tuning-with-algebraic-topology>`__ , that is sadly not open-sourced"
" yet. It would be very useful for the community, if someone tries to "
"implement the algorithm to make BORT fine-tuning work."
msgstr ""

#: ../../source/model_doc/bort.rst:46
msgid ""
"This model was contributed by `stefan-it <https://huggingface.co/stefan-"
"it>`__. The original code can be found `here "
"<https://github.com/alexa/bort/>`__."
msgstr ""


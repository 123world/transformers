# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/transformerxl.rst:14
msgid "Transformer XL"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:19
msgid ""
"The Transformer-XL model was proposed in `Transformer-XL: Attentive "
"Language Models Beyond a Fixed-Length Context "
"<https://arxiv.org/abs/1901.02860>`__ by Zihang Dai, Zhilin Yang, Yiming "
"Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. It's a causal "
"(uni-directional) transformer with relative positioning (sinuso√Ødal) "
"embeddings which can reuse previously computed hidden-states to attend to"
" longer context (memory). This model also uses adaptive softmax inputs "
"and outputs (tied)."
msgstr ""

#: ../../source/model_doc/transformerxl.rst:25
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:27
#, python-format
msgid ""
"*Transformers have a potential of learning longer-term dependency, but "
"are limited by a fixed-length context in the setting of language "
"modeling. We propose a novel neural architecture Transformer-XL that "
"enables learning dependency beyond a fixed length without disrupting "
"temporal coherence. It consists of a segment-level recurrence mechanism "
"and a novel positional encoding scheme. Our method not only enables "
"capturing longer-term dependency, but also resolves the context "
"fragmentation problem. As a result, Transformer-XL learns dependency that"
" is 80% longer than RNNs and 450% longer than vanilla Transformers, "
"achieves better performance on both short and long sequences, and is up "
"to 1,800+ times faster than vanilla Transformers during evaluation. "
"Notably, we improve the state-of-the-art results of bpc/perplexity to "
"0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion"
" Word, and 54.5 on Penn Treebank (without finetuning). When trained only "
"on WikiText-103, Transformer-XL manages to generate reasonably coherent, "
"novel text articles with thousands of tokens.*"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:38
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:40
msgid ""
"Transformer-XL uses relative sinusoidal positional embeddings. Padding "
"can be done on the left or on the right. The original implementation "
"trains on SQuAD with padding on the left, therefore the padding defaults "
"are set to left."
msgstr ""

#: ../../source/model_doc/transformerxl.rst:42
msgid "Transformer-XL is one of the few models that has no sequence length limit."
msgstr ""

#: ../../source/model_doc/transformerxl.rst:44
msgid ""
"This model was contributed by `thomwolf "
"<https://huggingface.co/thomwolf>`__. The original code can be found "
"`here <https://github.com/kimiyoung/transformer-xl>`__."
msgstr ""

#: ../../source/model_doc/transformerxl.rst:49
msgid "TransfoXLConfig"
msgstr ""

#: of transformers.TransfoXLConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.TransfoXLModel` or a "
":class:`~transformers.TFTransfoXLModel`. It is used to instantiate a "
"Transformer-XL model according to the specified arguments, defining the "
"model architecture. Instantiating a configuration with the defaults will "
"yield a similar configuration to that of the `Transformer XL "
"<https://huggingface.co/transfo-xl-wt103>`__ architecture."
msgstr ""

#: of transformers.TransfoXLConfig:6
msgid ""
"Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification
#: transformers.TFTransfoXLForSequenceClassification.call
#: transformers.TFTransfoXLLMHeadModel transformers.TFTransfoXLLMHeadModel.call
#: transformers.TFTransfoXLModel transformers.TFTransfoXLModel.call
#: transformers.TransfoXLConfig transformers.TransfoXLForSequenceClassification
#: transformers.TransfoXLForSequenceClassification.forward
#: transformers.TransfoXLLMHeadModel transformers.TransfoXLLMHeadModel.forward
#: transformers.TransfoXLModel transformers.TransfoXLModel.forward
#: transformers.TransfoXLTokenizer
#: transformers.TransfoXLTokenizer.save_vocabulary
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput
msgid "Parameters"
msgstr ""

#: of transformers.TransfoXLConfig:9
msgid ""
"Vocabulary size of the BERT model. Defines the number of different tokens"
" that can be represented by the :obj:`inputs_ids` passed when calling "
":class:`~transformers.TransfoXLModel` or "
":class:`~transformers.TFTransfoXLModel`."
msgstr ""

#: of transformers.TransfoXLConfig:13
msgid "Cutoffs for the adaptive softmax."
msgstr ""

#: of transformers.TransfoXLConfig:15
msgid "Dimensionality of the model's hidden states."
msgstr ""

#: of transformers.TransfoXLConfig:17
msgid "Dimensionality of the embeddings"
msgstr ""

#: of transformers.TransfoXLConfig:19
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.TransfoXLConfig:21
msgid "Dimensionality of the model's heads."
msgstr ""

#: of transformers.TransfoXLConfig:23
msgid "Inner dimension in FF"
msgstr ""

#: of transformers.TransfoXLConfig:25
msgid "Divident value for adapative input and softmax"
msgstr ""

#: of transformers.TransfoXLConfig:27
msgid ""
"Whether or not to apply LayerNorm to the input instead of the output in "
"the blocks."
msgstr ""

#: of transformers.TransfoXLConfig:29
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.TransfoXLConfig:31
msgid "Length of the retained previous heads."
msgstr ""

#: of transformers.TransfoXLConfig:33
msgid "Use the same pos embeddings after clamp_len."
msgstr ""

#: of transformers.TransfoXLConfig:35
msgid "Whether or not to use the same attn length for all tokens"
msgstr ""

#: of transformers.TransfoXLConfig:37
msgid "True to share all but first projs, False not to share."
msgstr ""

#: of transformers.TransfoXLConfig:39
msgid ""
"Attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et "
"al, 3 for Al Rfou et al."
msgstr ""

#: of transformers.TransfoXLConfig:41
msgid "Number of samples in the sampled softmax."
msgstr ""

#: of transformers.TransfoXLConfig:43
msgid "Whether or not to use adaptive softmax."
msgstr ""

#: of transformers.TransfoXLConfig:45
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.TransfoXLConfig:47
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.TransfoXLConfig:49
msgid "Whether ot not to untie relative position biases."
msgstr ""

#: of transformers.TransfoXLConfig:51
msgid "Parameter initializer to use."
msgstr ""

#: of transformers.TransfoXLConfig:53
msgid "Parameters initialized by U(-init_range, init_range)."
msgstr ""

#: of transformers.TransfoXLConfig:55 transformers.TransfoXLConfig:57
msgid "Parameters initialized by N(0, init_std)"
msgstr ""

#: of transformers.TransfoXLConfig:59
msgid "The epsilon to use in the layer normalization layers"
msgstr ""

#: of transformers.TransfoXLConfig:62
msgid "Examples::"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:56
msgid "TransfoXLTokenizer"
msgstr ""

#: of transformers.TransfoXLTokenizer:1
msgid ""
"Construct a Transformer-XL tokenizer adapted from Vocab class in `the "
"original code <https://github.com/kimiyoung/transformer-xl>`__. The "
"Transformer-XL tokenizer is a word-level tokenizer (no sub-word "
"tokenization)."
msgstr ""

#: of transformers.TransfoXLTokenizer:5
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods."
msgstr ""

#: of transformers.TransfoXLTokenizer:8
msgid ""
"A list of special tokens (to be treated by the original implementation of"
" this tokenizer)."
msgstr ""

#: of transformers.TransfoXLTokenizer:10
msgid ""
"The minimum number of times a token has to be present in order to be kept"
" in the vocabulary (otherwise it will be mapped to :obj:`unk_token`)."
msgstr ""

#: of transformers.TransfoXLTokenizer:13
msgid ""
"The maximum size of the vocabulary. If left unset, it will default to the"
" size of the vocabulary found after excluding the tokens according to the"
" :obj:`min_freq` rule."
msgstr ""

#: of transformers.TransfoXLTokenizer:16
msgid "Whether or not to lowercase the input when tokenizing."
msgstr ""

#: of transformers.TransfoXLTokenizer:18
msgid "The delimiter used between tokens."
msgstr ""

#: of transformers.TransfoXLTokenizer:20
msgid "File containing the vocabulary (from the original implementation)."
msgstr ""

#: of transformers.TransfoXLTokenizer:22
msgid ""
"File containing the vocabulary as saved with the :obj:`save_pretrained()`"
" method."
msgstr ""

#: of transformers.TransfoXLTokenizer:24
msgid ""
"List of tokens that should never be split. If no list is specified, will "
"simply use the existing special tokens."
msgstr ""

#: of transformers.TransfoXLTokenizer:27
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.TransfoXLTokenizer:30
msgid "The end of sequence token."
msgstr ""

#: of transformers.TransfoXLTokenizer:32
msgid "A list of additional special tokens (for the HuggingFace functionality)."
msgstr ""

#: of transformers.TransfoXLTokenizer:34
msgid "The language of this tokenizer (used for mose preprocessing)."
msgstr ""

#: of transformers.TransfoXLTokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.TransfoXLTokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.TransfoXLTokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.TransfoXLTokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call
#: transformers.TFTransfoXLLMHeadModel.call transformers.TFTransfoXLModel.call
#: transformers.TransfoXLForSequenceClassification.forward
#: transformers.TransfoXLLMHeadModel.forward
#: transformers.TransfoXLModel.forward
#: transformers.TransfoXLTokenizer.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.TransfoXLTokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call
#: transformers.TFTransfoXLLMHeadModel.call transformers.TFTransfoXLModel.call
#: transformers.TransfoXLForSequenceClassification.forward
#: transformers.TransfoXLLMHeadModel.forward
#: transformers.TransfoXLModel.forward
#: transformers.TransfoXLTokenizer.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.TransfoXLTokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:63
msgid "TransfoXL specific outputs"
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:1
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:1
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:1
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:1
msgid ""
"Base class for model's outputs that may also contain a past key/values "
"(to speed up sequential decoding)."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:3
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:3
msgid "Sequence of hidden-states at the output of the last layer of the model."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:7
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:5
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:7
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:5
msgid ""
"Contains pre-computed hidden-states (key and values in the attention "
"blocks). Can be used (see :obj:`mems` input) to speed up sequential "
"decoding. The token ids which have their past given to this model should "
"not be passed as input ids as they have already been computed."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:11
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:9
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden-states of the model at the output"
" of each layer plus the initial embedding outputs."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:11
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:9
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:59
#: transformers.TFTransfoXLLMHeadModel.call:57
#: transformers.TFTransfoXLModel.call:55
#: transformers.TransfoXLForSequenceClassification.forward:54
#: transformers.TransfoXLLMHeadModel.forward:55
#: transformers.TransfoXLModel.forward:49
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:14
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:12
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:14
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:12
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:16
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:14
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:16
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:14
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:63
#: transformers.TFTransfoXLLMHeadModel.call:61
#: transformers.TFTransfoXLModel.call:59
#: transformers.TransfoXLForSequenceClassification.forward:58
#: transformers.TransfoXLLMHeadModel.forward:59
#: transformers.TransfoXLModel.forward:53
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:19
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:17
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:19
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput:17
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:3
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:3
msgid "Language modeling losses (not reduced)."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:5
#: transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput:5
msgid ""
"Prediction scores of the language modeling head (scores for each "
"vocabulary token after SoftMax)."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:11
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:9
msgid ""
"Tuple of :obj:`tf.Tensor` (one for the output of the embeddings + one for"
" the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.  Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:11
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:9
msgid ""
"Tuple of :obj:`tf.Tensor` (one for the output of the embeddings + one for"
" the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:16
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:14
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput:16
#: transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput:14
msgid ""
"Tuple of :obj:`tf.Tensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: ../../source/model_doc/transformerxl.rst:79
msgid "TransfoXLModel"
msgstr ""

#: of transformers.TFTransfoXLModel:1 transformers.TransfoXLModel:1
msgid ""
"The bare Bert Model transformer outputting raw hidden-states without any "
"specific head on top."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification:13
#: transformers.TransfoXLLMHeadModel:5 transformers.TransfoXLModel:3
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TransfoXLForSequenceClassification:17
#: transformers.TransfoXLLMHeadModel:9 transformers.TransfoXLModel:7
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:40
#: transformers.TFTransfoXLLMHeadModel:32 transformers.TFTransfoXLModel:30
#: transformers.TransfoXLForSequenceClassification:21
#: transformers.TransfoXLLMHeadModel:13 transformers.TransfoXLModel:11
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.TransfoXLModel.forward:1
msgid ""
"The :class:`~transformers.TransfoXLModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:4
#: transformers.TFTransfoXLLMHeadModel.call:4
#: transformers.TFTransfoXLModel.call:4
#: transformers.TransfoXLForSequenceClassification.forward:4
#: transformers.TransfoXLLMHeadModel.forward:4
#: transformers.TransfoXLModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:8
#: transformers.TransfoXLLMHeadModel.forward:8
#: transformers.TransfoXLModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.TransfoXLTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:8
#: transformers.TFTransfoXLLMHeadModel.call:8
#: transformers.TFTransfoXLModel.call:8
#: transformers.TransfoXLForSequenceClassification.forward:8
#: transformers.TransfoXLLMHeadModel.forward:8
#: transformers.TransfoXLModel.forward:8
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:10
#: transformers.TransfoXLLMHeadModel.forward:10
#: transformers.TransfoXLModel.forward:10
msgid ""
"Indices can be obtained using :class:`~transformers.TransfoXLTokenizer`. "
"See :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:14
#: transformers.TFTransfoXLLMHeadModel.call:14
#: transformers.TFTransfoXLModel.call:14
#: transformers.TransfoXLForSequenceClassification.forward:14
#: transformers.TransfoXLLMHeadModel.forward:14
#: transformers.TransfoXLModel.forward:14
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:16
#: transformers.TFTransfoXLLMHeadModel.call:16
#: transformers.TFTransfoXLModel.call:16
#: transformers.TransfoXLForSequenceClassification.forward:16
#: transformers.TransfoXLLMHeadModel.forward:16
#: transformers.TransfoXLModel.forward:16
msgid ""
"Contains pre-computed hidden-states (key and values in the attention "
"blocks) as computed by the model (see :obj:`mems` output below). Can be "
"used to speed up sequential decoding. The token ids which have their mems"
" given to this model should not be passed as :obj:`input_ids` as they "
"have already been computed."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:20
#: transformers.TFTransfoXLLMHeadModel.call:20
#: transformers.TFTransfoXLModel.call:20
#: transformers.TransfoXLForSequenceClassification.forward:20
#: transformers.TransfoXLLMHeadModel.forward:20
#: transformers.TransfoXLModel.forward:20
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:20
#: transformers.TFTransfoXLLMHeadModel.call:20
#: transformers.TFTransfoXLModel.call:20
#: transformers.TransfoXLForSequenceClassification.forward:20
#: transformers.TransfoXLLMHeadModel.forward:20
#: transformers.TransfoXLModel.forward:20
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:22
#: transformers.TFTransfoXLLMHeadModel.call:22
#: transformers.TFTransfoXLModel.call:22
#: transformers.TransfoXLForSequenceClassification.forward:22
#: transformers.TransfoXLLMHeadModel.forward:22
#: transformers.TransfoXLModel.forward:22
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:23
#: transformers.TFTransfoXLLMHeadModel.call:23
#: transformers.TFTransfoXLModel.call:23
#: transformers.TransfoXLForSequenceClassification.forward:23
#: transformers.TransfoXLLMHeadModel.forward:23
#: transformers.TransfoXLModel.forward:23
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:25
#: transformers.TFTransfoXLLMHeadModel.call:25
#: transformers.TFTransfoXLModel.call:25
#: transformers.TransfoXLForSequenceClassification.forward:25
#: transformers.TransfoXLLMHeadModel.forward:25
#: transformers.TransfoXLModel.forward:25
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:29
#: transformers.TransfoXLLMHeadModel.forward:29
#: transformers.TransfoXLModel.forward:29
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:32
#: transformers.TransfoXLLMHeadModel.forward:32
#: transformers.TransfoXLModel.forward:32
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:35
#: transformers.TransfoXLLMHeadModel.forward:35
#: transformers.TransfoXLModel.forward:35
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.TransfoXLModel.forward:38
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TransfoXLConfig`) "
"and inputs.  - **last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **mems** "
"(:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`) -- "
"Contains pre-computed hidden-states (key and values in the attention "
"blocks). Can be used (see :obj:`mems`   input) to speed up sequential "
"decoding. The token ids which have their past given to this model should "
"not   be passed as input ids as they have already been computed. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.TransfoXLModel.forward:38
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TransfoXLConfig`) "
"and inputs."
msgstr ""

#: of transformers.TransfoXLModel.forward:42
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:48
#: transformers.TransfoXLLMHeadModel.forward:49
#: transformers.TransfoXLModel.forward:43
msgid ""
"**mems** (:obj:`List[torch.FloatTensor]` of length "
":obj:`config.n_layers`) -- Contains pre-computed hidden-states (key and "
"values in the attention blocks). Can be used (see :obj:`mems` input) to "
"speed up sequential decoding. The token ids which have their past given "
"to this model should not be passed as input ids as they have already been"
" computed."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:51
#: transformers.TransfoXLLMHeadModel.forward:52
#: transformers.TransfoXLModel.forward:46
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:55
#: transformers.TransfoXLLMHeadModel.forward:56
#: transformers.TransfoXLModel.forward:50
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.TransfoXLModel.forward:55
msgid ""
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:67
#: transformers.TFTransfoXLLMHeadModel.call:65
#: transformers.TFTransfoXLModel.call:63
#: transformers.TransfoXLForSequenceClassification.forward:62
#: transformers.TransfoXLLMHeadModel.forward:63
#: transformers.TransfoXLModel.forward:57
msgid "Example::"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:86
msgid "TransfoXLLMHeadModel"
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel:1 transformers.TransfoXLLMHeadModel:1
msgid ""
"The Transformer-XL Model with a language modeling head on top (adaptive "
"softmax with weights tied to the adaptive input embeddings)"
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:1
msgid ""
"The :class:`~transformers.TransfoXLLMHeadModel` forward method, overrides"
" the :func:`__call__` special method."
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:37
msgid ""
"Labels for language modeling. Note that the labels **are shifted** inside"
" the model, i.e. you can set ``labels = input_ids`` Indices are selected "
"in ``[-100, 0, ..., config.vocab_size]`` All labels set to ``-100`` are "
"ignored (masked), the loss is only computed for labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:42
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TransfoXLConfig`) "
"and inputs.  - **losses** (:obj:`torch.FloatTensor` of shape "
"`(batch_size, sequence_length-1)`, `optional`, returned when ``labels`` "
"is provided)   Language modeling losses (not reduced). - "
"**prediction_scores** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"after SoftMax). - **mems** (:obj:`List[torch.FloatTensor]` of length "
":obj:`config.n_layers`) -- Contains pre-computed hidden-states (key and "
"values in the attention blocks). Can be used (see :obj:`mems`   input) to"
" speed up sequential decoding. The token ids which have their past given "
"to this model should not   be passed as input ids as they have already "
"been computed. - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:42
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TransfoXLConfig`) "
"and inputs."
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:46
msgid ""
"**losses** (:obj:`torch.FloatTensor` of shape `(batch_size, "
"sequence_length-1)`, `optional`, returned when ``labels`` is provided) "
"Language modeling losses (not reduced)."
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:48
msgid ""
"**prediction_scores** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"after SoftMax)."
msgstr ""

#: of transformers.TransfoXLLMHeadModel.forward:61
msgid ""
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:93
msgid "TransfoXLForSequenceClassification"
msgstr ""

#: of transformers.TransfoXLForSequenceClassification:1
msgid ""
"The Transformer-XL Model transformer with a sequence classification head "
"on top (linear layer)."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification:3
msgid ""
":class:`~transformers.TransfoXLForSequenceClassification` uses the last "
"token in order to do the classification, as other causal models (e.g. "
"GPT-1) do."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:6
#: transformers.TransfoXLForSequenceClassification:6
msgid ""
"Since it does classification on the last token, it requires to know the "
"position of the last token. If a :obj:`pad_token_id` is defined in the "
"configuration, it finds the last token that is not a padding token in "
"each row. If no :obj:`pad_token_id` is defined, it simply takes the last "
"value in each row of the batch. Since it cannot guess the padding tokens "
"when :obj:`inputs_embeds` are passed instead of :obj:`input_ids`, it does"
" the same (take the last value in each row of the batch)."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:1
msgid ""
"The :class:`~transformers.TransfoXLForSequenceClassification` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:37
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy)."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:42
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TransfoXLConfig`) "
"and inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Classification "
"(or regression if config.num_labels==1) loss. - **logits** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax). - **mems** "
"(:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`) -- "
"Contains pre-computed hidden-states (key and values in the attention "
"blocks). Can be used (see :obj:`mems`   input) to speed up sequential "
"decoding. The token ids which have their past given to this model should "
"not   be passed as input ids as they have already been computed. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:42
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TransfoXLConfig`) "
"and inputs."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:46
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:47
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.TransfoXLForSequenceClassification.forward:60
msgid ""
":class:`~transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:100
msgid "TFTransfoXLModel"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:13
#: transformers.TFTransfoXLLMHeadModel:5 transformers.TFTransfoXLModel:3
msgid ""
"This model inherits from :class:`~transformers.TFPreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:17
#: transformers.TFTransfoXLLMHeadModel:9 transformers.TFTransfoXLModel:7
msgid ""
"This model is also a `tf.keras.Model "
"<https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. "
"Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:23
#: transformers.TFTransfoXLLMHeadModel:15 transformers.TFTransfoXLModel:13
msgid "TF 2.0 models accepts two formats as inputs:"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:25
#: transformers.TFTransfoXLLMHeadModel:17 transformers.TFTransfoXLModel:15
msgid "having all inputs as keyword arguments (like PyTorch models), or"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:26
#: transformers.TFTransfoXLLMHeadModel:18 transformers.TFTransfoXLModel:16
msgid ""
"having all inputs as a list, tuple or dict in the first positional "
"arguments."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:28
#: transformers.TFTransfoXLLMHeadModel:20 transformers.TFTransfoXLModel:18
msgid ""
"This second option is useful when using :meth:`tf.keras.Model.fit` method"
" which currently requires having all the tensors in the first argument of"
" the model call function: :obj:`model(inputs)`."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:31
#: transformers.TFTransfoXLLMHeadModel:23 transformers.TFTransfoXLModel:21
msgid ""
"If you choose this second option, there are three possibilities you can "
"use to gather all the input Tensors in the first positional argument :"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:34
#: transformers.TFTransfoXLLMHeadModel:26 transformers.TFTransfoXLModel:24
msgid ""
"a single Tensor with :obj:`input_ids` only and nothing else: "
":obj:`model(inputs_ids)`"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:35
#: transformers.TFTransfoXLLMHeadModel:27 transformers.TFTransfoXLModel:25
msgid ""
"a list of varying length with one or several input Tensors IN THE ORDER "
"given in the docstring: :obj:`model([input_ids, attention_mask])` or "
":obj:`model([input_ids, attention_mask, token_type_ids])`"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:37
#: transformers.TFTransfoXLLMHeadModel:29 transformers.TFTransfoXLModel:27
msgid ""
"a dictionary with one or several input Tensors associated to the input "
"names given in the docstring: :obj:`model({\"input_ids\": input_ids, "
"\"token_type_ids\": token_type_ids})`"
msgstr ""

#: of transformers.TFTransfoXLModel.call:1
msgid ""
"The :class:`~transformers.TFTransfoXLModel` forward method, overrides the"
" :func:`__call__` special method."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:8
#: transformers.TFTransfoXLLMHeadModel.call:8
#: transformers.TFTransfoXLModel.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.BertTokenizer`. See "
":func:`transformers.PreTrainedTokenizer.__call__` and "
":func:`transformers.PreTrainedTokenizer.encode` for details.  `What are "
"input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:10
#: transformers.TFTransfoXLLMHeadModel.call:10
#: transformers.TFTransfoXLModel.call:10
msgid ""
"Indices can be obtained using :class:`~transformers.BertTokenizer`. See "
":func:`transformers.PreTrainedTokenizer.__call__` and "
":func:`transformers.PreTrainedTokenizer.encode` for details."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:29
#: transformers.TFTransfoXLLMHeadModel.call:29
#: transformers.TFTransfoXLModel.call:29
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:33
#: transformers.TFTransfoXLLMHeadModel.call:33
#: transformers.TFTransfoXLModel.call:33
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail. This argument "
"can be used only in eager mode, in graph mode the value in the config "
"will be used instead."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:37
#: transformers.TFTransfoXLLMHeadModel.call:37
#: transformers.TFTransfoXLModel.call:37
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple. This argument can be used in eager mode, in "
"graph mode the value will always be set to True."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:40
#: transformers.TFTransfoXLLMHeadModel.call:40
#: transformers.TFTransfoXLModel.call:40
msgid ""
"Whether or not to use the model in training mode (some modules like "
"dropout modules have different behaviors between training and "
"evaluation)."
msgstr ""

#: of transformers.TFTransfoXLModel.call:44
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TransfoXLConfig`) and inputs."
"  - **last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model. - **mems** (:obj:`List[tf.Tensor]`"
" of length :obj:`config.n_layers`) -- Contains pre-computed hidden-states"
" (key and values in the attention blocks). Can be used (see :obj:`mems`"
"   input) to speed up sequential decoding. The token ids which have their"
" past given to this model should not   be passed as input ids as they "
"have already been computed. - **hidden_states** (:obj:`tuple(tf.Tensor)`,"
" `optional`, returned when ``output_hidden_states=True`` is passed or "
"when ``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` "
"(one for the output of the embeddings + one for the output of each layer)"
" of   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.TFTransfoXLModel.call:44
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TransfoXLConfig`) and inputs."
msgstr ""

#: of transformers.TFTransfoXLModel.call:48
msgid ""
"**last_hidden_state** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`) -- Sequence of hidden-states at the "
"output of the last layer of the model."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:53
#: transformers.TFTransfoXLLMHeadModel.call:51
#: transformers.TFTransfoXLModel.call:49
msgid ""
"**mems** (:obj:`List[tf.Tensor]` of length :obj:`config.n_layers`) -- "
"Contains pre-computed hidden-states (key and values in the attention "
"blocks). Can be used (see :obj:`mems` input) to speed up sequential "
"decoding. The token ids which have their past given to this model should "
"not be passed as input ids as they have already been computed."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:56
#: transformers.TFTransfoXLLMHeadModel.call:54
#: transformers.TFTransfoXLModel.call:52
msgid ""
"**hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:60
#: transformers.TFTransfoXLLMHeadModel.call:58
#: transformers.TFTransfoXLModel.call:56
msgid ""
"**attentions** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFTransfoXLModel.call:61
msgid ""
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:107
msgid "TFTransfoXLLMHeadModel"
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel.call:1
msgid ""
"The :class:`~transformers.TFTransfoXLLMHeadModel` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel.call:44
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TransfoXLConfig`) and inputs."
"  - **losses** (:obj:`tf.Tensor` of shape `(batch_size, "
"sequence_length-1)`, `optional`, returned when ``labels`` is provided)   "
"Language modeling losses (not reduced). - **prediction_scores** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head "
"(scores for each vocabulary token after SoftMax). - **mems** "
"(:obj:`List[tf.Tensor]` of length :obj:`config.n_layers`) -- Contains "
"pre-computed hidden-states (key and values in the attention blocks). Can "
"be used (see :obj:`mems`   input) to speed up sequential decoding. The "
"token ids which have their past given to this model should not   be "
"passed as input ids as they have already been computed. - "
"**hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel.call:44
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TransfoXLConfig`) and inputs."
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel.call:48
msgid ""
"**losses** (:obj:`tf.Tensor` of shape `(batch_size, sequence_length-1)`, "
"`optional`, returned when ``labels`` is provided) Language modeling "
"losses (not reduced)."
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel.call:50
msgid ""
"**prediction_scores** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token after SoftMax)."
msgstr ""

#: of transformers.TFTransfoXLLMHeadModel.call:63
msgid ""
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:114
msgid "TFTransfoXLForSequenceClassification"
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:1
msgid ""
"The Transfo XL Model transformer with a sequence classification head on "
"top (linear layer)."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification:3
msgid ""
":class:`~transformers.TFTransfoXLForSequenceClassification` uses the last"
" token in order to do the classification, as other causal models (e.g. "
"GPT-1,GPT-2) do."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:1
msgid ""
"The :class:`~transformers.TFTransfoXLForSequenceClassification` forward "
"method, overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:43
msgid ""
"Labels for computing the cross entropy classification loss. Indices "
"should be in ``[0, ..., config.vocab_size - 1]``."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:47
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TransfoXLConfig`) and inputs."
"  - **loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned"
" when :obj:`labels` is provided) -- Classification (or regression if "
"config.num_labels==1) loss. - **logits** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, config.num_labels)`) -- Classification (or regression "
"if config.num_labels==1) scores (before SoftMax). - **mems** "
"(:obj:`List[tf.Tensor]` of length :obj:`config.n_layers`) -- Contains "
"pre-computed hidden-states (key and values in the attention blocks). Can "
"be used (see :obj:`mems`   input) to speed up sequential decoding. The "
"token ids which have their past given to this model should not   be "
"passed as input ids as they have already been computed. - "
"**hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings + one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden-"
"states of the model at the output of each layer plus the initial "
"embedding outputs. - **attentions** (:obj:`tuple(tf.Tensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention   heads."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:47
msgid ""
"A "
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TransfoXLConfig`) and inputs."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:51
msgid ""
"**loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Classification (or regression if "
"config.num_labels==1) loss."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:52
msgid ""
"**logits** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.TFTransfoXLForSequenceClassification.call:65
msgid ""
":class:`~transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/transformerxl.rst:121
msgid "Internal Layers"
msgstr ""


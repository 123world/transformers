# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/cpm.rst:14
msgid "CPM"
msgstr ""

#: ../../source/model_doc/cpm.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/cpm.rst:19
msgid ""
"The CPM model was proposed in `CPM: A Large-scale Generative Chinese Pre-"
"trained Language Model <https://arxiv.org/abs/2012.00413>`__ by Zhengyan "
"Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng"
" Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang"
" Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, "
"Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun."
msgstr ""

#: ../../source/model_doc/cpm.rst:24
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/cpm.rst:26
msgid ""
"*Pre-trained Language Models (PLMs) have proven to be beneficial for "
"various downstream NLP tasks. Recently, GPT-3, with 175 billion "
"parameters and 570GB training data, drew a lot of attention due to the "
"capacity of few-shot (even zero-shot) learning. However, applying GPT-3 "
"to address Chinese NLP tasks is still challenging, as the training corpus"
" of GPT-3 is primarily English, and the parameters are not publicly "
"available. In this technical report, we release the Chinese Pre-trained "
"Language Model (CPM) with generative pre-training on large-scale Chinese "
"training data. To the best of our knowledge, CPM, with 2.6 billion "
"parameters and 100GB Chinese training data, is the largest Chinese pre-"
"trained language model, which could facilitate several downstream Chinese"
" NLP tasks, such as conversation, essay generation, cloze test, and "
"language understanding. Extensive experiments demonstrate that CPM "
"achieves strong performance on many NLP tasks in the settings of few-shot"
" (even zero-shot) learning.*"
msgstr ""

#: ../../source/model_doc/cpm.rst:36
msgid ""
"This model was contributed by `canwenxu "
"<https://huggingface.co/canwenxu>`__. The original implementation can be "
"found here: https://github.com/TsinghuaAI/CPM-Generate"
msgstr ""

#: ../../source/model_doc/cpm.rst:39
msgid ""
"Note: We only have a tokenizer here, since the model architecture is the "
"same as GPT-2."
msgstr ""

#: ../../source/model_doc/cpm.rst:42
msgid "CpmTokenizer"
msgstr ""

#: of transformers.CpmTokenizer:1
msgid ""
"Runs pre-tokenization with Jieba segmentation tool. It is used in CPM "
"models."
msgstr ""


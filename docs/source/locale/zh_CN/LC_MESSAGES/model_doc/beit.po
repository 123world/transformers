# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/beit.rst:14
msgid "BEiT"
msgstr ""

#: ../../source/model_doc/beit.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/beit.rst:19
msgid ""
"The BEiT model was proposed in `BEiT: BERT Pre-Training of Image "
"Transformers <https://arxiv.org/abs/2106.08254>`__ by Hangbo Bao, Li Dong"
" and Furu Wei. Inspired by BERT, BEiT is the first paper that makes self-"
"supervised pre-training of Vision Transformers (ViTs) outperform "
"supervised pre-training. Rather than pre-training the model to predict "
"the class of an image (as done in the `original ViT paper "
"<https://arxiv.org/abs/2010.11929>`__), BEiT models are pre-trained to "
"predict visual tokens from the codebook of OpenAI's `DALL-E model "
"<https://arxiv.org/abs/2102.12092>`__ given masked patches."
msgstr ""

#: ../../source/model_doc/beit.rst:26
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/beit.rst:28
#, python-format
msgid ""
"*We introduce a self-supervised vision representation model BEiT, which "
"stands for Bidirectional Encoder representation from Image Transformers. "
"Following BERT developed in the natural language processing area, we "
"propose a masked image modeling task to pretrain vision Transformers. "
"Specifically, each image has two views in our pre-training, i.e, image "
"patches (such as 16x16 pixels), and visual tokens (i.e., discrete "
"tokens). We first \"tokenize\" the original image into visual tokens. "
"Then we randomly mask some image patches and fed them into the backbone "
"Transformer. The pre-training objective is to recover the original visual"
" tokens based on the corrupted image patches. After pre-training BEiT, we"
" directly fine-tune the model parameters on downstream tasks by appending"
" task layers upon the pretrained encoder. Experimental results on image "
"classification and semantic segmentation show that our model achieves "
"competitive results with previous pre-training methods. For example, "
"base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, "
"significantly outperforming from-scratch DeiT training (81.8%) with the "
"same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-"
"1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K"
" (85.2%).*"
msgstr ""

#: ../../source/model_doc/beit.rst:40
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/beit.rst:42
msgid ""
"BEiT models are regular Vision Transformers, but pre-trained in a self-"
"supervised way rather than supervised. They outperform both the original "
"model (ViT) as well as Data-efficient Image Transformers (DeiT) when "
"fine-tuned on ImageNet-1K and CIFAR-100."
msgstr ""

#: ../../source/model_doc/beit.rst:45
msgid ""
"As the BEiT models expect each image to be of the same size (resolution),"
" one can use :class:`~transformers.BeitFeatureExtractor` to resize (or "
"rescale) and normalize images for the model."
msgstr ""

#: ../../source/model_doc/beit.rst:47
msgid ""
"Both the patch resolution and image resolution used during pre-training "
"or fine-tuning are reflected in the name of each checkpoint. For example,"
" :obj:`microsoft/beit-base-patch16-224` refers to a base-sized "
"architecture with patch resolution of 16x16 and fine-tuning resolution of"
" 224x224. All checkpoints can be found on the `hub "
"<https://huggingface.co/models?search=microsoft/beit>`__."
msgstr ""

#: ../../source/model_doc/beit.rst:51
msgid ""
"The available checkpoints are either (1) pre-trained on `ImageNet-22k "
"<http://www.image-net.org/>`__ (a collection of 14 million images and 22k"
" classes) only, (2) also fine-tuned on ImageNet-22k or (3) also fine-"
"tuned on `ImageNet-1k <http://www.image-"
"net.org/challenges/LSVRC/2012/>`__ (also referred to as ILSVRC 2012, a "
"collection of 1.3 million images and 1,000 classes)."
msgstr ""

#: ../../source/model_doc/beit.rst:55
msgid ""
"BEiT uses relative position embeddings, inspired by the T5 model. During "
"pre-training, the authors shared the relative position bias among the "
"several self-attention layers. During fine-tuning, each layer's relative "
"position bias is initialized with the shared relative position bias "
"obtained after pre-training. Note that, if one wants to pre-train a model"
" from scratch, one needs to either set the "
":obj:`use_relative_position_bias` or the "
":obj:`use_relative_position_bias` attribute of "
":class:`~transformers.BeitConfig` to :obj:`True` in order to add position"
" embeddings."
msgstr ""

#: ../../source/model_doc/beit.rst:62
msgid ""
"This model was contributed by `nielsr <https://huggingface.co/nielsr>`__."
" The original code can be found `here "
"<https://github.com/microsoft/unilm/tree/master/beit>`__."
msgstr ""

#: ../../source/model_doc/beit.rst:66
msgid "BeitConfig"
msgstr ""

#: of transformers.BeitConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.BeitModel`. It is used to instantiate an BEiT model"
" according to the specified arguments, defining the model architecture. "
"Instantiating a configuration with the defaults will yield a similar "
"configuration to that of the BEiT `microsoft/beit-base-patch16-224-in22k "
"<https://huggingface.co/microsoft/beit-base-patch16-224-in22k>`__ "
"architecture."
msgstr ""

#: of transformers.BeitConfig transformers.BeitFeatureExtractor
#: transformers.BeitFeatureExtractor.__call__
#: transformers.BeitForImageClassification
#: transformers.BeitForImageClassification.forward
#: transformers.BeitForMaskedImageModeling
#: transformers.BeitForMaskedImageModeling.forward transformers.BeitModel
#: transformers.BeitModel.forward
msgid "Parameters"
msgstr ""

#: of transformers.BeitConfig:7
msgid ""
"Vocabulary size of the BEiT model. Defines the number of different image "
"tokens that can be used during pre-training."
msgstr ""

#: of transformers.BeitConfig:10
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.BeitConfig:12
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.BeitConfig:14
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.BeitConfig:16
msgid ""
"Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the "
"Transformer encoder."
msgstr ""

#: of transformers.BeitConfig:18
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` "
"and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.BeitConfig:21
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.BeitConfig:23
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.BeitConfig:25
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.BeitConfig:27
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.BeitConfig:29
msgid ""
"If True, use gradient checkpointing to save memory at the expense of "
"slower backward pass."
msgstr ""

#: of transformers.BeitConfig:31
msgid "The size (resolution) of each image."
msgstr ""

#: of transformers.BeitConfig:33
msgid "The size (resolution) of each patch."
msgstr ""

#: of transformers.BeitConfig:35
msgid "The number of input channels."
msgstr ""

#: of transformers.BeitConfig:37
msgid "Whether to use a mask token for masked image modeling."
msgstr ""

#: of transformers.BeitConfig:39
msgid "Whether to use BERT-style absolute position embeddings."
msgstr ""

#: of transformers.BeitConfig:41
msgid ""
"Whether to use T5-style relative position embeddings in the self-"
"attention layers."
msgstr ""

#: of transformers.BeitConfig:43
msgid ""
"Whether to use the same relative position embeddings across all self-"
"attention layers of the Transformer."
msgstr ""

#: of transformers.BeitConfig:45
msgid ""
"Scale to use in the self-attention layers. 0.1 for base, 1e-5 for large. "
"Set 0 to disable layer scale."
msgstr ""

#: of transformers.BeitConfig:47
msgid ""
"Stochastic depth rate per sample (when applied in the main path of "
"residual layers)."
msgstr ""

#: of transformers.BeitConfig:49
msgid ""
"Whether to mean pool the final hidden states of the patches instead of "
"using the final hidden state of the CLS token, before applying the "
"classification head."
msgstr ""

#: of transformers.BeitConfig:53
msgid "Example::"
msgstr ""

#: ../../source/model_doc/beit.rst:73
msgid "BeitFeatureExtractor"
msgstr ""

#: of transformers.BeitFeatureExtractor:1
msgid "Constructs a BEiT feature extractor."
msgstr ""

#: of transformers.BeitFeatureExtractor:3
msgid ""
"This feature extractor inherits from "
":class:`~transformers.FeatureExtractionMixin` which contains most of the "
"main methods. Users should refer to this superclass for more information "
"regarding those methods."
msgstr ""

#: of transformers.BeitFeatureExtractor:6
msgid "Whether to resize the input to a certain :obj:`size`."
msgstr ""

#: of transformers.BeitFeatureExtractor:8
msgid ""
"Resize the input to the given size. If a tuple is provided, it should be "
"(width, height). If only an integer is provided, then the input will be "
"resized to (size, size). Only has an effect if :obj:`do_resize` is set to"
" :obj:`True`."
msgstr ""

#: of transformers.BeitFeatureExtractor:12
msgid ""
"An optional resampling filter. This can be one of "
":obj:`PIL.Image.NEAREST`, :obj:`PIL.Image.BOX`, "
":obj:`PIL.Image.BILINEAR`, :obj:`PIL.Image.HAMMING`, "
":obj:`PIL.Image.BICUBIC` or :obj:`PIL.Image.LANCZOS`. Only has an effect "
"if :obj:`do_resize` is set to :obj:`True`."
msgstr ""

#: of transformers.BeitFeatureExtractor:16
msgid ""
"Whether to crop the input at the center. If the input size is smaller "
"than :obj:`crop_size` along any edge, the image is padded with 0's and "
"then center cropped."
msgstr ""

#: of transformers.BeitFeatureExtractor:19
msgid ""
"Desired output size when applying center-cropping. Only has an effect if "
":obj:`do_center_crop` is set to :obj:`True`."
msgstr ""

#: of transformers.BeitFeatureExtractor:22
msgid ""
"Whether or not to normalize the input with :obj:`image_mean` and "
":obj:`image_std`."
msgstr ""

#: of transformers.BeitFeatureExtractor:24
msgid ""
"The sequence of means for each channel, to be used when normalizing "
"images."
msgstr ""

#: of transformers.BeitFeatureExtractor:26
msgid ""
"The sequence of standard deviations for each channel, to be used when "
"normalizing images."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:1
msgid "Main method to prepare for the model one or several image(s)."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:5
msgid ""
"NumPy arrays and PyTorch tensors are converted to PIL images when "
"resizing, so the most efficient is to pass PIL images."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:8
msgid ""
"The image or batch of images to be prepared. Each image can be a PIL "
"image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch "
"tensor, each image should be of shape (C, H, W), where C is a number of "
"channels, H and W are image height and width."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:12
msgid ""
"If set, will return tensors of a particular framework. Acceptable values "
"are:  * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects. * "
":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects. * :obj:`'np'`: "
"Return NumPy :obj:`np.ndarray` objects. * :obj:`'jax'`: Return JAX "
":obj:`jnp.ndarray` objects."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:12
msgid ""
"If set, will return tensors of a particular framework. Acceptable values "
"are:"
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:14
msgid ":obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:15
msgid ":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:16
msgid ":obj:`'np'`: Return NumPy :obj:`np.ndarray` objects."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:17
msgid ":obj:`'jax'`: Return JAX :obj:`jnp.ndarray` objects."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__
#: transformers.BeitForImageClassification.forward
#: transformers.BeitForMaskedImageModeling.forward
#: transformers.BeitModel.forward
msgid "Returns"
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:20
msgid ""
"A :class:`~transformers.BatchFeature` with the following fields:  - "
"**pixel_values** -- Pixel values to be fed to a model, of shape "
"(batch_size, num_channels, height,   width)."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:20
msgid "A :class:`~transformers.BatchFeature` with the following fields:"
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:22
msgid ""
"**pixel_values** -- Pixel values to be fed to a model, of shape "
"(batch_size, num_channels, height, width)."
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__
#: transformers.BeitForImageClassification.forward
#: transformers.BeitForMaskedImageModeling.forward
#: transformers.BeitModel.forward
msgid "Return type"
msgstr ""

#: of transformers.BeitFeatureExtractor.__call__:24
msgid ":class:`~transformers.BatchFeature`"
msgstr ""

#: ../../source/model_doc/beit.rst:80
msgid "BeitModel"
msgstr ""

#: of transformers.BeitModel:1
msgid ""
"The bare Beit Model transformer outputting raw hidden-states without any "
"specific head on top. This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use"
" it as a regular PyTorch Module and refer to the PyTorch documentation "
"for all matter related to general usage and behavior."
msgstr ""

#: of transformers.BeitForImageClassification:8
#: transformers.BeitForMaskedImageModeling:6 transformers.BeitModel:6
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.BeitModel.forward:1
msgid ""
"The :class:`~transformers.BeitModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.BeitForImageClassification.forward:4
#: transformers.BeitForMaskedImageModeling.forward:4
#: transformers.BeitModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.BeitForImageClassification.forward:8
#: transformers.BeitForMaskedImageModeling.forward:8
#: transformers.BeitModel.forward:8
msgid ""
"Pixel values. Pixel values can be obtained using "
":class:`~transformers.BeitFeatureExtractor`. See "
":meth:`transformers.BeitFeatureExtractor.__call__` for details."
msgstr ""

#: of transformers.BeitForImageClassification.forward:11
#: transformers.BeitForMaskedImageModeling.forward:11
#: transformers.BeitModel.forward:11
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.BeitForImageClassification.forward:11
#: transformers.BeitForMaskedImageModeling.forward:11
#: transformers.BeitModel.forward:11
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``:"
msgstr ""

#: of transformers.BeitForImageClassification.forward:13
#: transformers.BeitForMaskedImageModeling.forward:13
#: transformers.BeitModel.forward:13
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of transformers.BeitForImageClassification.forward:14
#: transformers.BeitForMaskedImageModeling.forward:14
#: transformers.BeitModel.forward:14
msgid "0 indicates the head is **masked**."
msgstr ""

#: of transformers.BeitForImageClassification.forward:16
#: transformers.BeitForMaskedImageModeling.forward:16
#: transformers.BeitModel.forward:16
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.BeitForImageClassification.forward:19
#: transformers.BeitForMaskedImageModeling.forward:19
#: transformers.BeitModel.forward:19
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.BeitForImageClassification.forward:22
#: transformers.BeitForMaskedImageModeling.forward:22
#: transformers.BeitModel.forward:22
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.BeitModel.forward:25
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.BeitConfig`) and inputs.  - "
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **pooler_output** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`) -- "
"Last layer hidden-state of the first token of the sequence "
"(classification token) further processed by a   Linear layer and a Tanh "
"activation function. The Linear layer weights are trained from the next "
"sentence   prediction (classification) objective during pretraining. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"BeitFeatureExtractor, BeitModel     >>> from PIL import Image     >>> "
"import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224"
"-pt22k-ft22k')     >>> model = BeitModel.from_pretrained('microsoft/beit-"
"base-patch16-224-pt22k-ft22k')      >>> inputs = "
"feature_extractor(images=image, return_tensors=\"pt\")     >>> outputs = "
"model(**inputs)     >>> last_hidden_states = outputs.last_hidden_state"
msgstr ""

#: of transformers.BeitModel.forward:25
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.BeitConfig`) and inputs."
msgstr ""

#: of transformers.BeitModel.forward:29
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.BeitModel.forward:30
msgid ""
"**pooler_output** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"hidden_size)`) -- Last layer hidden-state of the first token of the "
"sequence (classification token) further processed by a Linear layer and a"
" Tanh activation function. The Linear layer weights are trained from the "
"next sentence prediction (classification) objective during pretraining."
msgstr ""

#: of transformers.BeitForImageClassification.forward:35
#: transformers.BeitForMaskedImageModeling.forward:35
#: transformers.BeitModel.forward:33
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.BeitForImageClassification.forward:38
#: transformers.BeitForMaskedImageModeling.forward:38
#: transformers.BeitModel.forward:36
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.BeitForImageClassification.forward:39
#: transformers.BeitForMaskedImageModeling.forward:39
#: transformers.BeitModel.forward:37
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.BeitForImageClassification.forward:42
#: transformers.BeitForMaskedImageModeling.forward:42
#: transformers.BeitModel.forward:40
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.BeitForImageClassification.forward:46
#: transformers.BeitForMaskedImageModeling.forward:46
#: transformers.BeitModel.forward:44
msgid "Examples::"
msgstr ""

#: of transformers.BeitModel.forward:59
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/beit.rst:87
msgid "BeitForMaskedImageModeling"
msgstr ""

#: of transformers.BeitForMaskedImageModeling:1
msgid ""
"Beit Model transformer with a 'language' modeling head on top (to predict"
" visual tokens). This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use"
" it as a regular PyTorch Module and refer to the PyTorch documentation "
"for all matter related to general usage and behavior."
msgstr ""

#: of transformers.BeitForMaskedImageModeling.forward:1
msgid ""
"The :class:`~transformers.BeitForMaskedImageModeling` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.BeitForImageClassification.forward:24
#: transformers.BeitForMaskedImageModeling.forward:24
msgid ""
"Labels for computing the image classification/regression loss. Indices "
"should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy)."
msgstr ""

#: of transformers.BeitForMaskedImageModeling.forward:29
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.BeitConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss. - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"BeitFeatureExtractor, BeitForMaskedImageModeling     >>> from PIL import "
"Image     >>> import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-"
"patch16-224-pt22k')     >>> model = "
"BeitForMaskedImageModeling.from_pretrained('microsoft/beit-base-"
"patch16-224-pt22k')      >>> inputs = feature_extractor(images=image, "
"return_tensors=\"pt\")     >>> outputs = model(**inputs)     >>> logits ="
" outputs.logits"
msgstr ""

#: of transformers.BeitForMaskedImageModeling.forward:29
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.BeitConfig`) and inputs."
msgstr ""

#: of transformers.BeitForMaskedImageModeling.forward:33
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss."
msgstr ""

#: of transformers.BeitForMaskedImageModeling.forward:34
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.BeitForMaskedImageModeling.forward:61
msgid ""
":class:`~transformers.modeling_outputs.MaskedLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/beit.rst:94
msgid "BeitForImageClassification"
msgstr ""

#: of transformers.BeitForImageClassification:1
msgid ""
"Beit Model transformer with an image classification head on top (a linear"
" layer on top of the average of the final hidden states of the patch "
"tokens) e.g. for ImageNet."
msgstr ""

#: of transformers.BeitForImageClassification:4
msgid ""
"This model is a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use"
" it as a regular PyTorch Module and refer to the PyTorch documentation "
"for all matter related to general usage and behavior."
msgstr ""

#: of transformers.BeitForImageClassification.forward:1
msgid ""
"The :class:`~transformers.BeitForImageClassification` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.BeitForImageClassification.forward:29
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.BeitConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss. - **logits** (:obj:`torch.FloatTensor` of"
" shape :obj:`(batch_size, config.num_labels)`) -- Classification (or "
"regression if config.num_labels==1) scores (before SoftMax). - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"BeitFeatureExtractor, BeitForImageClassification     >>> from PIL import "
"Image     >>> import requests      >>> url = "
"'http://images.cocodataset.org/val2017/000000039769.jpg'     >>> image = "
"Image.open(requests.get(url, stream=True).raw)      >>> feature_extractor"
" = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-"
"patch16-224')     >>> model = "
"BeitForImageClassification.from_pretrained('microsoft/beit-base-"
"patch16-224')      >>> inputs = feature_extractor(images=image, "
"return_tensors=\"pt\")     >>> outputs = model(**inputs)     >>> logits ="
" outputs.logits     >>> # model predicts one of the 1000 ImageNet classes"
"     >>> predicted_class_idx = logits.argmax(-1).item()     >>> "
"print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
msgstr ""

#: of transformers.BeitForImageClassification.forward:29
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.BeitConfig`) and inputs."
msgstr ""

#: of transformers.BeitForImageClassification.forward:33
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.BeitForImageClassification.forward:34
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.BeitForImageClassification.forward:64
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""


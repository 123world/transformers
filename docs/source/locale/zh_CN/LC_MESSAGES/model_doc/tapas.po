# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/tapas.rst:2
msgid "TAPAS"
msgstr ""

#: ../../source/model_doc/tapas.rst:6
msgid ""
"This is a recently introduced model so the API hasn't been tested "
"extensively. There may be some bugs or slight breaking changes to fix "
"them in the future."
msgstr ""

#: ../../source/model_doc/tapas.rst:12
msgid "Overview"
msgstr ""

#: ../../source/model_doc/tapas.rst:14
msgid ""
"The TAPAS model was proposed in `TAPAS: Weakly Supervised Table Parsing "
"via Pre-training <https://www.aclweb.org/anthology/2020.acl-main.398>`__ "
"by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco "
"Piccinno and Julian Martin Eisenschlos. It's a BERT-based model "
"specifically designed (and pre-trained) for answering questions about "
"tabular data. Compared to BERT, TAPAS uses relative position embeddings "
"and has 7 token types that encode tabular structure. TAPAS is pre-trained"
" on the masked language modeling (MLM) objective on a large dataset "
"comprising millions of tables from English Wikipedia and corresponding "
"texts. For question answering, TAPAS has 2 heads on top: a cell selection"
" head and an aggregation head, for (optionally) performing aggregations "
"(such as counting or summing) among selected cells. TAPAS has been fine-"
"tuned on several datasets: `SQA <https://www.microsoft.com/en-"
"us/download/details.aspx?id=54253>`__ (Sequential Question Answering by "
"Microsoft), `WTQ <https://github.com/ppasupat/WikiTableQuestions>`__ "
"(Wiki Table Questions by Stanford University) and `WikiSQL "
"<https://github.com/salesforce/WikiSQL>`__ (by Salesforce). It achieves "
"state-of-the-art on both SQA and WTQ, while having comparable performance"
" to SOTA on WikiSQL, with a much simpler architecture."
msgstr ""

#: ../../source/model_doc/tapas.rst:27
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/tapas.rst:29
msgid ""
"*Answering natural language questions over tables is usually seen as a "
"semantic parsing task. To alleviate the collection cost of full logical "
"forms, one popular approach focuses on weak supervision consisting of "
"denotations instead of logical forms. However, training semantic parsers "
"from weak supervision poses difficulties, and in addition, the generated "
"logical forms are only used as an intermediate step prior to retrieving "
"the denotation. In this paper, we present TAPAS, an approach to question "
"answering over tables without generating logical forms. TAPAS trains from"
" weak supervision, and predicts the denotation by selecting table cells "
"and optionally applying a corresponding aggregation operator to such "
"selection. TAPAS extends BERT's architecture to encode tables as input, "
"initializes from an effective joint pre-training of text segments and "
"tables crawled from Wikipedia, and is trained end-to-end. We experiment "
"with three different semantic parsing datasets, and find that TAPAS "
"outperforms or rivals semantic parsing models by improving state-of-the-"
"art accuracy on SQA from 55.1 to 67.2 and performing on par with the "
"state-of-the-art on WIKISQL and WIKITQ, but with a simpler model "
"architecture. We additionally find that transfer learning, which is "
"trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2"
" points above the state-of-the-art.*"
msgstr ""

#: ../../source/model_doc/tapas.rst:42
msgid ""
"In addition, the authors have further pre-trained TAPAS to recognize "
"**table entailment**, by creating a balanced dataset of millions of "
"automatically created training examples which are learned in an "
"intermediate step prior to fine-tuning. The authors of TAPAS call this "
"further pre-training intermediate pre-training (since TAPAS is first pre-"
"trained on MLM, and then on another dataset). They found that "
"intermediate pre-training further improves performance on SQA, achieving "
"a new state-of-the-art as well as state-of-the-art on `TabFact "
"<https://github.com/wenhuchen/Table-Fact-Checking>`__, a large-scale "
"dataset with 16k Wikipedia tables for table entailment (a binary "
"classification task). For more details, see their follow-up paper: "
"`Understanding tables with intermediate pre-training "
"<https://www.aclweb.org/anthology/2020.findings-emnlp.27/>`__ by Julian "
"Martin Eisenschlos, Syrine Krichene and Thomas Müller."
msgstr ""

#: ../../source/model_doc/tapas.rst:52
msgid ""
"This model was contributed by `nielsr <https://huggingface.co/nielsr>`__."
" The original code can be found `here <https://github.com/google-"
"research/tapas>`__."
msgstr ""

#: ../../source/model_doc/tapas.rst:55
msgid "Tips:"
msgstr ""

#: ../../source/model_doc/tapas.rst:57
msgid ""
"TAPAS is a model that uses relative position embeddings by default "
"(restarting the position embeddings at every cell of the table). Note "
"that this is something that was added after the publication of the "
"original TAPAS paper. According to the authors, this usually results in a"
" slightly better performance, and allows you to encode longer sequences "
"without running out of embeddings. This is reflected in the "
"``reset_position_index_per_cell`` parameter of "
":class:`~transformers.TapasConfig`, which is set to ``True`` by default. "
"The default versions of the models available in the `model hub "
"<https://huggingface.co/models?search=tapas>`_ all use relative position "
"embeddings. You can still use the ones with absolute position embeddings "
"by passing in an additional argument ``revision=\"no_reset\"`` when "
"calling the ``.from_pretrained()`` method. Note that it's usually advised"
" to pad the inputs on the right rather than the left."
msgstr ""

#: ../../source/model_doc/tapas.rst:66
msgid ""
"TAPAS is based on BERT, so ``TAPAS-base`` for example corresponds to a "
"``BERT-base`` architecture. Of course, TAPAS-large will result in the "
"best performance (the results reported in the paper are from TAPAS-"
"large). Results of the various sized models are shown on the `original "
"Github repository <https://github.com/google-research/tapas>`_."
msgstr ""

#: ../../source/model_doc/tapas.rst:69
msgid ""
"TAPAS has checkpoints fine-tuned on SQA, which are capable of answering "
"questions related to a table in a conversational set-up. This means that "
"you can ask follow-up questions such as \"what is his age?\" related to "
"the previous question. Note that the forward pass of TAPAS is a bit "
"different in case of a conversational set-up: in that case, you have to "
"feed every table-question pair one by one to the model, such that the "
"`prev_labels` token type ids can be overwritten by the predicted `labels`"
" of the model to the previous question. See \"Usage\" section for more "
"info."
msgstr ""

#: ../../source/model_doc/tapas.rst:75
msgid ""
"TAPAS is similar to BERT and therefore relies on the masked language "
"modeling (MLM) objective. It is therefore efficient at predicting masked "
"tokens and at NLU in general, but is not optimal for text generation. "
"Models trained with a causal language modeling (CLM) objective are better"
" in that regard."
msgstr ""

#: ../../source/model_doc/tapas.rst:81
msgid "Usage: fine-tuning"
msgstr ""

#: ../../source/model_doc/tapas.rst:83
msgid ""
"Here we explain how you can fine-tune "
":class:`~transformers.TapasForQuestionAnswering` on your own dataset."
msgstr ""

#: ../../source/model_doc/tapas.rst:85
msgid ""
"**STEP 1: Choose one of the 3 ways in which you can use TAPAS - or "
"experiment**"
msgstr ""

#: ../../source/model_doc/tapas.rst:87
msgid ""
"Basically, there are 3 different ways in which one can fine-tune "
":class:`~transformers.TapasForQuestionAnswering`, corresponding to the "
"different datasets on which Tapas was fine-tuned:"
msgstr ""

#: ../../source/model_doc/tapas.rst:90
msgid ""
"SQA: if you're interested in asking follow-up questions related to a "
"table, in a conversational set-up. For example if you first ask \"what's "
"the name of the first actor?\" then you can ask a follow-up question such"
" as \"how old is he?\". Here, questions do not involve any aggregation "
"(all questions are cell selection questions)."
msgstr ""

#: ../../source/model_doc/tapas.rst:93
msgid ""
"WTQ: if you're not interested in asking questions in a conversational "
"set-up, but rather just asking questions related to a table, which might "
"involve aggregation, such as counting a number of rows, summing up cell "
"values or averaging cell values. You can then for example ask \"what's "
"the total number of goals Cristiano Ronaldo made in his career?\". This "
"case is also called **weak supervision**, since the model itself must "
"learn the appropriate aggregation operator (SUM/COUNT/AVERAGE/NONE) given"
" only the answer to the question as supervision."
msgstr ""

#: ../../source/model_doc/tapas.rst:98
msgid ""
"WikiSQL-supervised: this dataset is based on WikiSQL with the model being"
" given the ground truth aggregation operator during training. This is "
"also called **strong supervision**. Here, learning the appropriate "
"aggregation operator is much easier."
msgstr ""

#: ../../source/model_doc/tapas.rst:102
msgid "To summarize:"
msgstr ""

#: ../../source/model_doc/tapas.rst:105 ../../source/model_doc/tapas.rst:191
msgid "**Task**"
msgstr ""

#: ../../source/model_doc/tapas.rst:105
msgid "**Example dataset**"
msgstr ""

#: ../../source/model_doc/tapas.rst:105
msgid "**Description**"
msgstr ""

#: ../../source/model_doc/tapas.rst:107 ../../source/model_doc/tapas.rst:193
msgid "Conversational"
msgstr ""

#: ../../source/model_doc/tapas.rst:107
msgid "SQA"
msgstr ""

#: ../../source/model_doc/tapas.rst:107
msgid "Conversational, only cell selection questions"
msgstr ""

#: ../../source/model_doc/tapas.rst:109 ../../source/model_doc/tapas.rst:195
msgid "Weak supervision for aggregation"
msgstr ""

#: ../../source/model_doc/tapas.rst:109
msgid "WTQ"
msgstr ""

#: ../../source/model_doc/tapas.rst:109
msgid ""
"Questions might involve aggregation, and the model must learn this given "
"only the answer as supervision"
msgstr ""

#: ../../source/model_doc/tapas.rst:111 ../../source/model_doc/tapas.rst:198
msgid "Strong supervision for aggregation"
msgstr ""

#: ../../source/model_doc/tapas.rst:111
msgid "WikiSQL-supervised"
msgstr ""

#: ../../source/model_doc/tapas.rst:111
msgid ""
"Questions might involve aggregation, and the model must learn this given "
"the gold aggregation operator"
msgstr ""

#: ../../source/model_doc/tapas.rst:114
msgid ""
"Initializing a model with a pre-trained base and randomly initialized "
"classification heads from the model hub can be done as follows (be sure "
"to have installed the `torch-scatter dependency "
"<https://github.com/rusty1s/pytorch_scatter>`_ for your environment):"
msgstr ""

#: ../../source/model_doc/tapas.rst:134
msgid ""
"Of course, you don't necessarily have to follow one of these three ways "
"in which TAPAS was fine-tuned. You can also experiment by defining any "
"hyperparameters you want when initializing "
":class:`~transformers.TapasConfig`, and then create a "
":class:`~transformers.TapasForQuestionAnswering` based on that "
"configuration. For example, if you have a dataset that has both "
"conversational questions and questions that might involve aggregation, "
"then you can do it this way. Here's an example:"
msgstr ""

#: ../../source/model_doc/tapas.rst:149
msgid ""
"What you can also do is start from an already fine-tuned checkpoint. A "
"note here is that the already fine-tuned checkpoint on WTQ has some "
"issues due to the L2-loss which is somewhat brittle. See `here "
"<https://github.com/google-"
"research/tapas/issues/91#issuecomment-735719340>`__ for more info."
msgstr ""

#: ../../source/model_doc/tapas.rst:153
msgid ""
"For a list of all pre-trained and fine-tuned TAPAS checkpoints available "
"in the HuggingFace model hub, see `here "
"<https://huggingface.co/models?search=tapas>`__."
msgstr ""

#: ../../source/model_doc/tapas.rst:156
msgid "**STEP 2: Prepare your data in the SQA format**"
msgstr ""

#: ../../source/model_doc/tapas.rst:158
msgid ""
"Second, no matter what you picked above, you should prepare your dataset "
"in the `SQA format <https://www.microsoft.com/en-"
"us/download/details.aspx?id=54253>`__. This format is a TSV/CSV file with"
" the following columns:"
msgstr ""

#: ../../source/model_doc/tapas.rst:162
msgid "``id``: optional, id of the table-question pair, for bookkeeping purposes."
msgstr ""

#: ../../source/model_doc/tapas.rst:163
msgid ""
"``annotator``: optional, id of the person who annotated the table-"
"question pair, for bookkeeping purposes."
msgstr ""

#: ../../source/model_doc/tapas.rst:164
msgid ""
"``position``: integer indicating if the question is the first, second, "
"third,... related to the table. Only required in case of conversational "
"setup (SQA). You don't need this column in case you're going for WTQ"
"/WikiSQL-supervised."
msgstr ""

#: ../../source/model_doc/tapas.rst:166
msgid "``question``: string"
msgstr ""

#: ../../source/model_doc/tapas.rst:167
msgid "``table_file``: string, name of a csv file containing the tabular data"
msgstr ""

#: ../../source/model_doc/tapas.rst:168
msgid ""
"``answer_coordinates``: list of one or more tuples (each tuple being a "
"cell coordinate, i.e. row, column pair that is part of the answer)"
msgstr ""

#: ../../source/model_doc/tapas.rst:170
msgid ""
"``answer_text``: list of one or more strings (each string being a cell "
"value that is part of the answer)"
msgstr ""

#: ../../source/model_doc/tapas.rst:171
msgid ""
"``aggregation_label``: index of the aggregation operator. Only required "
"in case of strong supervision for aggregation (the WikiSQL-supervised "
"case)"
msgstr ""

#: ../../source/model_doc/tapas.rst:173
msgid ""
"``float_answer``: the float answer to the question, if there is one "
"(np.nan if there isn't). Only required in case of weak supervision for "
"aggregation (such as WTQ and WikiSQL)"
msgstr ""

#: ../../source/model_doc/tapas.rst:176
msgid ""
"The tables themselves should be present in a folder, each table being a "
"separate csv file. Note that the authors of the TAPAS algorithm used "
"conversion scripts with some automated logic to convert the other "
"datasets (WTQ, WikiSQL) into the SQA format. The author explains this "
"`here <https://github.com/google-"
"research/tapas/issues/50#issuecomment-705465960>`__. Interestingly, these"
" conversion scripts are not perfect (the ``answer_coordinates`` and "
"``float_answer`` fields are populated based on the ``answer_text``), "
"meaning that WTQ and WikiSQL results could actually be improved."
msgstr ""

#: ../../source/model_doc/tapas.rst:183
msgid "**STEP 3: Convert your data into PyTorch tensors using TapasTokenizer**"
msgstr ""

#: ../../source/model_doc/tapas.rst:185
msgid ""
"Third, given that you've prepared your data in this TSV/CSV format (and "
"corresponding CSV files containing the tabular data), you can then use "
":class:`~transformers.TapasTokenizer` to convert table-question pairs "
"into :obj:`input_ids`, :obj:`attention_mask`, :obj:`token_type_ids` and "
"so on. Again, based on which of the three cases you picked above, "
":class:`~transformers.TapasForQuestionAnswering` requires different "
"inputs to be fine-tuned:"
msgstr ""

#: ../../source/model_doc/tapas.rst:191
msgid "**Required inputs**"
msgstr ""

#: ../../source/model_doc/tapas.rst:193
msgid "``input_ids``, ``attention_mask``, ``token_type_ids``, ``labels``"
msgstr ""

#: ../../source/model_doc/tapas.rst:195
msgid ""
"``input_ids``, ``attention_mask``, ``token_type_ids``, ``labels``, "
"``numeric_values``, ``numeric_values_scale``, ``float_answer``"
msgstr ""

#: ../../source/model_doc/tapas.rst:198
msgid ""
"``input ids``, ``attention mask``, ``token type ids``, ``labels``, "
"``aggregation_labels``"
msgstr ""

#: ../../source/model_doc/tapas.rst:201
msgid ""
":class:`~transformers.TapasTokenizer` creates the ``labels``, "
"``numeric_values`` and ``numeric_values_scale`` based on the "
"``answer_coordinates`` and ``answer_text`` columns of the TSV file. The "
"``float_answer`` and ``aggregation_labels`` are already in the TSV file "
"of step 2. Here's an example:"
msgstr ""

#: ../../source/model_doc/tapas.rst:223
msgid ""
"Note that :class:`~transformers.TapasTokenizer` expects the data of the "
"table to be **text-only**. You can use ``.astype(str)`` on a dataframe to"
" turn it into text-only data. Of course, this only shows how to encode a "
"single training example. It is advised to create a PyTorch dataset and a "
"corresponding dataloader:"
msgstr ""

#: ../../source/model_doc/tapas.rst:264
msgid ""
"Note that here, we encode each table-question pair independently. This is"
" fine as long as your dataset is **not conversational**. In case your "
"dataset involves conversational questions (such as in SQA), then you "
"should first group together the ``queries``, ``answer_coordinates`` and "
"``answer_text`` per table (in the order of their ``position`` index) and "
"batch encode each table with its questions. This will make sure that the "
"``prev_labels`` token types (see docs of "
":class:`~transformers.TapasTokenizer`) are set correctly. See `this "
"notebook <https://github.com/NielsRogge/Transformers-"
"Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb>`__"
" for more info."
msgstr ""

#: ../../source/model_doc/tapas.rst:272
msgid "**STEP 4: Train (fine-tune) TapasForQuestionAnswering**"
msgstr ""

#: ../../source/model_doc/tapas.rst:274
msgid ""
"You can then fine-tune :class:`~transformers.TapasForQuestionAnswering` "
"using native PyTorch as follows (shown here for the weak supervision for "
"aggregation case):"
msgstr ""

#: ../../source/model_doc/tapas.rst:320
msgid "Usage: inference"
msgstr ""

#: ../../source/model_doc/tapas.rst:322
msgid ""
"Here we explain how you can use "
":class:`~transformers.TapasForQuestionAnswering` for inference (i.e. "
"making predictions on new data). For inference, only ``input_ids``, "
"``attention_mask`` and ``token_type_ids`` (which you can obtain using "
":class:`~transformers.TapasTokenizer`) have to be provided to the model "
"to obtain the logits. Next, you can use the handy "
"``convert_logits_to_predictions`` method of "
":class:`~transformers.TapasTokenizer` to convert these into predicted "
"coordinates and optional aggregation indices."
msgstr ""

#: ../../source/model_doc/tapas.rst:328
msgid ""
"However, note that inference is **different** depending on whether or not"
" the setup is conversational. In a non-conversational set-up, inference "
"can be done in parallel on all table-question pairs of a batch. Here's an"
" example of that:"
msgstr ""

#: ../../source/model_doc/tapas.rst:383
msgid ""
"In case of a conversational set-up, then each table-question pair must be"
" provided **sequentially** to the model, such that the ``prev_labels`` "
"token types can be overwritten by the predicted ``labels`` of the "
"previous table-question pair. Again, more info can be found in `this "
"notebook <https://github.com/NielsRogge/Transformers-"
"Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb>`__."
msgstr ""

#: ../../source/model_doc/tapas.rst:390
msgid "Tapas specific outputs"
msgstr ""

#: of transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput:1
msgid "Output type of :class:`~transformers.TapasForQuestionAnswering`."
msgstr ""

#: of transformers.TapasConfig transformers.TapasForMaskedLM
#: transformers.TapasForMaskedLM.forward transformers.TapasForQuestionAnswering
#: transformers.TapasForQuestionAnswering.forward
#: transformers.TapasForSequenceClassification
#: transformers.TapasForSequenceClassification.forward transformers.TapasModel
#: transformers.TapasModel.forward transformers.TapasTokenizer
#: transformers.TapasTokenizer.__call__
#: transformers.TapasTokenizer.convert_logits_to_predictions
#: transformers.TapasTokenizer.save_vocabulary
#: transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput
msgid "Parameters"
msgstr ""

#: of transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput:3
msgid ""
"Total loss as the sum of the hierarchical cell selection log-likelihood "
"loss and (optionally) the semi-supervised regression loss and "
"(optionally) supervised loss for aggregations."
msgstr ""

#: of transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput:6
msgid "Prediction scores of the cell selection head, for every token."
msgstr ""

#: of transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput:8
msgid "Prediction scores of the aggregation head, for every aggregation operator."
msgstr ""

#: of transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput:10
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`. Hidden-states of the model at the output "
"of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput:14
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`. "
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: ../../source/model_doc/tapas.rst:397
msgid "TapasConfig"
msgstr ""

#: of transformers.TapasConfig:1
msgid ""
"This is the configuration class to store the configuration of a "
":class:`~transformers.TapasModel`. It is used to instantiate a TAPAS "
"model according to the specified arguments, defining the model "
"architecture. Instantiating a configuration with the defaults will yield "
"a similar configuration to that of the TAPAS `tapas-base-finetuned-sqa` "
"architecture. Configuration objects inherit from "
":class:`~transformers.PreTrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.TapasConfig:7
msgid ""
"Hyperparameters additional to BERT are taken from run_task_main.py and "
"hparam_utils.py of the original implementation. Original implementation "
"available at https://github.com/google-research/tapas/tree/master."
msgstr ""

#: of transformers.TapasConfig:10
msgid ""
"Vocabulary size of the TAPAS model. Defines the number of different "
"tokens that can be represented by the :obj:`inputs_ids` passed when "
"calling :class:`~transformers.TapasModel`."
msgstr ""

#: of transformers.TapasConfig:13
msgid "Dimensionality of the encoder layers and the pooler layer."
msgstr ""

#: of transformers.TapasConfig:15
msgid "Number of hidden layers in the Transformer encoder."
msgstr ""

#: of transformers.TapasConfig:17
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder."
msgstr ""

#: of transformers.TapasConfig:19
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in the Transformer encoder."
msgstr ""

#: of transformers.TapasConfig:21
msgid ""
"The non-linear activation function (function or string) in the encoder "
"and pooler. If string, :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"swish\"`"
" and :obj:`\"gelu_new\"` are supported."
msgstr ""

#: of transformers.TapasConfig:24
msgid ""
"The dropout probability for all fully connected layers in the embeddings,"
" encoder, and pooler."
msgstr ""

#: of transformers.TapasConfig:26
msgid "The dropout ratio for the attention probabilities."
msgstr ""

#: of transformers.TapasConfig:28
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048)."
msgstr ""

#: of transformers.TapasConfig:31
msgid ""
"The vocabulary sizes of the :obj:`token_type_ids` passed when calling "
":class:`~transformers.TapasModel`."
msgstr ""

#: of transformers.TapasConfig:33
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices."
msgstr ""

#: of transformers.TapasConfig:35
msgid "The epsilon used by the layer normalization layers."
msgstr ""

#: of transformers.TapasConfig:37
msgid ""
"Whether to use gradient checkpointing to save memory at the expense of a "
"slower backward pass."
msgstr ""

#: of transformers.TapasConfig:39
msgid "Weight for positive labels."
msgstr ""

#: of transformers.TapasConfig:41
msgid "The number of aggregation operators to predict."
msgstr ""

#: of transformers.TapasConfig:43
msgid "Importance weight for the aggregation loss."
msgstr ""

#: of transformers.TapasConfig:45
msgid ""
"Whether to use the answer as the only supervision for aggregation "
"examples."
msgstr ""

#: of transformers.TapasConfig:47
msgid "Importance weight for the regression loss."
msgstr ""

#: of transformers.TapasConfig:49
msgid ""
"Whether to normalize the answer loss by the maximum of the predicted and "
"expected value."
msgstr ""

#: of transformers.TapasConfig:51
msgid "Delta parameter used to calculate the regression loss."
msgstr ""

#: of transformers.TapasConfig:53
msgid ""
"Value used to control (OR change) the skewness of cell logits "
"probabilities."
msgstr ""

#: of transformers.TapasConfig:55
msgid "Scales aggregation logits to control the skewness of probabilities."
msgstr ""

#: of transformers.TapasConfig:57
msgid "Whether to apply Gumbel-Softmax to cell selection."
msgstr ""

#: of transformers.TapasConfig:59
msgid "Whether to apply Gumbel-Softmax to aggregation selection."
msgstr ""

#: of transformers.TapasConfig:61
msgid ""
"Method to calculate the expected average of cells in the weak supervision"
" case. One of :obj:`\"ratio\"`, :obj:`\"first_order\"` or "
":obj:`\"second_order\"`."
msgstr ""

#: of transformers.TapasConfig:64
msgid ""
"Preference for cell selection in ambiguous cases. Only applicable in case"
" of weak supervision for aggregation (WTQ, WikiSQL). If the total mass of"
" the aggregation probabilities (excluding the \"NONE\" operator) is "
"higher than this hyperparameter, then aggregation is predicted for an "
"example."
msgstr ""

#: of transformers.TapasConfig:68
msgid "Ignore examples with answer loss larger than cutoff."
msgstr ""

#: of transformers.TapasConfig:70
msgid "Maximum number of rows."
msgstr ""

#: of transformers.TapasConfig:72
msgid "Maximum number of columns."
msgstr ""

#: of transformers.TapasConfig:74
msgid "Whether to average logits per cell."
msgstr ""

#: of transformers.TapasConfig:76
msgid "Whether to constrain the model to only select cells from a single column."
msgstr ""

#: of transformers.TapasConfig:78
msgid "Whether to allow not to select any column."
msgstr ""

#: of transformers.TapasConfig:80
msgid ""
"Whether to initialize cell selection weights to 0 so that the initial "
"probabilities are 50%."
msgstr ""

#: of transformers.TapasConfig:82
msgid ""
"Whether to restart position indexes at every cell (i.e. use relative "
"position embeddings)."
msgstr ""

#: of transformers.TapasConfig:84
msgid "Whether to disable any (strong or weak) supervision on cells."
msgstr ""

#: of transformers.TapasConfig:86
msgid ""
"The aggregation labels used to aggregate the results. For example, the "
"WTQ models have the following aggregation labels: :obj:`{0: \"NONE\", 1: "
"\"SUM\", 2: \"AVERAGE\", 3: \"COUNT\"}`"
msgstr ""

#: of transformers.TapasConfig:89
msgid ""
"If the aggregation labels are defined and one of these labels represents "
"\"No aggregation\", this should be set to its index. For example, the WTQ"
" models have the \"NONE\" aggregation label at index 0, so that value "
"should be set to 0 for these models."
msgstr ""

#: of transformers.TapasConfig:94
msgid "Example::"
msgstr ""

#: ../../source/model_doc/tapas.rst:404
msgid "TapasTokenizer"
msgstr ""

#: of transformers.TapasTokenizer:1
msgid ""
"Construct a TAPAS tokenizer. Based on WordPiece. Flattens a table and one"
" or more related sentences to be used by TAPAS models."
msgstr ""

#: of transformers.TapasTokenizer:4
msgid ""
"This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` "
"which contains most of the main methods. Users should refer to this "
"superclass for more information regarding those methods. "
":class:`~transformers.TapasTokenizer` creates several token type ids to "
"encode tabular structure. To be more precise, it adds 7 token type ids, "
"in the following order: :obj:`segment_ids`, :obj:`column_ids`, "
":obj:`row_ids`, :obj:`prev_labels`, :obj:`column_ranks`, "
":obj:`inv_column_ranks` and :obj:`numeric_relations`:"
msgstr ""

#: of transformers.TapasTokenizer:10
msgid ""
"segment_ids: indicate whether a token belongs to the question (0) or the "
"table (1). 0 for special tokens and padding."
msgstr ""

#: of transformers.TapasTokenizer:12
msgid ""
"column_ids: indicate to which column of the table a token belongs "
"(starting from 1). Is 0 for all question tokens, special tokens and "
"padding."
msgstr ""

#: of transformers.TapasTokenizer:14
msgid ""
"row_ids: indicate to which row of the table a token belongs (starting "
"from 1). Is 0 for all question tokens, special tokens and padding. Tokens"
" of column headers are also 0."
msgstr ""

#: of transformers.TapasTokenizer:16
msgid ""
"prev_labels: indicate whether a token was (part of) an answer to the "
"previous question (1) or not (0). Useful in a conversational setup (such "
"as SQA)."
msgstr ""

#: of transformers.TapasTokenizer:18
msgid ""
"column_ranks: indicate the rank of a table token relative to a column, if"
" applicable. For example, if you have a column \"number of movies\" with "
"values 87, 53 and 69, then the column ranks of these tokens are 3, 1 and "
"2 respectively. 0 for all question tokens, special tokens and padding."
msgstr ""

#: of transformers.TapasTokenizer:21
msgid ""
"inv_column_ranks: indicate the inverse rank of a table token relative to "
"a column, if applicable. For example, if you have a column \"number of "
"movies\" with values 87, 53 and 69, then the inverse column ranks of "
"these tokens are 1, 3 and 2 respectively. 0 for all question tokens, "
"special tokens and padding."
msgstr ""

#: of transformers.TapasTokenizer:24
msgid ""
"numeric_relations: indicate numeric relations between the question and "
"the tokens of the table. 0 for all question tokens, special tokens and "
"padding."
msgstr ""

#: of transformers.TapasTokenizer:27
msgid ""
":class:`~transformers.TapasTokenizer` runs end-to-end tokenization on a "
"table and associated sentences: punctuation splitting and wordpiece."
msgstr ""

#: of transformers.TapasTokenizer:30
msgid "File containing the vocabulary."
msgstr ""

#: of transformers.TapasTokenizer:32
msgid "Whether or not to lowercase the input when tokenizing."
msgstr ""

#: of transformers.TapasTokenizer:34
msgid "Whether or not to do basic tokenization before WordPiece."
msgstr ""

#: of transformers.TapasTokenizer:36
msgid ""
"Collection of tokens which will never be split during tokenization. Only "
"has an effect when :obj:`do_basic_tokenize=True`"
msgstr ""

#: of transformers.TapasTokenizer:39
msgid ""
"The unknown token. A token that is not in the vocabulary cannot be "
"converted to an ID and is set to be this token instead."
msgstr ""

#: of transformers.TapasTokenizer:42
msgid ""
"The separator token, which is used when building a sequence from multiple"
" sequences, e.g. two sequences for sequence classification or for a text "
"and a question for question answering. It is also used as the last token "
"of a sequence built with special tokens."
msgstr ""

#: of transformers.TapasTokenizer:46
msgid ""
"The token used for padding, for example when batching sequences of "
"different lengths."
msgstr ""

#: of transformers.TapasTokenizer:48
msgid ""
"The classifier token which is used when doing sequence classification "
"(classification of the whole sequence instead of per-token "
"classification). It is the first token of the sequence when built with "
"special tokens."
msgstr ""

#: of transformers.TapasTokenizer:51
msgid ""
"The token used for masking values. This is the token used when training "
"this model with masked language modeling. This is the token which the "
"model will try to predict."
msgstr ""

#: of transformers.TapasTokenizer:54
msgid ""
"The token used for empty cell values in a table. Empty cell values "
"include \"\", \"n/a\", \"nan\" and \"?\"."
msgstr ""

#: of transformers.TapasTokenizer:56
msgid ""
"Whether or not to tokenize Chinese characters. This should likely be "
"deactivated for Japanese (see this `issue "
"<https://github.com/huggingface/transformers/issues/328>`__)."
msgstr ""

#: of transformers.TapasTokenizer:59
msgid ""
"(:obj:`bool`, `optional`): Whether or not to strip all accents. If this "
"option is not specified, then it will be determined by the value for "
":obj:`lowercase` (as in the original BERT)."
msgstr ""

#: of transformers.TapasTokenizer:62
msgid ""
"If > 0: Trim cells so that the length is <= this value. Also disables "
"further cell trimming, should thus be used with :obj:`truncation` set to "
":obj:`True`."
msgstr ""

#: of transformers.TapasTokenizer:65
msgid "Max column id to extract."
msgstr ""

#: of transformers.TapasTokenizer:67
msgid "Max row id to extract."
msgstr ""

#: of transformers.TapasTokenizer:69
msgid "Whether to add empty strings instead of column names."
msgstr ""

#: of transformers.TapasTokenizer:71
msgid "Whether to recompute the answer coordinates from the answer text."
msgstr ""

#: of transformers.TapasTokenizer.__call__:1
msgid ""
"Main method to tokenize and prepare for the model one or several "
"sequence(s) related to a table."
msgstr ""

#: of transformers.TapasTokenizer.__call__:3
msgid ""
"Table containing tabular data. Note that all cell values must be text. "
"Use `.astype(str)` on a Pandas dataframe to convert it to string."
msgstr ""

#: of transformers.TapasTokenizer.__call__:6
msgid ""
"Question or batch of questions related to a table to be encoded. Note "
"that in case of a batch, all questions must refer to the **same** table."
msgstr ""

#: of transformers.TapasTokenizer.__call__:9
msgid ""
"Answer coordinates of each table-question pair in the batch. In case only"
" a single table-question pair is provided, then the answer_coordinates "
"must be a single list of one or more tuples. Each tuple must be a "
"(row_index, column_index) pair. The first data row (not the column header"
" row) has index 0. The first column has index 0. In case a batch of "
"table-question pairs is provided, then the answer_coordinates must be a "
"list of lists of tuples (each list corresponding to a single table-"
"question pair)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:16
msgid ""
"Answer text of each table-question pair in the batch. In case only a "
"single table-question pair is provided, then the answer_text must be a "
"single list of one or more strings. Each string must be the answer text "
"of a corresponding answer coordinate. In case a batch of table-question "
"pairs is provided, then the answer_coordinates must be a list of lists of"
" strings (each list corresponding to a single table-question pair)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:22
msgid ""
"Whether or not to encode the sequences with the special tokens relative "
"to their model."
msgstr ""

#: of transformers.TapasTokenizer.__call__:24
msgid ""
"Activates and controls padding. Accepts the following values:  * "
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a   single sequence if provided). * "
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the   maximum acceptable input length for the "
"model if that argument is not provided. * :obj:`False` or "
":obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with "
"sequences of   different lengths)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:24
msgid "Activates and controls padding. Accepts the following values:"
msgstr ""

#: of transformers.TapasTokenizer.__call__:26
msgid ""
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single sequence if provided)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:28
msgid ""
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the maximum acceptable input length for the model"
" if that argument is not provided."
msgstr ""

#: of transformers.TapasTokenizer.__call__:30
msgid ""
":obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can "
"output a batch with sequences of different lengths)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:33
msgid ""
"Activates and controls truncation. Accepts the following values:  * "
":obj:`True` or :obj:`'drop_rows_to_fit'`: Truncate to a maximum length "
"specified with the argument   :obj:`max_length` or to the maximum "
"acceptable input length for the model if that argument is not   provided."
" This will truncate row by row, removing rows from the table. * "
":obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., "
"can output batch with   sequence lengths greater than the model maximum "
"admissible input size)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:33
msgid "Activates and controls truncation. Accepts the following values:"
msgstr ""

#: of transformers.TapasTokenizer.__call__:35
msgid ""
":obj:`True` or :obj:`'drop_rows_to_fit'`: Truncate to a maximum length "
"specified with the argument :obj:`max_length` or to the maximum "
"acceptable input length for the model if that argument is not provided. "
"This will truncate row by row, removing rows from the table."
msgstr ""

#: of transformers.TapasTokenizer.__call__:38
msgid ""
":obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., "
"can output batch with sequence lengths greater than the model maximum "
"admissible input size)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:41
msgid ""
"Controls the maximum length to use by one of the truncation/padding "
"parameters.  If left unset or set to :obj:`None`, this will use the "
"predefined model maximum length if a maximum length is required by one of"
" the truncation/padding parameters. If the model has no specific maximum "
"input length (like XLNet) truncation/padding to a maximum length will be "
"deactivated."
msgstr ""

#: of transformers.TapasTokenizer.__call__:41
msgid ""
"Controls the maximum length to use by one of the truncation/padding "
"parameters."
msgstr ""

#: of transformers.TapasTokenizer.__call__:43
msgid ""
"If left unset or set to :obj:`None`, this will use the predefined model "
"maximum length if a maximum length is required by one of the "
"truncation/padding parameters. If the model has no specific maximum input"
" length (like XLNet) truncation/padding to a maximum length will be "
"deactivated."
msgstr ""

#: of transformers.TapasTokenizer.__call__:47
msgid ""
"Whether or not the input is already pre-tokenized (e.g., split into "
"words). If set to :obj:`True`, the tokenizer assumes the input is already"
" split into words (for instance, by splitting it on whitespace) which it "
"will tokenize. This is useful for NER or token classification."
msgstr ""

#: of transformers.TapasTokenizer.__call__:51
msgid ""
"If set will pad the sequence to a multiple of the provided value. This is"
" especially useful to enable the use of Tensor Cores on NVIDIA hardware "
"with compute capability >= 7.5 (Volta)."
msgstr ""

#: of transformers.TapasTokenizer.__call__:54
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:  * :obj:`'tf'`: Return TensorFlow "
":obj:`tf.constant` objects. * :obj:`'pt'`: Return PyTorch "
":obj:`torch.Tensor` objects. * :obj:`'np'`: Return Numpy "
":obj:`np.ndarray` objects."
msgstr ""

#: of transformers.TapasTokenizer.__call__:54
msgid ""
"If set, will return tensors instead of list of python integers. "
"Acceptable values are:"
msgstr ""

#: of transformers.TapasTokenizer.__call__:56
msgid ":obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects."
msgstr ""

#: of transformers.TapasTokenizer.__call__:57
msgid ":obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects."
msgstr ""

#: of transformers.TapasTokenizer.__call__:58
msgid ":obj:`'np'`: Return Numpy :obj:`np.ndarray` objects."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:1
msgid ""
"Converts logits of :class:`~transformers.TapasForQuestionAnswering` to "
"actual predicted answer coordinates and optional aggregation indices."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:4
msgid ""
"The original implementation, on which this function is based, can be "
"found `here <https://github.com/google-"
"research/tapas/blob/4908213eb4df7aa988573350278b44c4dbe3f71b/tapas/experiments/prediction_utils.py#L288>`__."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:7
msgid ""
"Dictionary mapping features to actual values. Should be created using "
":class:`~transformers.TapasTokenizer`."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:10
msgid "Tensor containing the logits at the token level."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:12
msgid "Tensor containing the aggregation logits."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:14
msgid ""
"Threshold to be used for cell selection. All table cells for which their "
"probability is larger than this threshold will be selected."
msgstr ""

#: of transformers.TapasForMaskedLM.forward
#: transformers.TapasForQuestionAnswering.forward
#: transformers.TapasForSequenceClassification.forward
#: transformers.TapasModel.forward
#: transformers.TapasTokenizer.convert_logits_to_predictions
#: transformers.TapasTokenizer.save_vocabulary
msgid "Returns"
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:18
msgid ""
"- predicted_answer_coordinates (``List[List[[tuple]]`` of length "
"``batch_size``): Predicted answer   coordinates as a list of lists of "
"tuples. Each element in the list contains the predicted answer   "
"coordinates of a single example in the batch, as a list of tuples. Each "
"tuple is a cell, i.e. (row index,   column index). - "
"predicted_aggregation_indices (``List[int]``of length ``batch_size``, "
"`optional`, returned when   ``logits_aggregation`` is provided): "
"Predicted aggregation operator indices of the aggregation head."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:20
msgid ""
"predicted_answer_coordinates (``List[List[[tuple]]`` of length "
"``batch_size``): Predicted answer coordinates as a list of lists of "
"tuples. Each element in the list contains the predicted answer "
"coordinates of a single example in the batch, as a list of tuples. Each "
"tuple is a cell, i.e. (row index, column index)."
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:24
msgid ""
"predicted_aggregation_indices (``List[int]``of length ``batch_size``, "
"`optional`, returned when ``logits_aggregation`` is provided): Predicted "
"aggregation operator indices of the aggregation head."
msgstr ""

#: of transformers.TapasForMaskedLM.forward
#: transformers.TapasForQuestionAnswering.forward
#: transformers.TapasForSequenceClassification.forward
#: transformers.TapasModel.forward
#: transformers.TapasTokenizer.convert_logits_to_predictions
#: transformers.TapasTokenizer.save_vocabulary
msgid "Return type"
msgstr ""

#: of transformers.TapasTokenizer.convert_logits_to_predictions:26
msgid ":obj:`tuple` comprising various elements depending on the inputs"
msgstr ""

#: of transformers.TapasTokenizer.save_vocabulary:1
msgid "Save only the vocabulary of the tokenizer (vocabulary + added tokens)."
msgstr ""

#: of transformers.TapasTokenizer.save_vocabulary:3
msgid ""
"This method won't save the configuration and special token mappings of "
"the tokenizer. Use "
":meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save "
"the whole state of the tokenizer."
msgstr ""

#: of transformers.TapasTokenizer.save_vocabulary:6
msgid "The directory in which to save the vocabulary."
msgstr ""

#: of transformers.TapasTokenizer.save_vocabulary:8
msgid "An optional prefix to add to the named of the saved files."
msgstr ""

#: of transformers.TapasTokenizer.save_vocabulary:11
msgid "Paths to the files saved."
msgstr ""

#: of transformers.TapasTokenizer.save_vocabulary:12
msgid ":obj:`Tuple(str)`"
msgstr ""

#: ../../source/model_doc/tapas.rst:411
msgid "TapasModel"
msgstr ""

#: of transformers.TapasModel:1
msgid ""
"The bare Tapas Model transformer outputting raw hidden-states without any"
" specific head on top. This model inherits from "
":class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"models (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.TapasForMaskedLM:6 transformers.TapasForQuestionAnswering:9
#: transformers.TapasForSequenceClassification:8 transformers.TapasModel:6
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TapasForMaskedLM:10
#: transformers.TapasForQuestionAnswering:13
#: transformers.TapasForSequenceClassification:12 transformers.TapasModel:10
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.TapasModel:16
msgid ""
"This class is a small change compared to "
":class:`~transformers.BertModel`, taking into account the additional "
"token type ids."
msgstr ""

#: of transformers.TapasModel:19
msgid ""
"The model can behave as an encoder (with only self-attention) as well as "
"a decoder, in which case a layer of cross-attention is added between the "
"self-attention layers, following the architecture described in `Attention"
" is all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani,"
" Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,"
" Lukasz Kaiser and Illia Polosukhin."
msgstr ""

#: of transformers.TapasModel.forward:1
msgid ""
"The :class:`~transformers.TapasModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:4
#: transformers.TapasForQuestionAnswering.forward:4
#: transformers.TapasForSequenceClassification.forward:4
#: transformers.TapasModel.forward:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:8
#: transformers.TapasForQuestionAnswering.forward:8
#: transformers.TapasForSequenceClassification.forward:8
#: transformers.TapasModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. Indices can be "
"obtained using :class:`~transformers.TapasTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:8
#: transformers.TapasForQuestionAnswering.forward:8
#: transformers.TapasForSequenceClassification.forward:8
#: transformers.TapasModel.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. Indices can be "
"obtained using :class:`~transformers.TapasTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:12
#: transformers.TapasForQuestionAnswering.forward:12
#: transformers.TapasForSequenceClassification.forward:12
#: transformers.TapasModel.forward:12
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:14
#: transformers.TapasForQuestionAnswering.forward:14
#: transformers.TapasForSequenceClassification.forward:14
#: transformers.TapasModel.forward:14
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:14
#: transformers.TapasForQuestionAnswering.forward:14
#: transformers.TapasForSequenceClassification.forward:14
#: transformers.TapasModel.forward:14
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:16
#: transformers.TapasForQuestionAnswering.forward:16
#: transformers.TapasForSequenceClassification.forward:16
#: transformers.TapasModel.forward:16
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:17
#: transformers.TapasForQuestionAnswering.forward:17
#: transformers.TapasForSequenceClassification.forward:17
#: transformers.TapasModel.forward:17
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:19
#: transformers.TapasForQuestionAnswering.forward:19
#: transformers.TapasForSequenceClassification.forward:19
#: transformers.TapasModel.forward:19
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:21
#: transformers.TapasForQuestionAnswering.forward:21
#: transformers.TapasForSequenceClassification.forward:21
#: transformers.TapasModel.forward:21
msgid ""
"Token indices that encode tabular structure. Indices can be obtained "
"using :class:`~transformers.TapasTokenizer`. See this class for more "
"info.  `What are token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:21
#: transformers.TapasForQuestionAnswering.forward:21
#: transformers.TapasForSequenceClassification.forward:21
#: transformers.TapasModel.forward:21
msgid ""
"Token indices that encode tabular structure. Indices can be obtained "
"using :class:`~transformers.TapasTokenizer`. See this class for more "
"info."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:24
#: transformers.TapasForQuestionAnswering.forward:24
#: transformers.TapasForSequenceClassification.forward:24
#: transformers.TapasModel.forward:24
msgid "`What are token type IDs? <../glossary.html#token-type-ids>`_"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:26
#: transformers.TapasForQuestionAnswering.forward:26
#: transformers.TapasForSequenceClassification.forward:26
#: transformers.TapasModel.forward:26
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. If ``reset_position_index_per_cell`` of "
":class:`~transformers.TapasConfig` is set to ``True``, relative position "
"embeddings will be used. Selected in the range ``[0, "
"config.max_position_embeddings - 1]``.  `What are position IDs? "
"<../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:26
#: transformers.TapasForQuestionAnswering.forward:26
#: transformers.TapasForSequenceClassification.forward:26
#: transformers.TapasModel.forward:26
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. If ``reset_position_index_per_cell`` of "
":class:`~transformers.TapasConfig` is set to ``True``, relative position "
"embeddings will be used. Selected in the range ``[0, "
"config.max_position_embeddings - 1]``."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:30
#: transformers.TapasForQuestionAnswering.forward:30
#: transformers.TapasForSequenceClassification.forward:30
#: transformers.TapasModel.forward:30
msgid "`What are position IDs? <../glossary.html#position-ids>`_"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:32
#: transformers.TapasForQuestionAnswering.forward:32
#: transformers.TapasForSequenceClassification.forward:32
#: transformers.TapasModel.forward:32
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" selected in ``[0, 1]``: - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:35
#: transformers.TapasForQuestionAnswering.forward:35
#: transformers.TapasForSequenceClassification.forward:35
#: transformers.TapasModel.forward:35
msgid ""
"Optionally, instead of passing :obj:`input_ids` you can choose to "
"directly pass an embedded representation. This is useful if you want more"
" control over how to convert :obj:`input_ids` indices into associated "
"vectors than the model's internal embedding lookup matrix."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:39
#: transformers.TapasForQuestionAnswering.forward:39
#: transformers.TapasForSequenceClassification.forward:39
#: transformers.TapasModel.forward:39
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:42
#: transformers.TapasForQuestionAnswering.forward:42
#: transformers.TapasForSequenceClassification.forward:42
#: transformers.TapasModel.forward:42
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:45
#: transformers.TapasForQuestionAnswering.forward:45
#: transformers.TapasForSequenceClassification.forward:45
#: transformers.TapasModel.forward:45
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.TapasModel.forward:48
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.TapasConfig`) and inputs.  -"
" **last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model. - **pooler_output** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`) -- "
"Last layer hidden-state of the first token of the sequence "
"(classification token) further processed by a   Linear layer and a Tanh "
"activation function. The Linear layer weights are trained from the next "
"sentence   prediction (classification) objective during pretraining. - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"TapasTokenizer, TapasModel     >>> import pandas as pd      >>> tokenizer"
" = TapasTokenizer.from_pretrained('google/tapas-base')     >>> model = "
"TapasModel.from_pretrained('google/tapas-base')      >>> data = "
"{'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"],"
"     ...         'Age': [\"56\", \"45\", \"59\"],     ...         'Number"
" of movies': [\"87\", \"53\", \"69\"]     ... }     >>> table = "
"pd.DataFrame.from_dict(data)     >>> queries = [\"How many movies has "
"George Clooney played in?\", \"How old is Brad Pitt?\"]      >>> inputs ="
" tokenizer(table=table, queries=queries, padding=\"max_length\", "
"return_tensors=\"pt\")     >>> outputs = model(**inputs)      >>> "
"last_hidden_states = outputs.last_hidden_state"
msgstr ""

#: of transformers.TapasModel.forward:48
msgid ""
"A :class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or a"
" tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or"
" when ``config.return_dict=False``) comprising various elements depending"
" on the configuration (:class:`~transformers.TapasConfig`) and inputs."
msgstr ""

#: of transformers.TapasModel.forward:52
msgid ""
"**last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-"
"states at the output of the last layer of the model."
msgstr ""

#: of transformers.TapasModel.forward:53
msgid ""
"**pooler_output** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"hidden_size)`) -- Last layer hidden-state of the first token of the "
"sequence (classification token) further processed by a Linear layer and a"
" Tanh activation function. The Linear layer weights are trained from the "
"next sentence prediction (classification) objective during pretraining."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:58
#: transformers.TapasForSequenceClassification.forward:59
#: transformers.TapasModel.forward:56
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:61
#: transformers.TapasForSequenceClassification.forward:62
#: transformers.TapasModel.forward:59
msgid ""
"Hidden-states of the model at the output of each layer plus the initial "
"embedding outputs."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:62
#: transformers.TapasForSequenceClassification.forward:63
#: transformers.TapasModel.forward:60
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:65
#: transformers.TapasForSequenceClassification.forward:66
#: transformers.TapasModel.forward:63
msgid ""
"Attentions weights after the attention softmax, used to compute the "
"weighted average in the self-attention heads."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:69
#: transformers.TapasForQuestionAnswering.forward:87
#: transformers.TapasForSequenceClassification.forward:70
#: transformers.TapasModel.forward:67
msgid "Examples::"
msgstr ""

#: of transformers.TapasModel.forward:86
msgid ""
":class:`~transformers.modeling_outputs.BaseModelOutputWithPooling` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/tapas.rst:418
msgid "TapasForMaskedLM"
msgstr ""

#: of transformers.TapasForMaskedLM:1
msgid ""
"Tapas Model with a `language modeling` head on top. This model inherits "
"from :class:`~transformers.PreTrainedModel`. Check the superclass "
"documentation for the generic methods the library implements for all its "
"models (such as downloading or saving, resizing the input embeddings, "
"pruning heads etc.)"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:1
msgid ""
"The :class:`~transformers.TapasForMaskedLM` forward method, overrides the"
" :func:`__call__` special method."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:47
msgid ""
"Labels for computing the masked language modeling loss. Indices should be"
" in ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) "
"Tokens with indices set to ``-100`` are ignored (masked), the loss is "
"only computed for the tokens with labels in ``[0, ..., "
"config.vocab_size]``"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:52
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.TapasConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss. - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, config.vocab_size)`) -- Prediction "
"scores of the language modeling head (scores for each vocabulary token "
"before SoftMax). - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"TapasTokenizer, TapasForMaskedLM     >>> import pandas as pd      >>> "
"tokenizer = TapasTokenizer.from_pretrained('google/tapas-base')     >>> "
"model = TapasForMaskedLM.from_pretrained('google/tapas-base')      >>> "
"data = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George "
"Clooney\"],     ...         'Age': [\"56\", \"45\", \"59\"],     ..."
"         'Number of movies': [\"87\", \"53\", \"69\"]     ... }     >>> "
"table = pd.DataFrame.from_dict(data)      >>> inputs = "
"tokenizer(table=table, queries=\"How many [MASK] has George [MASK] played"
" in?\", return_tensors=\"pt\")     >>> labels = tokenizer(table=table, "
"queries=\"How many movies has George Clooney played in?\", "
"return_tensors=\"pt\")[\"input_ids\"]      >>> outputs = model(**inputs, "
"labels=labels)     >>> last_hidden_states = outputs.last_hidden_state"
msgstr ""

#: of transformers.TapasForMaskedLM.forward:52
msgid ""
"A :class:`~transformers.modeling_outputs.MaskedLMOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.TapasConfig`) and inputs."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:56
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Masked language modeling "
"(MLM) loss."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:57
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head (scores for each vocabulary token before SoftMax)."
msgstr ""

#: of transformers.TapasForMaskedLM.forward:88
msgid ""
":class:`~transformers.modeling_outputs.MaskedLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/tapas.rst:425
msgid "TapasForSequenceClassification"
msgstr ""

#: of transformers.TapasForSequenceClassification:1
msgid ""
"Tapas Model with a sequence classification head on top (a linear layer on"
" top of the pooled output), e.g. for table entailment tasks, such as "
"TabFact (Chen et al., 2020)."
msgstr ""

#: of transformers.TapasForQuestionAnswering:5
#: transformers.TapasForSequenceClassification:4
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its models (such as downloading or saving, resizing "
"the input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:1
msgid ""
"The :class:`~transformers.TapasForSequenceClassification` forward method,"
" overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:47
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in :obj:`[0, ..., config.num_labels - 1]`. If "
":obj:`config.num_labels == 1` a regression loss is computed (Mean-Square "
"loss), If :obj:`config.num_labels > 1` a classification loss is computed "
"(Cross-Entropy). Note: this is called \"classification_class_index\" in "
"the original implementation."
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:53
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TapasConfig`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss. - **logits** (:obj:`torch.FloatTensor` of"
" shape :obj:`(batch_size, config.num_labels)`) -- Classification (or "
"regression if config.num_labels==1) scores (before SoftMax). - "
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`.    Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention   heads.   Examples::      >>> from transformers import "
"TapasTokenizer, TapasForSequenceClassification     >>> import torch     "
">>> import pandas as pd      >>> tokenizer = "
"TapasTokenizer.from_pretrained('google/tapas-base-finetuned-tabfact')"
"     >>> model = TapasForSequenceClassification.from_pretrained('google"
"/tapas-base-finetuned-tabfact')      >>> data = {'Actors': [\"Brad "
"Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"],     ...         "
"'Age': [\"56\", \"45\", \"59\"],     ...         'Number of movies': "
"[\"87\", \"53\", \"69\"]     ... }     >>> table = "
"pd.DataFrame.from_dict(data)     >>> queries = [\"There is only one actor"
" who is 45 years old\", \"There are 3 actors which played in more than 60"
" movies\"]      >>> inputs = tokenizer(table=table, queries=queries, "
"padding=\"max_length\", return_tensors=\"pt\")     >>> labels = "
"torch.tensor([1, 0]) # 1 means entailed, 0 means refuted      >>> outputs"
" = model(**inputs, labels=labels)     >>> loss = outputs.loss     >>> "
"logits = outputs.logits"
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:53
msgid ""
"A :class:`~transformers.modeling_outputs.SequenceClassifierOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.TapasConfig`) and inputs."
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:57
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Classification (or regression"
" if config.num_labels==1) loss."
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:58
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.num_labels)`) -- Classification (or regression if "
"config.num_labels==1) scores (before SoftMax)."
msgstr ""

#: of transformers.TapasForSequenceClassification.forward:92
msgid ""
":class:`~transformers.modeling_outputs.SequenceClassifierOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/tapas.rst:432
msgid "TapasForQuestionAnswering"
msgstr ""

#: of transformers.TapasForQuestionAnswering:1
msgid ""
"Tapas Model with a cell selection head and optional aggregation head on "
"top for question-answering tasks on tables (linear layers on top of the "
"hidden-states output to compute `logits` and optional "
"`logits_aggregation`), e.g. for SQA, WTQ or WikiSQL-supervised tasks."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:1
msgid ""
"The :class:`~transformers.TapasForQuestionAnswering` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:47
msgid ""
"Mask for the table. Indicates which tokens belong to the table (1). "
"Question tokens, table headers and padding are 0."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:50
msgid ""
"Labels per token for computing the hierarchical cell selection loss. This"
" encodes the positions of the answer appearing in the table. Can be "
"obtained using :class:`~transformers.TapasTokenizer`.  - 1 for tokens "
"that are **part of the answer**, - 0 for tokens that are **not part of "
"the answer**."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:50
msgid ""
"Labels per token for computing the hierarchical cell selection loss. This"
" encodes the positions of the answer appearing in the table. Can be "
"obtained using :class:`~transformers.TapasTokenizer`."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:53
msgid "1 for tokens that are **part of the answer**,"
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:54
msgid "0 for tokens that are **not part of the answer**."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:56
msgid ""
"Aggregation function index for every example in the batch for computing "
"the aggregation loss. Indices should be in :obj:`[0, ..., "
"config.num_aggregation_labels - 1]`. Only required in case of strong "
"supervision for aggregation (WikiSQL-supervised)."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:60
msgid ""
"Float answer for every example in the batch. Set to `float('nan')` for "
"cell selection questions. Only required in case of weak supervision (WTQ)"
" to calculate the aggregate mask and regression loss."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:63
msgid ""
"Numeric values of every token, NaN for tokens which are not numeric "
"values. Can be obtained using :class:`~transformers.TapasTokenizer`. Only"
" required in case of weak supervision for aggregation (WTQ) to calculate "
"the regression loss."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:67
msgid ""
"Scale of the numeric values of every token. Can be obtained using "
":class:`~transformers.TapasTokenizer`. Only required in case of weak "
"supervision for aggregation (WTQ) to calculate the regression loss."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:71
msgid ""
"A "
":class:`~transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TapasConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` (and possibly :obj:`answer`, "
":obj:`aggregation_labels`, :obj:`numeric_values` and "
":obj:`numeric_values_scale` are provided)) -- Total loss as the sum of "
"the hierarchical cell selection log-likelihood loss and (optionally) the"
"   semi-supervised regression loss and (optionally) supervised loss for "
"aggregations. - **logits** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length)`) -- Prediction scores of the cell "
"selection head, for every token. - **logits_aggregation** "
"(:obj:`torch.FloatTensor`, `optional`, of shape :obj:`(batch_size, "
"num_aggregation_labels)`) -- Prediction scores of the aggregation head, "
"for every aggregation operator. - **hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer)   of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the model at the output of   each layer "
"plus the initial embedding outputs. - **attentions** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`. Attentions weights after the "
"attention softmax, used to compute the   weighted average in the self-"
"attention heads.   Examples::      >>> from transformers import "
"TapasTokenizer, TapasForQuestionAnswering     >>> import pandas as pd"
"      >>> tokenizer = TapasTokenizer.from_pretrained('google/tapas-base-"
"finetuned-wtq')     >>> model = "
"TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-"
"wtq')      >>> data = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", "
"\"George Clooney\"],     ...         'Age': [\"56\", \"45\", \"59\"],"
"     ...         'Number of movies': [\"87\", \"53\", \"69\"]     ... }"
"     >>> table = pd.DataFrame.from_dict(data)     >>> queries = [\"How "
"many movies has George Clooney played in?\", \"How old is Brad Pitt?\"]"
"      >>> inputs = tokenizer(table=table, queries=queries, "
"padding=\"max_length\", return_tensors=\"pt\")     >>> outputs = "
"model(**inputs)      >>> logits = outputs.logits     >>> "
"logits_aggregation = outputs.logits_aggregation"
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:71
msgid ""
"A "
":class:`~transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.TapasConfig`) and "
"inputs."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:75
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` (and possibly :obj:`answer`, "
":obj:`aggregation_labels`, :obj:`numeric_values` and "
":obj:`numeric_values_scale` are provided)) -- Total loss as the sum of "
"the hierarchical cell selection log-likelihood loss and (optionally) the "
"semi-supervised regression loss and (optionally) supervised loss for "
"aggregations."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:77
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length)`) -- Prediction scores of the cell selection head, for "
"every token."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:78
msgid ""
"**logits_aggregation** (:obj:`torch.FloatTensor`, `optional`, of shape "
":obj:`(batch_size, num_aggregation_labels)`) -- Prediction scores of the "
"aggregation head, for every aggregation operator."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:79
msgid ""
"**hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings + one for "
"the output of each layer) of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`. Hidden-states of the model at the output of each layer "
"plus the initial embedding outputs."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:82
msgid ""
"**attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned "
"when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`. Attentions weights after the "
"attention softmax, used to compute the weighted average in the self-"
"attention heads."
msgstr ""

#: of transformers.TapasForQuestionAnswering.forward:107
msgid ""
":class:`~transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput`"
" or :obj:`tuple(torch.FloatTensor)`"
msgstr ""


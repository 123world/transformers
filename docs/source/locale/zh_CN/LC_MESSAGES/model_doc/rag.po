# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_doc/rag.rst:14
msgid "RAG"
msgstr ""

#: ../../source/model_doc/rag.rst:17
msgid "Overview"
msgstr ""

#: ../../source/model_doc/rag.rst:19
msgid ""
"Retrieval-augmented generation (\"RAG\") models combine the powers of "
"pretrained dense retrieval (DPR) and sequence-to-sequence models. RAG "
"models retrieve documents, pass them to a seq2seq model, then marginalize"
" to generate outputs. The retriever and seq2seq modules are initialized "
"from pretrained models, and fine-tuned jointly, allowing both retrieval "
"and generation to adapt to downstream tasks."
msgstr ""

#: ../../source/model_doc/rag.rst:24
msgid ""
"It is based on the paper `Retrieval-Augmented Generation for Knowledge-"
"Intensive NLP Tasks <https://arxiv.org/abs/2005.11401>`__ by Patrick "
"Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir "
"Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim "
"Rocktäschel, Sebastian Riedel, Douwe Kiela."
msgstr ""

#: ../../source/model_doc/rag.rst:28
msgid "The abstract from the paper is the following:"
msgstr ""

#: ../../source/model_doc/rag.rst:30
msgid ""
"*Large pre-trained language models have been shown to store factual "
"knowledge in their parameters, and achieve state-of-the-art results when "
"fine-tuned on downstream NLP tasks. However, their ability to access and "
"precisely manipulate knowledge is still limited, and hence on knowledge-"
"intensive tasks, their performance lags behind task-specific "
"architectures. Additionally, providing provenance for their decisions and"
" updating their world knowledge remain open research problems. Pre-"
"trained models with a differentiable access mechanism to explicit "
"nonparametric memory can overcome this issue, but have so far been only "
"investigated for extractive downstream tasks. We explore a general-"
"purpose fine-tuning recipe for retrieval-augmented generation (RAG) — "
"models which combine pre-trained parametric and non-parametric memory for"
" language generation. We introduce RAG models where the parametric memory"
" is a pre-trained seq2seq model and the non-parametric memory is a dense "
"vector index of Wikipedia, accessed with a pre-trained neural retriever. "
"We compare two RAG formulations, one which conditions on the same "
"retrieved passages across the whole generated sequence, the other can use"
" different passages per token. We fine-tune and evaluate our models on a "
"wide range of knowledge-intensive NLP tasks and set the state-of-the-art "
"on three open domain QA tasks, outperforming parametric seq2seq models "
"and task-specific retrieve-and-extract architectures. For language "
"generation tasks, we find that RAG models generate more specific, diverse"
" and factual language than a state-of-the-art parametric-only seq2seq "
"baseline.*"
msgstr ""

#: ../../source/model_doc/rag.rst:46
msgid "This model was contributed by `ola13 <https://huggingface.co/ola13>`__."
msgstr ""

#: ../../source/model_doc/rag.rst:50
msgid "RagConfig"
msgstr ""

#: of transformers.RagConfig:1
msgid ""
":class:`~transformers.RagConfig` stores the configuration of a "
"`RagModel`. Configuration objects inherit from "
":class:`~transformers.PretrainedConfig` and can be used to control the "
"model outputs. Read the documentation from "
":class:`~transformers.PretrainedConfig` for more information."
msgstr ""

#: of transformers.RagConfig transformers.RagModel
#: transformers.RagModel.forward transformers.RagRetriever
#: transformers.RagRetriever.postprocess_docs
#: transformers.RagRetriever.retrieve transformers.RagSequenceForGeneration
#: transformers.RagSequenceForGeneration.forward
#: transformers.RagSequenceForGeneration.generate
#: transformers.RagTokenForGeneration
#: transformers.RagTokenForGeneration.forward
#: transformers.RagTokenForGeneration.generate transformers.TFRagModel
#: transformers.TFRagModel.call transformers.TFRagSequenceForGeneration
#: transformers.TFRagSequenceForGeneration.call
#: transformers.TFRagSequenceForGeneration.generate
#: transformers.TFRagTokenForGeneration
#: transformers.TFRagTokenForGeneration.call
#: transformers.TFRagTokenForGeneration.generate
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput
msgid "Parameters"
msgstr ""

#: of transformers.RagConfig:5
msgid ""
"Separator inserted between the title and the text of the retrieved "
"document when calling :class:`~transformers.RagRetriever`."
msgstr ""

#: of transformers.RagConfig:8
msgid ""
"Separator inserted between the the text of the retrieved document and the"
" original input when calling :class:`~transformers.RagRetriever`."
msgstr ""

#: of transformers.RagConfig:11
msgid "Number of documents to retrieve."
msgstr ""

#: of transformers.RagConfig:13
msgid ""
"Max length of contextualized input returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagConfig:15
msgid ""
"Dimensionality of the document embeddings indexed by "
":class:`~transformers.RagRetriever`."
msgstr ""

#: of transformers.RagConfig:17
msgid ""
"Retrieval batch size, defined as the number of queries issues "
"concurrently to the faiss index encapsulated "
":class:`~transformers.RagRetriever`."
msgstr ""

#: of transformers.RagConfig:20
msgid ""
"A dataset identifier of the indexed dataset in HuggingFace Datasets (list"
" all available datasets and ids using :obj:`datasets.list_datasets()`)."
msgstr ""

#: of transformers.RagConfig:23
msgid "Which split of the :obj:`dataset` to load."
msgstr ""

#: of transformers.RagConfig:25
msgid ""
"The index name of the index associated with the :obj:`dataset`. One can "
"choose between :obj:`\"legacy\"`, :obj:`\"exact\"` and "
":obj:`\"compressed\"`."
msgstr ""

#: of transformers.RagConfig:28
msgid "The path to the serialized faiss index on disk."
msgstr ""

#: of transformers.RagConfig:30
msgid ""
"(:obj:`str`, `optional`): A path to text passages compatible with the "
"faiss index. Required if using "
":class:`~transformers.models.rag.retrieval_rag.LegacyIndex`"
msgstr ""

#: of transformers.RagConfig:33
msgid ""
"Whether to load a \"dummy\" variant of the dataset specified by "
":obj:`dataset`."
msgstr ""

#: of transformers.RagConfig:35
msgid ""
"Only relevant if ``return_loss`` is set to :obj:`True`. Controls the "
"``epsilon`` parameter value for label smoothing in the loss calculation. "
"If set to 0, no label smoothing is performed."
msgstr ""

#: of transformers.RagConfig:38 transformers.RagTokenForGeneration.forward:72
#: transformers.TFRagTokenForGeneration.call:72
msgid ""
"If :obj:`True`, the logits are marginalized over all documents by making "
"use of ``torch.nn.functional.log_softmax``."
msgstr ""

#: of transformers.RagConfig:41
msgid ""
"Whether or not to reduce the NLL loss using the ``torch.Tensor.sum`` "
"operation."
msgstr ""

#: of transformers.RagConfig:43
#: transformers.RagSequenceForGeneration.generate:32
#: transformers.TFRagSequenceForGeneration.generate:25
msgid ""
"Whether or not to deduplicate the generations from different context "
"documents for a given input. Has to be set to :obj:`False` if used while "
"training with distributed backend."
msgstr ""

#: of transformers.RagConfig:46
msgid "Whether or not to disregard the BOS token when computing the loss."
msgstr ""

#: of transformers.RagConfig:48
msgid ""
"If set to ``True``, :obj:`retrieved_doc_embeds`, "
":obj:`retrieved_doc_ids`, :obj:`context_input_ids` and "
":obj:`context_attention_mask` are returned. See returned tensors for more"
" detail."
msgstr ""

#: of transformers.RagConfig:51
msgid ""
"Whether or not the model should return the last key/values attentions "
"(not used by all models)."
msgstr ""

#: of transformers.RagConfig:53
msgid ""
"The id of the token to force as the last generated token when "
":obj:`max_length` is reached. Usually set to :obj:`eos_token_id`."
msgstr ""

#: of transformers.RagConfig.from_question_encoder_generator_configs:1
msgid ""
"Instantiate a :class:`~transformers.EncoderDecoderConfig` (or a derived "
"class) from a pre-trained encoder model configuration and decoder model "
"configuration."
msgstr ""

#: of transformers.RagConfig.from_question_encoder_generator_configs
#: transformers.RagConfig.to_dict transformers.RagModel.forward
#: transformers.RagRetriever.postprocess_docs
#: transformers.RagRetriever.retrieve
#: transformers.RagSequenceForGeneration.forward
#: transformers.RagSequenceForGeneration.generate
#: transformers.RagTokenForGeneration.forward
#: transformers.RagTokenForGeneration.generate transformers.TFRagModel.call
#: transformers.TFRagSequenceForGeneration.call
#: transformers.TFRagSequenceForGeneration.generate
#: transformers.TFRagTokenForGeneration.call
#: transformers.TFRagTokenForGeneration.generate
msgid "Returns"
msgstr ""

#: of transformers.RagConfig.from_question_encoder_generator_configs:4
msgid "An instance of a configuration object"
msgstr ""

#: of transformers.RagConfig.from_question_encoder_generator_configs
#: transformers.RagConfig.to_dict transformers.RagModel.forward
#: transformers.RagRetriever.postprocess_docs
#: transformers.RagRetriever.retrieve
#: transformers.RagSequenceForGeneration.forward
#: transformers.RagSequenceForGeneration.generate
#: transformers.RagTokenForGeneration.forward
#: transformers.RagTokenForGeneration.generate transformers.TFRagModel.call
#: transformers.TFRagSequenceForGeneration.call
#: transformers.TFRagSequenceForGeneration.generate
#: transformers.TFRagTokenForGeneration.call
#: transformers.TFRagTokenForGeneration.generate
msgid "Return type"
msgstr ""

#: of transformers.RagConfig.from_question_encoder_generator_configs:5
msgid ":class:`EncoderDecoderConfig`"
msgstr ""

#: of transformers.RagConfig.to_dict:1
msgid ""
"Serializes this instance to a Python dictionary. Override the default "
":meth:`~transformers.PretrainedConfig.to_dict`."
msgstr ""

#: of transformers.RagConfig.to_dict:4
msgid "Dictionary of all the attributes that make up this configuration instance,"
msgstr ""

#: of transformers.RagConfig.to_dict:5
msgid ":obj:`Dict[str, any]`"
msgstr ""

#: ../../source/model_doc/rag.rst:57
msgid "RagTokenizer"
msgstr ""

#: of transformers.RagTokenizer.as_target_tokenizer:1
msgid ""
"Temporarily sets the tokenizer for encoding the targets. Useful for "
"tokenizer associated to sequence-to-sequence models that need a slightly "
"different processing for the labels."
msgstr ""

#: ../../source/model_doc/rag.rst:64
msgid "Rag specific outputs"
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:1
msgid "Base class for retriever augmented marginalized models outputs."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:3
msgid "Language modeling loss."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:5
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:1
msgid ""
"Prediction scores of the language modeling head. The score is possibly "
"marginalized over all documents for each vocabulary token."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:25
#: transformers.RagTokenForGeneration.generate:27
#: transformers.TFRagTokenForGeneration.generate:27
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:8
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:4
msgid ""
"Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:11
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:7
msgid ""
"List of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with "
"each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`).  Contains precomputed hidden-states (key and "
"values in the attention blocks) of the decoder that can be used (see "
":obj:`past_key_values` input) to speed up sequential decoding."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:11
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:7
msgid ""
"List of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with "
"each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`)."
msgstr ""

#: of transformers.RagModel.forward:84
#: transformers.RagSequenceForGeneration.forward:93
#: transformers.RagTokenForGeneration.forward:93
#: transformers.TFRagModel.call:82
#: transformers.TFRagSequenceForGeneration.call:95
#: transformers.TFRagTokenForGeneration.call:95
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:14
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:10
msgid ""
"Contains precomputed hidden-states (key and values in the attention "
"blocks) of the decoder that can be used (see :obj:`past_key_values` "
"input) to speed up sequential decoding."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:17
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:13
msgid ""
"Embedded documents retrieved by the retriever. Is used with "
"``question_encoder_last_hidden_state`` to compute the ``doc_scores``."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:20
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:16
msgid "The indexes of the embedded documents retrieved by the retriever."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:22
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:18
msgid ""
"Input ids post-processed from the retrieved documents and the question "
"encoder input_ids by the retriever."
msgstr ""

#: of transformers.RagModel.forward:51
#: transformers.RagSequenceForGeneration.forward:51
#: transformers.RagSequenceForGeneration.generate:18
#: transformers.RagTokenForGeneration.forward:51
#: transformers.RagTokenForGeneration.generate:20
#: transformers.TFRagModel.call:49
#: transformers.TFRagSequenceForGeneration.call:49
#: transformers.TFRagTokenForGeneration.call:49
#: transformers.TFRagTokenForGeneration.generate:20
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:24
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:20
msgid ""
"Attention mask post-processed from the retrieved documents and the "
"question encoder :obj:`input_ids` by the retriever."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:27
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:23
msgid ""
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the model."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:30
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:26
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings "
"and one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden states of the question encoder at"
" the output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:30
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:43
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:54
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:26
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:39
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:50
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings "
"and one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.RagModel.forward:97
#: transformers.RagSequenceForGeneration.forward:106
#: transformers.RagTokenForGeneration.forward:106
#: transformers.TFRagModel.call:97
#: transformers.TFRagSequenceForGeneration.call:110
#: transformers.TFRagTokenForGeneration.call:110
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:33
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:29
msgid ""
"Hidden states of the question encoder at the output of each layer plus "
"the initial embedding outputs."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:35
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:31
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the question encoder, after the attention softmax, "
"used to compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:35
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:48
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:59
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:65
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:31
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:44
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:55
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:61
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.RagModel.forward:101
#: transformers.RagSequenceForGeneration.forward:110
#: transformers.RagTokenForGeneration.forward:110
#: transformers.TFRagModel.call:101
#: transformers.TFRagSequenceForGeneration.call:114
#: transformers.TFRagTokenForGeneration.call:114
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:38
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:34
msgid ""
"Attentions weights of the question encoder, after the attention softmax, "
"used to compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:41
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:37
msgid ""
"Sequence of hidden-states at the output of the last layer of the "
"generator encoder of the model."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:43
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:39
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings "
"and one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden states of the generator encoder "
"at the output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.RagModel.forward:107
#: transformers.RagSequenceForGeneration.forward:116
#: transformers.RagTokenForGeneration.forward:116
#: transformers.TFRagModel.call:107
#: transformers.TFRagSequenceForGeneration.call:120
#: transformers.TFRagTokenForGeneration.call:120
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:46
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:42
msgid ""
"Hidden states of the generator encoder at the output of each layer plus "
"the initial embedding outputs."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:48
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:44
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the generator encoder, after the attention softmax,"
" used to compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.RagModel.forward:111
#: transformers.RagSequenceForGeneration.forward:120
#: transformers.RagTokenForGeneration.forward:120
#: transformers.TFRagModel.call:111
#: transformers.TFRagSequenceForGeneration.call:124
#: transformers.TFRagTokenForGeneration.call:124
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:51
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:47
msgid ""
"Attentions weights of the generator encoder, after the attention softmax,"
" used to compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:54
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:50
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings "
"and one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.  Hidden states of the generator decoder "
"at the output of each layer plus the initial embedding outputs."
msgstr ""

#: of transformers.RagModel.forward:116
#: transformers.RagSequenceForGeneration.forward:125
#: transformers.RagTokenForGeneration.forward:125
#: transformers.TFRagModel.call:116
#: transformers.TFRagSequenceForGeneration.call:129
#: transformers.TFRagTokenForGeneration.call:129
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:57
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:53
msgid ""
"Hidden states of the generator decoder at the output of each layer plus "
"the initial embedding outputs."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:59
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:55
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  "
"Attentions weights of the generator decoder, after the attention softmax,"
" used to compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.RagModel.forward:120
#: transformers.RagSequenceForGeneration.forward:129
#: transformers.RagTokenForGeneration.forward:129
#: transformers.TFRagModel.call:120
#: transformers.TFRagSequenceForGeneration.call:133
#: transformers.TFRagTokenForGeneration.call:133
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:62
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:58
msgid ""
"Attentions weights of the generator decoder, after the attention softmax,"
" used to compute the weighted average in the self-attention heads."
msgstr ""

#: of transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:65
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:61
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape "
":obj:`(batch_size, num_heads, sequence_length, sequence_length)`.  Cross-"
"attentions weights of the generator decoder, after the attention softmax,"
" used to compute the weighted average in the cross-attention heads."
msgstr ""

#: of transformers.RagModel.forward:125
#: transformers.RagSequenceForGeneration.forward:134
#: transformers.RagTokenForGeneration.forward:134
#: transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput:68
#: transformers.models.rag.modeling_rag.RetrievAugLMOutput:64
msgid ""
"Cross-attentions weights of the generator decoder, after the attention "
"softmax, used to compute the weighted average in the cross-attention "
"heads."
msgstr ""

#: ../../source/model_doc/rag.rst:73
msgid "RagRetriever"
msgstr ""

#: of transformers.RagRetriever:1
msgid ""
"Retriever used to get documents from vector queries. It retrieves the "
"documents embeddings as well as the documents contents, and it formats "
"them to be used with a RagModel."
msgstr ""

#: of transformers.RagRetriever:4
msgid ""
"The configuration of the RAG model this Retriever is used with. Contains "
"parameters indicating which ``Index`` to build. You can load your own "
"custom dataset with ``config.index_name=\"custom\"`` or use a canonical "
"one (default) from the datasets library with "
"``config.index_name=\"wiki_dpr\"`` for example."
msgstr ""

#: of transformers.RagRetriever:8
msgid ""
"The tokenizer that was used to tokenize the question. It is used to "
"decode the question and then use the generator_tokenizer."
msgstr ""

#: of transformers.RagRetriever:11
msgid "The tokenizer used for the generator part of the RagModel."
msgstr ""

#: of transformers.RagRetriever:13
msgid ""
"If specified, use this index instead of the one built using the "
"configuration"
msgstr ""

#: of transformers.RagRetriever:16
msgid "Examples::"
msgstr ""

#: of transformers.RagRetriever.init_retrieval:1
msgid "Retriever initialization function. It loads the index into memory."
msgstr ""

#: of transformers.RagRetriever.postprocess_docs:1
msgid ""
"Postprocessing retrieved ``docs`` and combining them with "
"``input_strings``."
msgstr ""

#: of transformers.RagRetriever.postprocess_docs:3
msgid "Retrieved documents."
msgstr ""

#: of transformers.RagRetriever.postprocess_docs:5
msgid "Input strings decoded by ``preprocess_query``."
msgstr ""

#: of transformers.RagRetriever.postprocess_docs:7
msgid ""
"Prefix added at the beginning of each input, typically used with T5-based"
" models."
msgstr ""

#: of transformers.RagRetriever.postprocess_docs:10
msgid ""
"a tuple consisting of two elements: contextualized ``input_ids`` and a "
"compatible ``attention_mask``."
msgstr ""

#: of transformers.RagRetriever.postprocess_docs:12
msgid ":obj:`tuple(tensors)`"
msgstr ""

#: of transformers.RagRetriever.retrieve:1
msgid "Retrieves documents for specified ``question_hidden_states``."
msgstr ""

#: of transformers.RagRetriever.retrieve:3
msgid "A batch of query vectors to retrieve with."
msgstr ""

#: of transformers.RagRetriever.retrieve:5
msgid "The number of docs retrieved per query."
msgstr ""

#: of transformers.RagRetriever.retrieve:8
msgid ""
"A tuple with the following objects:  - **retrieved_doc_embeds** "
"(:obj:`np.ndarray` of shape :obj:`(batch_size, n_docs, dim)`) -- The "
"retrieval   embeddings of the retrieved docs per query. - **doc_ids** "
"(:obj:`np.ndarray` of shape :obj:`(batch_size, n_docs)`) -- The ids of "
"the documents in the   index - **doc_dicts** (:obj:`List[dict]`): The "
":obj:`retrieved_doc_embeds` examples per query."
msgstr ""

#: of transformers.RagRetriever.retrieve:8
msgid "A tuple with the following objects:"
msgstr ""

#: of transformers.RagRetriever.retrieve:10
msgid ""
"**retrieved_doc_embeds** (:obj:`np.ndarray` of shape :obj:`(batch_size, "
"n_docs, dim)`) -- The retrieval embeddings of the retrieved docs per "
"query."
msgstr ""

#: of transformers.RagRetriever.retrieve:12
msgid ""
"**doc_ids** (:obj:`np.ndarray` of shape :obj:`(batch_size, n_docs)`) -- "
"The ids of the documents in the index"
msgstr ""

#: of transformers.RagRetriever.retrieve:14
msgid ""
"**doc_dicts** (:obj:`List[dict]`): The :obj:`retrieved_doc_embeds` "
"examples per query."
msgstr ""

#: of transformers.RagRetriever.retrieve:15
msgid ":obj:`Tuple[np.ndarray, np.ndarray, List[dict]]`"
msgstr ""

#: ../../source/model_doc/rag.rst:80
msgid "RagModel"
msgstr ""

#: of transformers.RagModel:1 transformers.RagModel.forward:1
msgid ""
"The :class:`~transformers.RagModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.RagModel:4 transformers.RagModel.forward:4
#: transformers.RagSequenceForGeneration:4
#: transformers.RagSequenceForGeneration.forward:4
#: transformers.RagTokenForGeneration:4
#: transformers.RagTokenForGeneration.forward:4 transformers.TFRagModel:4
#: transformers.TFRagModel.call:4 transformers.TFRagSequenceForGeneration:4
#: transformers.TFRagSequenceForGeneration.call:4
#: transformers.TFRagTokenForGeneration:4
#: transformers.TFRagTokenForGeneration.call:4
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the pre and post "
"processing steps while the latter silently ignores them."
msgstr ""

#: of transformers.RagModel:9 transformers.RagSequenceForGeneration:11
#: transformers.RagTokenForGeneration:11
msgid ""
"RAG is a seq2seq model which encapsulates two core components: a question"
" encoder and a generator. During a forward pass, we encode the input with"
" the question encoder and pass it to the retriever to extract relevant "
"context documents. The documents are then prepended to the input. Such "
"contextualized inputs is passed to the generator."
msgstr ""

#: of transformers.RagModel:13 transformers.RagSequenceForGeneration:15
#: transformers.RagTokenForGeneration:15
msgid ""
"The question encoder can be any `autoencoding` model, preferably "
":class:`~transformers.DPRQuestionEncoder`, and the generator can be any "
"`seq2seq` model, preferably "
":class:`~transformers.BartForConditionalGeneration`."
msgstr ""

#: of transformers.RagModel:16 transformers.RagSequenceForGeneration:18
#: transformers.RagTokenForGeneration:18
msgid ""
"The model can be initialized with a :class:`~transformers.RagRetriever` "
"for end-to-end generation or used in combination with the outputs of a "
"retriever in multiple steps---see examples for more details. The model is"
" compatible any `autoencoding` model as the ``question_encoder`` and any "
"`seq2seq` model with language model head as the ``generator``. It has "
"been tested with :class:`~transformers.DPRQuestionEncoder` as the "
"``question_encoder`` and "
":class:`~transformers.BartForConditionalGeneration` or "
":class:`~transformers.T5ForConditionalGeneration` as the ``generator``."
msgstr ""

#: of transformers.RagModel:23 transformers.RagSequenceForGeneration:25
#: transformers.RagTokenForGeneration:25
msgid ""
"This model inherits from :class:`~transformers.PreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.RagModel:27 transformers.RagSequenceForGeneration:29
#: transformers.RagTokenForGeneration:29
msgid ""
"This model is also a PyTorch `torch.nn.Module "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass. "
"Use it as a regular PyTorch Module and refer to the PyTorch documentation"
" for all matter related to general usage and behavior."
msgstr ""

#: of transformers.RagModel:32 transformers.RagSequenceForGeneration:34
#: transformers.RagTokenForGeneration:34
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.PreTrainedModel.from_pretrained` method to load the "
"model weights."
msgstr ""

#: of transformers.RagModel:36 transformers.RagSequenceForGeneration:38
#: transformers.RagTokenForGeneration:38 transformers.TFRagModel:38
#: transformers.TFRagSequenceForGeneration:40
#: transformers.TFRagTokenForGeneration:40
msgid ""
"An encoder model compatible with the faiss index encapsulated by the "
"``retriever``."
msgstr ""

#: of transformers.RagModel:38 transformers.RagSequenceForGeneration:40
#: transformers.RagTokenForGeneration:40 transformers.TFRagModel:40
#: transformers.TFRagSequenceForGeneration:42
#: transformers.TFRagTokenForGeneration:42
msgid "A seq2seq model used as the generator in the RAG architecture."
msgstr ""

#: of transformers.RagModel:40 transformers.RagSequenceForGeneration:42
#: transformers.RagTokenForGeneration:42 transformers.TFRagModel:42
#: transformers.TFRagSequenceForGeneration:44
#: transformers.TFRagTokenForGeneration:44
msgid ""
"A retriever class encapsulating a faiss index queried to obtain context "
"documents for current inputs."
msgstr ""

#: of transformers.RagModel.forward:8
#: transformers.RagSequenceForGeneration.forward:8
#: transformers.RagTokenForGeneration.forward:8
msgid ""
"Indices of input sequence tokens in the vocabulary. "
":class:`~transformers.RagConfig`, used to initialize the model, specifies"
" which generator to use, it also specifies a compatible generator "
"tokenizer. Use that tokenizer class to obtain the indices.  `What are "
"input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.RagModel.forward:8
#: transformers.RagSequenceForGeneration.forward:8
#: transformers.RagTokenForGeneration.forward:8 transformers.TFRagModel.call:8
#: transformers.TFRagSequenceForGeneration.call:8
#: transformers.TFRagTokenForGeneration.call:8
msgid ""
"Indices of input sequence tokens in the vocabulary. "
":class:`~transformers.RagConfig`, used to initialize the model, specifies"
" which generator to use, it also specifies a compatible generator "
"tokenizer. Use that tokenizer class to obtain the indices."
msgstr ""

#: of transformers.RagModel.forward:12
#: transformers.RagSequenceForGeneration.forward:12
#: transformers.RagTokenForGeneration.forward:12
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.RagModel.forward:14
#: transformers.RagSequenceForGeneration.forward:14
#: transformers.RagSequenceForGeneration.generate:8
#: transformers.RagTokenForGeneration.forward:14
#: transformers.RagTokenForGeneration.generate:6
#: transformers.TFRagModel.call:12
#: transformers.TFRagSequenceForGeneration.call:12
#: transformers.TFRagTokenForGeneration.call:12
#: transformers.TFRagTokenForGeneration.generate:6
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.RagModel.forward:14
#: transformers.RagSequenceForGeneration.forward:14
#: transformers.RagSequenceForGeneration.generate:8
#: transformers.RagTokenForGeneration.forward:14
#: transformers.RagTokenForGeneration.generate:6
#: transformers.TFRagModel.call:12
#: transformers.TFRagSequenceForGeneration.call:12
#: transformers.TFRagTokenForGeneration.call:12
#: transformers.TFRagTokenForGeneration.generate:6
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``:"
msgstr ""

#: of transformers.RagModel.forward:16
#: transformers.RagSequenceForGeneration.forward:16
#: transformers.RagSequenceForGeneration.generate:10
#: transformers.RagTokenForGeneration.forward:16
#: transformers.RagTokenForGeneration.generate:8
#: transformers.TFRagModel.call:14
#: transformers.TFRagSequenceForGeneration.call:14
#: transformers.TFRagTokenForGeneration.call:14
#: transformers.TFRagTokenForGeneration.generate:8
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of transformers.RagModel.forward:17
#: transformers.RagSequenceForGeneration.forward:17
#: transformers.RagSequenceForGeneration.generate:11
#: transformers.RagTokenForGeneration.forward:17
#: transformers.RagTokenForGeneration.generate:9
#: transformers.TFRagModel.call:15
#: transformers.TFRagSequenceForGeneration.call:15
#: transformers.TFRagTokenForGeneration.call:15
#: transformers.TFRagTokenForGeneration.generate:9
msgid "0 for tokens that are **masked**."
msgstr ""

#: of transformers.RagModel.forward:19
#: transformers.RagSequenceForGeneration.forward:19
#: transformers.RagSequenceForGeneration.generate:13
#: transformers.RagTokenForGeneration.forward:19
#: transformers.RagTokenForGeneration.generate:11
#: transformers.TFRagModel.call:17
#: transformers.TFRagSequenceForGeneration.call:17
#: transformers.TFRagTokenForGeneration.call:17
#: transformers.TFRagTokenForGeneration.generate:11
msgid "`What are attention masks? <../glossary.html#attention-mask>`__"
msgstr ""

#: of transformers.RagModel.forward:21
#: transformers.RagSequenceForGeneration.forward:21
#: transformers.RagTokenForGeneration.forward:21
msgid ""
"Tuple consists of (:obj:`generator_enc_last_hidden_state`, `optional`: "
":obj:`generator_enc_hidden_states`, `optional`: "
":obj:`generator_enc_attentions`). :obj:`generator_enc_last_hidden_state` "
"of shape :obj:`(batch_size, n_docs * sequence_length, hidden_size)` is a "
"sequence of hidden-states at the output of the last layer of the "
"generator's encoder.  Used by the (:class:`~transformers.RagModel`) model"
" during decoding."
msgstr ""

#: of transformers.RagModel.forward:21
#: transformers.RagSequenceForGeneration.forward:21
#: transformers.RagTokenForGeneration.forward:21
#: transformers.TFRagModel.call:19
#: transformers.TFRagSequenceForGeneration.call:19
#: transformers.TFRagTokenForGeneration.call:19
msgid ""
"Tuple consists of (:obj:`generator_enc_last_hidden_state`, `optional`: "
":obj:`generator_enc_hidden_states`, `optional`: "
":obj:`generator_enc_attentions`). :obj:`generator_enc_last_hidden_state` "
"of shape :obj:`(batch_size, n_docs * sequence_length, hidden_size)` is a "
"sequence of hidden-states at the output of the last layer of the "
"generator's encoder."
msgstr ""

#: of transformers.RagModel.forward:26
#: transformers.RagSequenceForGeneration.forward:26
#: transformers.RagTokenForGeneration.forward:26
msgid "Used by the (:class:`~transformers.RagModel`) model during decoding."
msgstr ""

#: of transformers.RagModel.forward:28
#: transformers.RagSequenceForGeneration.forward:28
#: transformers.RagTokenForGeneration.forward:28
#: transformers.TFRagModel.call:26
#: transformers.TFRagSequenceForGeneration.call:26
#: transformers.TFRagTokenForGeneration.call:26
msgid ""
"Provide for generation tasks. `None` by default, construct as per "
"instructions for the generator model you're using with your RAG instance."
msgstr ""

#: of transformers.RagModel.forward:31
#: transformers.RagSequenceForGeneration.forward:31
#: transformers.RagTokenForGeneration.forward:31
#: transformers.TFRagModel.call:29
#: transformers.TFRagSequenceForGeneration.call:29
#: transformers.TFRagTokenForGeneration.call:29
msgid ""
"Default behavior: generate a tensor that ignores pad tokens in "
":obj:`decoder_input_ids`. Causal mask will also be used by default."
msgstr ""

#: of transformers.RagModel.forward:34
#: transformers.RagSequenceForGeneration.forward:34
#: transformers.RagTokenForGeneration.forward:34
#: transformers.TFRagModel.call:32
#: transformers.TFRagSequenceForGeneration.call:32
#: transformers.TFRagTokenForGeneration.call:32
msgid ""
"Tuple consists of two elements: :obj:`encoder_outputs` of the RAG model "
"(see :obj:`encoder_outputs`) and :obj:`past_key_values` of the underlying"
" generator. Can be used to speed up decoding. :obj:`past_key_values` are "
"used in the (:class:`~transformers.RagTokenForGeneration`) model during "
"decoding."
msgstr ""

#: of transformers.RagModel.forward:39
#: transformers.RagSequenceForGeneration.forward:39
#: transformers.RagTokenForGeneration.forward:39
#: transformers.TFRagModel.call:37
#: transformers.TFRagSequenceForGeneration.call:37
#: transformers.TFRagTokenForGeneration.call:37
msgid ""
"Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`. If the model has is not "
"initialized with a ``retriever`` :obj:`doc_scores` has to be provided to "
"the forward pass. :obj:`doc_scores` can be computed via "
":obj:`question_encoder_last_hidden_state` and "
":obj:`retrieved_doc_embeds`, see examples for more information."
msgstr ""

#: of transformers.RagModel.forward:45
#: transformers.RagSequenceForGeneration.forward:45
#: transformers.RagTokenForGeneration.forward:45
#: transformers.TFRagModel.call:43
#: transformers.TFRagSequenceForGeneration.call:43
#: transformers.TFRagTokenForGeneration.call:43
msgid ""
"Input IDs post-processed from the retrieved documents and the question "
"encoder :obj:`input_ids` by the retriever.  If the model has is not "
"initialized with a ``retriever`` :obj:`context_input_ids` has to be "
"provided to the forward pass. :obj:`context_input_ids` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagModel.forward:45
#: transformers.RagSequenceForGeneration.forward:45
#: transformers.RagTokenForGeneration.forward:45
#: transformers.RagTokenForGeneration.generate:13
#: transformers.TFRagModel.call:43
#: transformers.TFRagSequenceForGeneration.call:43
#: transformers.TFRagTokenForGeneration.call:43
#: transformers.TFRagTokenForGeneration.generate:13
msgid ""
"Input IDs post-processed from the retrieved documents and the question "
"encoder :obj:`input_ids` by the retriever."
msgstr ""

#: of transformers.RagModel.forward:48
#: transformers.RagSequenceForGeneration.forward:48
#: transformers.RagTokenForGeneration.forward:48
#: transformers.TFRagModel.call:46
#: transformers.TFRagSequenceForGeneration.call:46
#: transformers.TFRagTokenForGeneration.call:46
msgid ""
"If the model has is not initialized with a ``retriever`` "
":obj:`context_input_ids` has to be provided to the forward pass. "
":obj:`context_input_ids` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagModel.forward:51
#: transformers.RagSequenceForGeneration.forward:51
#: transformers.RagTokenForGeneration.forward:51
#: transformers.TFRagModel.call:49
#: transformers.TFRagSequenceForGeneration.call:49
#: transformers.TFRagTokenForGeneration.call:49
msgid ""
"Attention mask post-processed from the retrieved documents and the "
"question encoder :obj:`input_ids` by the retriever.  If the model has is "
"not initialized with a ``retriever`` :obj:`context_attention_mask` has to"
" be provided to the forward pass. :obj:`context_attention_mask` are "
"returned by :meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagModel.forward:54
#: transformers.RagSequenceForGeneration.forward:54
#: transformers.RagTokenForGeneration.forward:54
#: transformers.TFRagModel.call:52
#: transformers.TFRagSequenceForGeneration.call:52
#: transformers.TFRagTokenForGeneration.call:52
msgid ""
"If the model has is not initialized with a ``retriever`` "
":obj:`context_attention_mask` has to be provided to the forward pass. "
":obj:`context_attention_mask` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagModel.forward:58
#: transformers.RagSequenceForGeneration.forward:58
#: transformers.RagTokenForGeneration.forward:58
#: transformers.TFRagModel.call:56
#: transformers.TFRagSequenceForGeneration.call:56
#: transformers.TFRagTokenForGeneration.call:56
msgid ""
"If set to :obj:`True`, :obj:`past_key_values` key value states are "
"returned and can be used to speed up decoding (see "
":obj:`past_key_values`)."
msgstr ""

#: of transformers.RagModel.forward:61
#: transformers.RagSequenceForGeneration.forward:61
#: transformers.RagTokenForGeneration.forward:61
#: transformers.TFRagModel.call:59
#: transformers.TFRagSequenceForGeneration.call:59
#: transformers.TFRagTokenForGeneration.call:59
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more detail."
msgstr ""

#: of transformers.RagModel.forward:64
#: transformers.RagSequenceForGeneration.forward:64
#: transformers.RagTokenForGeneration.forward:64
#: transformers.TFRagModel.call:62
#: transformers.TFRagSequenceForGeneration.call:62
#: transformers.TFRagTokenForGeneration.call:62
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more detail."
msgstr ""

#: of transformers.RagModel.forward:67
#: transformers.RagSequenceForGeneration.forward:67
#: transformers.RagTokenForGeneration.forward:67
#: transformers.TFRagModel.call:65
#: transformers.TFRagSequenceForGeneration.call:65
#: transformers.TFRagTokenForGeneration.call:65
msgid ""
"Whether or not to return the :obj:`retrieved_doc_embeds`, "
":obj:`retrieved_doc_ids`, :obj:`context_input_ids` and "
":obj:`context_attention_mask`. See returned tensors for more detail."
msgstr ""

#: of transformers.RagModel.forward:70
#: transformers.RagSequenceForGeneration.forward:70
#: transformers.RagSequenceForGeneration.generate:42
#: transformers.RagTokenForGeneration.forward:70
#: transformers.RagTokenForGeneration.generate:79
#: transformers.TFRagModel.call:70
#: transformers.TFRagSequenceForGeneration.call:70
#: transformers.TFRagSequenceForGeneration.generate:35
#: transformers.TFRagTokenForGeneration.call:70
#: transformers.TFRagTokenForGeneration.generate:69
msgid ""
"Number of documents to retrieve and/or number of documents for which to "
"generate an answer."
msgstr ""

#: of transformers.RagModel.forward:73
msgid ""
"A :class:`~transformers.models.rag.modeling_rag.RetrievAugLMOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs.  - "
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head. The score is possibly marginalized over all "
"documents for   each vocabulary token. - **doc_scores** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.n_docs)`) --"
" Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and   "
":obj:`question_encoder_last_hidden_state`. - **past_key_values** "
"(:obj:`List[torch.FloatTensor]`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- List "
"of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with each "
"tensor of shape :obj:`(2,   batch_size, num_heads, sequence_length, "
"embed_size_per_head)`).    Contains precomputed hidden-states (key and "
"values in the attention blocks) of the decoder that can be used   (see "
":obj:`past_key_values` input) to speed up sequential decoding. - "
"**retrieved_doc_embeds** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, config.n_docs, hidden_size)`, `optional`, returned "
"when `output_retrieved=True`) -- Embedded documents retrieved by the "
"retriever. Is used with ``question_encoder_last_hidden_state`` to   "
"compute the ``doc_scores``. - **retrieved_doc_ids** "
"(:obj:`torch.LongTensor` of shape :obj:`(batch_size, config.n_docs)`, "
"`optional`, returned when `output_retrieved=True`) -- The indexes of the "
"embedded documents retrieved by the retriever. - **context_input_ids** "
"(:obj:`torch.LongTensor` of shape :obj:`(batch_size * config.n_docs, "
"config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever. - "
"**context_attention_mask** (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the   retriever. - "
"**question_encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape"
" :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the   model. - **question_enc_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the question encoder "
"at the output of each layer plus the initial embedding outputs. - "
"**question_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the question"
" encoder, after the attention softmax, used to compute the weighted   "
"average in the self-attention heads. - "
"**generator_enc_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the "
"generator encoder of the model. - **generator_enc_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the generator encoder"
" at the output of each layer plus the initial embedding outputs. - "
"**generator_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"generator encoder, after the attention softmax, used to compute the "
"weighted   average in the self-attention heads. - "
"**generator_dec_hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the generator decoder"
" at the output of each layer plus the initial embedding outputs. - "
"**generator_dec_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"generator decoder, after the attention softmax, used to compute the "
"weighted   average in the self-attention heads. - "
"**generator_cross_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Cross-attentions weights of the "
"generator decoder, after the attention softmax, used to compute the   "
"weighted average in the cross-attention heads.   Example::      >>> from "
"transformers import RagTokenizer, RagRetriever, RagModel     >>> import "
"torch      >>> tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-"
"token-base\")     >>> retriever = RagRetriever.from_pretrained(\"facebook"
"/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True)     >>> "
"# initialize with RagRetriever to do everything in one forward call     "
">>> model = RagModel.from_pretrained(\"facebook/rag-token-base\", "
"retriever=retriever)      >>> inputs = tokenizer(\"How many people live "
"in Paris?\", return_tensors=\"pt\")     >>> outputs = "
"model(input_ids=inputs[\"input_ids\"])"
msgstr ""

#: of transformers.RagModel.forward:73
msgid ""
"A :class:`~transformers.models.rag.modeling_rag.RetrievAugLMOutput` or a "
"tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs."
msgstr ""

#: of transformers.RagModel.forward:77
#: transformers.RagSequenceForGeneration.forward:86
#: transformers.RagTokenForGeneration.forward:86
msgid ""
"**logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head. The score is possibly marginalized over all "
"documents for each vocabulary token."
msgstr ""

#: of transformers.RagModel.forward:79
#: transformers.RagSequenceForGeneration.forward:88
#: transformers.RagTokenForGeneration.forward:88
msgid ""
"**doc_scores** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.n_docs)`) -- Score between each retrieved document embeddings (see"
" :obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`."
msgstr ""

#: of transformers.RagModel.forward:81
#: transformers.RagSequenceForGeneration.forward:90
#: transformers.RagTokenForGeneration.forward:90
msgid ""
"**past_key_values** (:obj:`List[torch.FloatTensor]`, `optional`, returned"
" when ``use_cache=True`` is passed or when ``config.use_cache=True``) -- "
"List of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with "
"each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`)."
msgstr ""

#: of transformers.RagModel.forward:86
#: transformers.RagSequenceForGeneration.forward:95
#: transformers.RagTokenForGeneration.forward:95
msgid ""
"**retrieved_doc_embeds** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, config.n_docs, hidden_size)`, `optional`, returned "
"when `output_retrieved=True`) -- Embedded documents retrieved by the "
"retriever. Is used with ``question_encoder_last_hidden_state`` to compute"
" the ``doc_scores``."
msgstr ""

#: of transformers.RagModel.forward:88
#: transformers.RagSequenceForGeneration.forward:97
#: transformers.RagTokenForGeneration.forward:97
msgid ""
"**retrieved_doc_ids** (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size, config.n_docs)`, `optional`, returned when "
"`output_retrieved=True`) -- The indexes of the embedded documents "
"retrieved by the retriever."
msgstr ""

#: of transformers.RagModel.forward:89
#: transformers.RagSequenceForGeneration.forward:98
#: transformers.RagTokenForGeneration.forward:98
msgid ""
"**context_input_ids** (:obj:`torch.LongTensor` of shape :obj:`(batch_size"
" * config.n_docs, config.max_combined_length)`, `optional`, returned when"
" `output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever."
msgstr ""

#: of transformers.RagModel.forward:90
#: transformers.RagSequenceForGeneration.forward:99
#: transformers.RagTokenForGeneration.forward:99
msgid ""
"**context_attention_mask** (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the retriever."
msgstr ""

#: of transformers.RagModel.forward:92
#: transformers.RagSequenceForGeneration.forward:101
#: transformers.RagTokenForGeneration.forward:101
msgid ""
"**question_encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape"
" :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the model."
msgstr ""

#: of transformers.RagModel.forward:94
#: transformers.RagSequenceForGeneration.forward:103
#: transformers.RagTokenForGeneration.forward:103
msgid ""
"**question_enc_hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.RagModel.forward:98
#: transformers.RagSequenceForGeneration.forward:107
#: transformers.RagTokenForGeneration.forward:107
msgid ""
"**question_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.RagModel.forward:103
#: transformers.RagSequenceForGeneration.forward:112
#: transformers.RagTokenForGeneration.forward:112
msgid ""
"**generator_enc_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the "
"generator encoder of the model."
msgstr ""

#: of transformers.RagModel.forward:104
#: transformers.RagSequenceForGeneration.forward:113
#: transformers.RagTokenForGeneration.forward:113
msgid ""
"**generator_enc_hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.RagModel.forward:108
#: transformers.RagSequenceForGeneration.forward:117
#: transformers.RagTokenForGeneration.forward:117
msgid ""
"**generator_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.RagModel.forward:113
#: transformers.RagSequenceForGeneration.forward:122
#: transformers.RagTokenForGeneration.forward:122
msgid ""
"**generator_dec_hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.RagModel.forward:117
#: transformers.RagSequenceForGeneration.forward:126
#: transformers.RagTokenForGeneration.forward:126
msgid ""
"**generator_dec_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.RagModel.forward:122
#: transformers.RagSequenceForGeneration.forward:131
#: transformers.RagTokenForGeneration.forward:131
msgid ""
"**generator_cross_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads, "
"sequence_length, sequence_length)`."
msgstr ""

#: of transformers.RagModel.forward:129
#: transformers.RagSequenceForGeneration.forward:138
#: transformers.RagTokenForGeneration.forward:138
#: transformers.TFRagModel.call:124
#: transformers.TFRagSequenceForGeneration.call:137
#: transformers.TFRagTokenForGeneration.call:137
msgid "Example::"
msgstr ""

#: of transformers.RagModel.forward:141
msgid ""
":class:`~transformers.models.rag.modeling_rag.RetrievAugLMOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: ../../source/model_doc/rag.rst:87
msgid "RagSequenceForGeneration"
msgstr ""

#: of transformers.RagSequenceForGeneration:1
#: transformers.RagSequenceForGeneration.forward:1
msgid ""
"The :class:`~transformers.RagSequenceForGeneration` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.RagSequenceForGeneration:8
msgid ""
"A RAG-sequence model implementation. It performs RAG-sequence specific "
"marginalization in the forward pass."
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:72
#: transformers.TFRagSequenceForGeneration.call:72
msgid ""
"Only relevant if ``labels`` is passed. If :obj:`True`, the score of the "
"BOS token is disregarded when computing the loss."
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:75
#: transformers.RagTokenForGeneration.forward:75
msgid ""
"Only relevant if ``labels`` is passed. If :obj:`True`, the NLL loss is "
"reduced using the ``torch.Tensor.sum`` operation."
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:78
#: transformers.RagTokenForGeneration.forward:78
#: transformers.TFRagSequenceForGeneration.call:82
#: transformers.TFRagTokenForGeneration.call:82
msgid ""
"Legacy dictionary, which is required so that model can use `generate()` "
"function."
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:81
msgid ""
"A :class:`~transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.RagConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Language modeling"
" loss. - **logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size,"
" sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head. The score is possibly marginalized over all "
"documents for   each vocabulary token. - **doc_scores** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.n_docs)`) --"
" Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and   "
":obj:`question_encoder_last_hidden_state`. - **past_key_values** "
"(:obj:`List[torch.FloatTensor]`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- List "
"of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with each "
"tensor of shape :obj:`(2,   batch_size, num_heads, sequence_length, "
"embed_size_per_head)`).    Contains precomputed hidden-states (key and "
"values in the attention blocks) of the decoder that can be used   (see "
":obj:`past_key_values` input) to speed up sequential decoding. - "
"**retrieved_doc_embeds** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, config.n_docs, hidden_size)`, `optional`, returned "
"when `output_retrieved=True`) -- Embedded documents retrieved by the "
"retriever. Is used with ``question_encoder_last_hidden_state`` to   "
"compute the ``doc_scores``. - **retrieved_doc_ids** "
"(:obj:`torch.LongTensor` of shape :obj:`(batch_size, config.n_docs)`, "
"`optional`, returned when `output_retrieved=True`) -- The indexes of the "
"embedded documents retrieved by the retriever. - **context_input_ids** "
"(:obj:`torch.LongTensor` of shape :obj:`(batch_size * config.n_docs, "
"config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever. - "
"**context_attention_mask** (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the   retriever. - "
"**question_encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape"
" :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the   model. - **question_enc_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the question encoder "
"at the output of each layer plus the initial embedding outputs. - "
"**question_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the question"
" encoder, after the attention softmax, used to compute the weighted   "
"average in the self-attention heads. - "
"**generator_enc_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the "
"generator encoder of the model. - **generator_enc_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the generator encoder"
" at the output of each layer plus the initial embedding outputs. - "
"**generator_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"generator encoder, after the attention softmax, used to compute the "
"weighted   average in the self-attention heads. - "
"**generator_dec_hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the generator decoder"
" at the output of each layer plus the initial embedding outputs. - "
"**generator_dec_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"generator decoder, after the attention softmax, used to compute the "
"weighted   average in the self-attention heads. - "
"**generator_cross_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Cross-attentions weights of the "
"generator decoder, after the attention softmax, used to compute the   "
"weighted average in the cross-attention heads.   Example::      >>> from "
"transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration"
"     >>> import torch      >>> tokenizer = "
"RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")     >>> "
"retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", "
"index_name=\"exact\", use_dummy_dataset=True)     >>> # initialize with "
"RagRetriever to do everything in one forward call     >>> model = "
"RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", "
"retriever=retriever)      >>> inputs = tokenizer(\"How many people live "
"in Paris?\", return_tensors=\"pt\")     >>> with "
"tokenizer.as_target_tokenizer():     ...    targets = tokenizer(\"In "
"Paris, there are 10 million people.\", return_tensors=\"pt\")     >>> "
"input_ids = inputs[\"input_ids\"]     >>> labels = targets[\"input_ids\"]"
"     >>> outputs = model(input_ids=input_ids, labels=labels)      >>> # "
"or use retriever separately     >>> model = "
"RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", "
"use_dummy_dataset=True)     >>> # 1. Encode     >>> "
"question_hidden_states = model.question_encoder(input_ids)[0]     >>> # "
"2. Retrieve     >>> docs_dict = retriever(input_ids.numpy(), "
"question_hidden_states.detach().numpy(), return_tensors=\"pt\")     >>> "
"doc_scores = torch.bmm(question_hidden_states.unsqueeze(1), "
"docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)).squeeze(1)"
"     >>> # 3. Forward to generator     >>> outputs = "
"model(context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores, decoder_input_ids=labels)"
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:81
#: transformers.RagTokenForGeneration.forward:81
msgid ""
"A :class:`~transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.RagConfig`) and "
"inputs."
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:85
#: transformers.RagTokenForGeneration.forward:85
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.RagSequenceForGeneration.forward:164
#: transformers.RagTokenForGeneration.forward:168
msgid ""
":class:`~transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput` "
"or :obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:1
msgid ""
"Implements RAG sequence \"thorough\" decoding. Read the "
":meth:`~transformers.generation_utils.GenerationMixin.generate`` "
"documentation for more information on how to set other generate input "
"parameters."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:5
#: transformers.RagTokenForGeneration.generate:3
#: transformers.TFRagSequenceForGeneration.generate:5
#: transformers.TFRagTokenForGeneration.generate:3
msgid ""
"The sequence used as a prompt for the generation. If :obj:`input_ids` is "
"not passed, then :obj:`context_input_ids` has to be provided."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:15
#: transformers.TFRagSequenceForGeneration.generate:12
msgid ""
"Input IDs post-processed from the retrieved documents and the question "
"encoder input_ids by the retriever."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:18
msgid ""
"Attention mask post-processed from the retrieved documents and the "
"question encoder :obj:`input_ids` by the retriever.  If the model is not "
"initialized with a ``retriever`` or ``input_ids`` is not given, "
":obj:`context_input_ids` and :obj:`context_attention_mask` have to be "
"provided to the forward pass. They are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:21
msgid ""
"If the model is not initialized with a ``retriever`` or ``input_ids`` is "
"not given, :obj:`context_input_ids` and :obj:`context_attention_mask` "
"have to be provided to the forward pass. They are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:25
msgid ""
"Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`.  If the model is not "
"initialized with a ``retriever`` or ``input_ids`` is not given, "
":obj:`doc_scores` has to be provided to the forward pass. "
":obj:`doc_scores` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:28
msgid ""
"If the model is not initialized with a ``retriever`` or ``input_ids`` is "
"not given, :obj:`doc_scores` has to be provided to the forward pass. "
":obj:`doc_scores` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:35
#: transformers.TFRagSequenceForGeneration.generate:28
msgid ""
"The number of independently computed returned sequences for each element "
"in the batch. Note that this is not the value we pass to the "
"``generator``'s "
"`:func:`~transformers.generation_utils.GenerationMixin.generate`` "
"function, where we set ``num_return_sequences`` to :obj:`num_beams`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:40
#: transformers.RagTokenForGeneration.generate:63
#: transformers.TFRagSequenceForGeneration.generate:33
#: transformers.TFRagTokenForGeneration.generate:60
msgid "Number of beams for beam search. 1 means no beam search."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:44
msgid ""
"Additional kwargs will be passed to "
":meth:`~transformers.generation_utils.GenerationMixin.generate`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:46
#: transformers.TFRagSequenceForGeneration.generate:39
msgid ""
"The generated sequences. The second dimension (sequence length) is either"
" equal to :obj:`max_length` or shorter if all batches finished early due "
"to the :obj:`eos_token_id`."
msgstr ""

#: of transformers.RagSequenceForGeneration.generate:49
#: transformers.RagTokenForGeneration.generate:101
msgid ""
":obj:`torch.LongTensor` of shape :obj:`(batch_size * "
"num_return_sequences, sequence_length)`"
msgstr ""

#: ../../source/model_doc/rag.rst:94
msgid "RagTokenForGeneration"
msgstr ""

#: of transformers.RagTokenForGeneration:1
#: transformers.RagTokenForGeneration.forward:1
msgid ""
"The :class:`~transformers.RagTokenForGeneration` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.RagTokenForGeneration:8
msgid ""
"A RAG-token model implementation. It performs RAG-token specific "
"marginalization in the forward pass."
msgstr ""

#: of transformers.RagTokenForGeneration.forward:81
msgid ""
"A :class:`~transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`"
" or a tuple of :obj:`torch.FloatTensor` (if ``return_dict=False`` is "
"passed or when ``config.return_dict=False``) comprising various elements "
"depending on the configuration (:class:`~transformers.RagConfig`) and "
"inputs.  - **loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, "
"`optional`, returned when :obj:`labels` is provided) -- Language modeling"
" loss. - **logits** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size,"
" sequence_length, config.vocab_size)`) -- Prediction scores of the "
"language modeling head. The score is possibly marginalized over all "
"documents for   each vocabulary token. - **doc_scores** "
"(:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.n_docs)`) --"
" Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and   "
":obj:`question_encoder_last_hidden_state`. - **past_key_values** "
"(:obj:`List[torch.FloatTensor]`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- List "
"of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`, with each "
"tensor of shape :obj:`(2,   batch_size, num_heads, sequence_length, "
"embed_size_per_head)`).    Contains precomputed hidden-states (key and "
"values in the attention blocks) of the decoder that can be used   (see "
":obj:`past_key_values` input) to speed up sequential decoding. - "
"**retrieved_doc_embeds** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, config.n_docs, hidden_size)`, `optional`, returned "
"when `output_retrieved=True`) -- Embedded documents retrieved by the "
"retriever. Is used with ``question_encoder_last_hidden_state`` to   "
"compute the ``doc_scores``. - **retrieved_doc_ids** "
"(:obj:`torch.LongTensor` of shape :obj:`(batch_size, config.n_docs)`, "
"`optional`, returned when `output_retrieved=True`) -- The indexes of the "
"embedded documents retrieved by the retriever. - **context_input_ids** "
"(:obj:`torch.LongTensor` of shape :obj:`(batch_size * config.n_docs, "
"config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever. - "
"**context_attention_mask** (:obj:`torch.LongTensor` of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the   retriever. - "
"**question_encoder_last_hidden_state** (:obj:`torch.FloatTensor` of shape"
" :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the   model. - **question_enc_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the question encoder "
"at the output of each layer plus the initial embedding outputs. - "
"**question_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`,"
" returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the question"
" encoder, after the attention softmax, used to compute the weighted   "
"average in the self-attention heads. - "
"**generator_enc_last_hidden_state** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the "
"generator encoder of the model. - **generator_enc_hidden_states** "
"(:obj:`tuple(torch.FloatTensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the generator encoder"
" at the output of each layer plus the initial embedding outputs. - "
"**generator_enc_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"generator encoder, after the attention softmax, used to compute the "
"weighted   average in the self-attention heads. - "
"**generator_dec_hidden_states** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_hidden_states=True`` is passed or when"
" ``config.output_hidden_states=True``) -- Tuple of "
":obj:`torch.FloatTensor` (one for the output of the embeddings and one "
"for the output of each   layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`.    Hidden states of the generator decoder"
" at the output of each layer plus the initial embedding outputs. - "
"**generator_dec_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Attentions weights of the "
"generator decoder, after the attention softmax, used to compute the "
"weighted   average in the self-attention heads. - "
"**generator_cross_attentions** (:obj:`tuple(torch.FloatTensor)`, "
"`optional`, returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` "
"(one for each layer) of shape :obj:`(batch_size, num_heads,   "
"sequence_length, sequence_length)`.    Cross-attentions weights of the "
"generator decoder, after the attention softmax, used to compute the   "
"weighted average in the cross-attention heads.   Example::      >>> from "
"transformers import RagTokenizer, RagRetriever, RagTokenForGeneration"
"     >>> import torch      >>> tokenizer = "
"RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")     >>> retriever"
" = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", "
"index_name=\"exact\", use_dummy_dataset=True)     >>> # initialize with "
"RagRetriever to do everything in one forward call     >>> model = "
"RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", "
"retriever=retriever)      >>> inputs = tokenizer(\"How many people live "
"in Paris?\", return_tensors=\"pt\")     >>> with "
"tokenizer.as_target_tokenizer():     ...    targets = tokenizer(\"In "
"Paris, there are 10 million people.\", return_tensors=\"pt\")     >>> "
"input_ids = inputs[\"input_ids\"]     >>> labels = targets[\"input_ids\"]"
"     >>> outputs = model(input_ids=input_ids, labels=labels)      >>> # "
"or use retriever separately     >>> model = "
"RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", "
"use_dummy_dataset=True)     >>> # 1. Encode     >>> "
"question_hidden_states = model.question_encoder(input_ids)[0]     >>> # "
"2. Retrieve     >>> docs_dict = retriever(input_ids.numpy(), "
"question_hidden_states.detach().numpy(), return_tensors=\"pt\")     >>> "
"doc_scores = torch.bmm(question_hidden_states.unsqueeze(1), "
"docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)).squeeze(1)"
"     >>> # 3. Forward to generator     >>> outputs = "
"model(context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores, decoder_input_ids=labels)      >>> # or directly "
"generate     >>> generated = "
"model.generate(context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores)     >>> generated_string = "
"tokenizer.batch_decode(generated, skip_special_tokens=True)"
msgstr ""

#: of transformers.RagTokenForGeneration.generate:1
msgid "Implements RAG token decoding."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:13
#: transformers.TFRagTokenForGeneration.generate:13
msgid ""
"Input IDs post-processed from the retrieved documents and the question "
"encoder :obj:`input_ids` by the retriever.  If the model has is not "
"initialized with a ``retriever``, :obj:`context_input_ids` has to be "
"provided to the forward pass. :obj:`context_input_ids` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:16
#: transformers.RagTokenForGeneration.generate:23
#: transformers.RagTokenForGeneration.generate:30
#: transformers.TFRagTokenForGeneration.generate:16
#: transformers.TFRagTokenForGeneration.generate:23
#: transformers.TFRagTokenForGeneration.generate:30
msgid ""
"If the model has is not initialized with a ``retriever``, "
":obj:`context_input_ids` has to be provided to the forward pass. "
":obj:`context_input_ids` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:20
#: transformers.TFRagTokenForGeneration.generate:20
msgid ""
"Attention mask post-processed from the retrieved documents and the "
"question encoder :obj:`input_ids` by the retriever.  If the model has is "
"not initialized with a ``retriever``, :obj:`context_input_ids` has to be "
"provided to the forward pass. :obj:`context_input_ids` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:27
#: transformers.TFRagTokenForGeneration.generate:27
msgid ""
"Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`.  If the model has is not "
"initialized with a ``retriever``, :obj:`context_input_ids` has to be "
"provided to the forward pass. :obj:`context_input_ids` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:34
#: transformers.TFRagTokenForGeneration.generate:34
msgid "The maximum length of the sequence to be generated."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:36
#: transformers.TFRagTokenForGeneration.generate:36
msgid "The minimum length of the sequence to be generated."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:38
#: transformers.TFRagTokenForGeneration.generate:38
msgid ""
"Whether or not to stop the beam search when at least ``num_beams`` "
"sentences are finished per batch or not."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:41
#: transformers.TFRagTokenForGeneration.generate:41
msgid ""
"(:obj:`bool`, `optional`, defaults to :obj:`True`): Whether or not the "
"model should use the past last key/values attentions (if applicable to "
"the model) to speed up decoding."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:44
#: transformers.TFRagTokenForGeneration.generate:44
msgid "The id of the `padding` token."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:46
#: transformers.TFRagTokenForGeneration.generate:46
msgid "The id of the `beginning-of-sequence` token."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:48
#: transformers.TFRagTokenForGeneration.generate:48
msgid "The id of the `end-of-sequence` token."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:50
#: transformers.TFRagTokenForGeneration.generate:50
msgid ""
"Exponential penalty to the length. 1.0 means no penalty.  Set to values <"
" 1.0 in order to encourage the model to generate shorter sequences, to a "
"value > 1.0 in order to encourage the model to produce longer sequences."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:50
#: transformers.TFRagTokenForGeneration.generate:50
msgid "Exponential penalty to the length. 1.0 means no penalty."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:52
#: transformers.TFRagTokenForGeneration.generate:52
msgid ""
"Set to values < 1.0 in order to encourage the model to generate shorter "
"sequences, to a value > 1.0 in order to encourage the model to produce "
"longer sequences."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:55
#: transformers.TFRagTokenForGeneration.generate:55
msgid "If set to int > 0, all ngrams of that size can only occur once."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:57
msgid ""
"If set to int > 0, all ngrams of that size that occur in the "
"``encoder_input_ids`` cannot occur in the ``decoder_input_ids``."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:60
#: transformers.TFRagTokenForGeneration.generate:57
msgid ""
"List of token ids that are not allowed to be generated. In order to get "
"the tokens of the words that should not appear in the generated text, use"
" :obj:`tokenizer.encode(bad_word, add_prefix_space=True)`."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:65
msgid ""
"Number of groups to divide :obj:`num_beams` into in order to ensure "
"diversity among different groups of beams. `this paper "
"<https://arxiv.org/pdf/1610.02424.pdf>`__ for more details."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:68
msgid ""
"This value is subtracted from a beam's score if it generates a token same"
" as any beam from other group at a particular time. Note that "
":obj:`diversity_penalty` is only effective if ``group beam search`` is "
"enabled."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:72
#: transformers.TFRagTokenForGeneration.generate:62
msgid ""
"The number of independently computed returned sequences for each element "
"in the batch. Note that this is not the value we pass to the "
"``generator``'s "
"`:func:`~transformers.generation_utils.GenerationMixin.generate` "
"function, where we set ``num_return_sequences`` to :obj:`num_beams`."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:77
#: transformers.TFRagTokenForGeneration.generate:67
msgid ""
"If an encoder-decoder model starts decoding with a different token than "
"`bos`, the id of that token."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:81
msgid ""
"(:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`): If "
"provided, this function constraints the beam search to allowed tokens "
"only at each step. If not provided no constraint is applied. This "
"function takes 2 arguments :obj:`inputs_ids` and the batch ID "
":obj:`batch_id`. It has to return a list with the allowed tokens for the "
"next generation step conditioned on the previously generated tokens "
":obj:`inputs_ids` and the batch ID :obj:`batch_id`. This argument is "
"useful for constrained generation conditioned on the prefix, as described"
" in `Autoregressive Entity Retrieval "
"<https://arxiv.org/abs/2010.00904>`__."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:88
msgid ""
"The id of the token to force as the first generated token after the "
":obj:`decoder_start_token_id`. Useful for multilingual models like "
":doc:`mBART <../model_doc/mbart>` where the first generated token needs "
"to be the target language token."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:92
msgid ""
"The id of the token to force as the last generated token when "
":obj:`max_length` is reached."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:94
msgid ""
"Whether to remove possible `nan` and `inf` outputs of the model to "
"prevent the generation method to crash. Note that using "
"``remove_invalid_values`` can slow down generation."
msgstr ""

#: of transformers.RagTokenForGeneration.generate:98
#: transformers.TFRagTokenForGeneration.generate:83
msgid ""
"The generated sequences. The second dimension (sequence_length) is either"
" equal to :obj:`max_length` or shorter if all batches finished early due "
"to the :obj:`eos_token_id`."
msgstr ""

#: ../../source/model_doc/rag.rst:101
msgid "TFRagModel"
msgstr ""

#: of transformers.TFRagModel:1 transformers.TFRagModel.call:1
msgid ""
"The :class:`~transformers.TFRagModel` forward method, overrides the "
":func:`__call__` special method."
msgstr ""

#: of transformers.TFRagModel:9 transformers.TFRagSequenceForGeneration:11
#: transformers.TFRagTokenForGeneration:11
msgid ""
"RAG is a sequence-to-sequence model which encapsulates two core "
"components: a question encoder and a generator. During a forward pass, we"
" encode the input with the question encoder and pass it to the retriever "
"to extract relevant context documents. The documents are then prepended "
"to the input. Such contextualized inputs is passed to the generator."
msgstr ""

#: of transformers.TFRagModel:14 transformers.TFRagSequenceForGeneration:16
#: transformers.TFRagTokenForGeneration:16
msgid ""
"The question encoder can be any `autoencoding` model, preferably "
":class:`~transformers.TFDPRQuestionEncoder`, and the generator can be any"
" `seq2seq` model, preferably "
":class:`~transformers.TFBartForConditionalGeneration`."
msgstr ""

#: of transformers.TFRagModel:17 transformers.TFRagSequenceForGeneration:19
#: transformers.TFRagTokenForGeneration:19
msgid ""
"The model can be initialized with a :class:`~transformers.RagRetriever` "
"for end-to-end generation or used in combination with the outputs of a "
"retriever in multiple steps---see examples for more details. The model is"
" compatible any `autoencoding` model as the ``question_encoder`` and any "
"`seq2seq` model with language model head as the ``generator``. It has "
"been tested with :class:`~transformers.TFDPRQuestionEncoder` as the "
"``question_encoder`` and "
":class:`~transformers.TFBartForConditionalGeneration` as the "
"``generator``."
msgstr ""

#: of transformers.TFRagModel:23 transformers.TFRagSequenceForGeneration:25
#: transformers.TFRagTokenForGeneration:25
msgid ""
"This model inherits from :class:`~transformers.TFPreTrainedModel`. Check "
"the superclass documentation for the generic methods the library "
"implements for all its model (such as downloading or saving, resizing the"
" input embeddings, pruning heads etc.)"
msgstr ""

#: of transformers.TFRagModel:27 transformers.TFRagSequenceForGeneration:29
#: transformers.TFRagTokenForGeneration:29
msgid ""
"This model is also a Tensorflow `tf.keras.Model "
"<https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. "
"Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 "
"documentation for all matter related to general usage and behavior."
msgstr ""

#: of transformers.TFRagModel:31 transformers.TFRagSequenceForGeneration:33
#: transformers.TFRagTokenForGeneration:33
msgid ""
"The model is in a developing state as it is now fully supports in eager-"
"mode only, and may not be exported in SavedModel format."
msgstr ""

#: of transformers.TFRagModel:34 transformers.TFRagSequenceForGeneration:36
#: transformers.TFRagTokenForGeneration:36
msgid ""
"Model configuration class with all the parameters of the model. "
"Initializing with a config file does not load the weights associated with"
" the model, only the configuration. Check out the "
":meth:`~transformers.TFPreTrainedModel.from_pretrained` method to load "
"the model weights."
msgstr ""

#: of transformers.TFRagModel.call:19
#: transformers.TFRagSequenceForGeneration.call:19
#: transformers.TFRagTokenForGeneration.call:19
msgid ""
"Tuple consists of (:obj:`generator_enc_last_hidden_state`, `optional`: "
":obj:`generator_enc_hidden_states`, `optional`: "
":obj:`generator_enc_attentions`). :obj:`generator_enc_last_hidden_state` "
"of shape :obj:`(batch_size, n_docs * sequence_length, hidden_size)` is a "
"sequence of hidden-states at the output of the last layer of the "
"generator's encoder.  Used by the (:class:`~transformers.TFRagModel`) "
"model during decoding."
msgstr ""

#: of transformers.TFRagModel.call:24
#: transformers.TFRagSequenceForGeneration.call:24
#: transformers.TFRagTokenForGeneration.call:24
msgid "Used by the (:class:`~transformers.TFRagModel`) model during decoding."
msgstr ""

#: of transformers.TFRagModel.call:68
#: transformers.TFRagSequenceForGeneration.call:68
#: transformers.TFRagTokenForGeneration.call:68
msgid ""
"Whether or not to return a :class:`~TFRetrievAugLMOutput` instead of a "
"plain tuple."
msgstr ""

#: of transformers.TFRagModel.call:73
msgid ""
"A :class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` "
"or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs.  - "
"**logits** (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length,"
" config.vocab_size)`) -- Prediction scores of the language modeling head."
" The score is possibly marginalized over all documents for   each "
"vocabulary token. - **past_key_values** (:obj:`List[tf.Tensor]`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- List of :obj:`tf.Tensor` of length "
":obj:`config.n_layers`, with each tensor of shape :obj:`(2, batch_size,"
"   num_heads, sequence_length, embed_size_per_head)`).    Contains "
"precomputed hidden-states (key and values in the attention blocks) of the"
" decoder that can be used   (see :obj:`past_key_values` input) to speed "
"up sequential decoding. - **doc_scores** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, config.n_docs)`) -- Score between each retrieved "
"document embeddings (see :obj:`retrieved_doc_embeds`) and   "
":obj:`question_encoder_last_hidden_state`. - **retrieved_doc_embeds** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, config.n_docs, "
"hidden_size)`, `optional`, returned when `output_retrieved=True`) -- "
"Embedded documents retrieved by the retriever. Is used with "
"``question_encoder_last_hidden_state`` to   compute the ``doc_scores``. -"
" **retrieved_doc_ids** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"config.n_docs)`, `optional`, returned when `output_retrieved=True`) -- "
"The indexes of the embedded documents retrieved by the retriever. - "
"**context_input_ids** (:obj:`tf.Tensor` of shape :obj:`(batch_size * "
"config.n_docs, config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever. - "
"**context_attention_mask** (:obj:`tf.Tensor` of shape :obj:`(batch_size *"
" config.n_docs, config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Attention mask post-processed from the "
"retrieved documents and the question encoder :obj:`input_ids` by the   "
"retriever. - **question_encoder_last_hidden_state** (:obj:`tf.Tensor` of "
"shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the   model. - **question_enc_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the question encoder at the output of each layer plus the "
"initial embedding outputs. - **question_enc_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the question encoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads. - **generator_enc_last_hidden_state** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) -- Sequence of hidden-states at the output of "
"the last layer of the generator encoder of the model. - "
"**generator_enc_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the generator encoder at the output of each layer plus the "
"initial embedding outputs. - **generator_enc_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the generator encoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads. - **generator_dec_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the generator decoder at the output of each layer plus the "
"initial embedding outputs. - **generator_dec_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the generator decoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads.   Example::      >>> from transformers import "
"RagTokenizer, RagRetriever, RagModel     >>> import torch      >>> "
"tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")"
"     >>> retriever = RagRetriever.from_pretrained(\"facebook/rag-token-"
"base\", index_name=\"exact\", use_dummy_dataset=True)     >>> # "
"initialize with RagRetriever to do everything in one forward call     >>>"
" model = TFRagModel.from_pretrained(\"facebook/rag-token-base\", "
"retriever=retriever, from_pt=True)      >>> input_dict = "
"tokenizer.prepare_seq2seq_batch(\"How many people live in Paris?\", \"In "
"Paris, there are 10 million people.\", return_tensors=\"tf\")     >>> "
"input_ids = input_dict[\"input_ids\"]     >>> outputs = model(input_ids)"
msgstr ""

#: of transformers.TFRagModel.call:73
msgid ""
"A :class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` "
"or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs."
msgstr ""

#: of transformers.TFRagModel.call:77
#: transformers.TFRagSequenceForGeneration.call:90
#: transformers.TFRagTokenForGeneration.call:90
msgid ""
"**logits** (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length,"
" config.vocab_size)`) -- Prediction scores of the language modeling head."
" The score is possibly marginalized over all documents for each "
"vocabulary token."
msgstr ""

#: of transformers.TFRagModel.call:79
#: transformers.TFRagSequenceForGeneration.call:92
#: transformers.TFRagTokenForGeneration.call:92
msgid ""
"**past_key_values** (:obj:`List[tf.Tensor]`, `optional`, returned when "
"``use_cache=True`` is passed or when ``config.use_cache=True``) -- List "
"of :obj:`tf.Tensor` of length :obj:`config.n_layers`, with each tensor of"
" shape :obj:`(2, batch_size, num_heads, sequence_length, "
"embed_size_per_head)`)."
msgstr ""

#: of transformers.TFRagModel.call:84
#: transformers.TFRagSequenceForGeneration.call:97
#: transformers.TFRagTokenForGeneration.call:97
msgid ""
"**doc_scores** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"config.n_docs)`) -- Score between each retrieved document embeddings (see"
" :obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`."
msgstr ""

#: of transformers.TFRagModel.call:86
#: transformers.TFRagSequenceForGeneration.call:99
#: transformers.TFRagTokenForGeneration.call:99
msgid ""
"**retrieved_doc_embeds** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"config.n_docs, hidden_size)`, `optional`, returned when "
"`output_retrieved=True`) -- Embedded documents retrieved by the "
"retriever. Is used with ``question_encoder_last_hidden_state`` to compute"
" the ``doc_scores``."
msgstr ""

#: of transformers.TFRagModel.call:88
msgid ""
"**retrieved_doc_ids** (:obj:`tf.Tensor` of shape :obj:`(batch_size, "
"config.n_docs)`, `optional`, returned when `output_retrieved=True`) -- "
"The indexes of the embedded documents retrieved by the retriever."
msgstr ""

#: of transformers.TFRagModel.call:89
msgid ""
"**context_input_ids** (:obj:`tf.Tensor` of shape :obj:`(batch_size * "
"config.n_docs, config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever."
msgstr ""

#: of transformers.TFRagModel.call:90
msgid ""
"**context_attention_mask** (:obj:`tf.Tensor` of shape :obj:`(batch_size *"
" config.n_docs, config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Attention mask post-processed from the "
"retrieved documents and the question encoder :obj:`input_ids` by the "
"retriever."
msgstr ""

#: of transformers.TFRagModel.call:92
#: transformers.TFRagSequenceForGeneration.call:105
#: transformers.TFRagTokenForGeneration.call:105
msgid ""
"**question_encoder_last_hidden_state** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the model."
msgstr ""

#: of transformers.TFRagModel.call:94
#: transformers.TFRagSequenceForGeneration.call:107
#: transformers.TFRagTokenForGeneration.call:107
msgid ""
"**question_enc_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
" shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFRagModel.call:98
#: transformers.TFRagSequenceForGeneration.call:111
#: transformers.TFRagTokenForGeneration.call:111
msgid ""
"**question_enc_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFRagModel.call:103
#: transformers.TFRagSequenceForGeneration.call:116
#: transformers.TFRagTokenForGeneration.call:116
msgid ""
"**generator_enc_last_hidden_state** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden-states at the output of the last layer of the "
"generator encoder of the model."
msgstr ""

#: of transformers.TFRagModel.call:104
#: transformers.TFRagSequenceForGeneration.call:117
#: transformers.TFRagTokenForGeneration.call:117
msgid ""
"**generator_enc_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
" shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFRagModel.call:108
#: transformers.TFRagSequenceForGeneration.call:121
#: transformers.TFRagTokenForGeneration.call:121
msgid ""
"**generator_enc_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFRagModel.call:113
#: transformers.TFRagSequenceForGeneration.call:126
#: transformers.TFRagTokenForGeneration.call:126
msgid ""
"**generator_dec_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
" shape :obj:`(batch_size, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.TFRagModel.call:117
#: transformers.TFRagSequenceForGeneration.call:130
#: transformers.TFRagTokenForGeneration.call:130
msgid ""
"**generator_dec_attentions** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length, "
"sequence_length)`."
msgstr ""

#: of transformers.TFRagModel.call:137
msgid ""
":class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` or"
" :obj:`tuple(tf.Tensor)`"
msgstr ""

#: ../../source/model_doc/rag.rst:108
msgid "TFRagSequenceForGeneration"
msgstr ""

#: of transformers.TFRagSequenceForGeneration:1
#: transformers.TFRagSequenceForGeneration.call:1
msgid ""
"The :class:`~transformers.TFRagSequenceForGeneration` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TFRagSequenceForGeneration:8
msgid ""
"A TF RAG-sequence model implementation. It performs RAG-sequence specific"
" marginalization in the forward pass."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:75
msgid ""
"Labels for computing the cross entropy classification loss according to "
"Rag-Sequence model formulation See https://arxiv.org/pdf/2005.11401.pdf "
"Section 2.1 for details about Rag-Sequence formulation. Indices should be"
" in ``[0, ..., config.vocab_size - 1]``."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:79
#: transformers.TFRagTokenForGeneration.call:79
msgid ""
"Only relevant if ``labels`` is passed. If :obj:`True`, the NLL loss is "
"reduced using the ``tf.Tensor.sum`` operation."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:85
msgid ""
"A "
":class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs.  - "
"**loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Language modeling loss. - **logits** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head. "
"The score is possibly marginalized over all documents for   each "
"vocabulary token. - **past_key_values** (:obj:`List[tf.Tensor]`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- List of :obj:`tf.Tensor` of length "
":obj:`config.n_layers`, with each tensor of shape :obj:`(2, batch_size,"
"   num_heads, sequence_length, embed_size_per_head)`).    Contains "
"precomputed hidden-states (key and values in the attention blocks) of the"
" decoder that can be used   (see :obj:`past_key_values` input) to speed "
"up sequential decoding. - **doc_scores** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, config.n_docs)`) -- Score between each retrieved "
"document embeddings (see :obj:`retrieved_doc_embeds`) and   "
":obj:`question_encoder_last_hidden_state`. - **retrieved_doc_embeds** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, config.n_docs, "
"hidden_size)`, `optional`, returned when `output_retrieved=True`) -- "
"Embedded documents retrieved by the retriever. Is used with "
"``question_encoder_last_hidden_state`` to   compute the ``doc_scores``. -"
" **retrieved_doc_ids** (:obj:`tf.Tensor` (int32) of shape "
":obj:`(batch_size, config.n_docs)`, `optional`, returned when "
"`output_retrieved=True`) -- The indexes of the embedded documents "
"retrieved by the retriever. - **context_input_ids** "
"(:obj:`tf.Tensor`(int32) of shape :obj:`(batch_size * config.n_docs, "
"config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever. - "
"**context_attention_mask** (:obj:`tf.Tensor` (int32) of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the   retriever. - "
"**question_encoder_last_hidden_state** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the   model. - **question_enc_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the question encoder at the output of each layer plus the "
"initial embedding outputs. - **question_enc_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the question encoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads. - **generator_enc_last_hidden_state** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) -- Sequence of hidden-states at the output of "
"the last layer of the generator encoder of the model. - "
"**generator_enc_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the generator encoder at the output of each layer plus the "
"initial embedding outputs. - **generator_enc_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the generator encoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads. - **generator_dec_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the generator decoder at the output of each layer plus the "
"initial embedding outputs. - **generator_dec_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the generator decoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads.   Example::      >>> from transformers import "
"RagTokenizer, RagRetriever, TFRagSequenceForGeneration      >>> tokenizer"
" = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")     >>> "
"retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", "
"index_name=\"exact\", use_dummy_dataset=True)     >>> # initialize with "
"RagRetriever to do everything in one forward call     >>> model = "
"TFRagRagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-"
"nq\", retriever=retriever, from_pt=True)      >>> input_dict = "
"tokenizer.prepare_seq2seq_batch(\"How many people live in Paris?\", \"In "
"Paris, there are 10 million people.\", return_tensors=\"tf\")     >>> "
"outputs = model(input_dict, output_retrieved=True)      >>> # or use "
"retriever separately     >>> # 1. Encode     >>> input_ids = "
"input_dict[\"input_ids\"]     >>> question_hidden_states = "
"model.question_encoder(input_ids)[0]     >>> # 2. Retrieve     >>> "
"docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), "
"return_tensors=\"tf\")     >>> doc_scores = "
"tf.squeeze(tf.matmul(tf.expand_dims(question_hidden_states, axis=1), "
"docs_dict[\"retrieved_doc_embeds\"], transpose_b=True), axis=1)     >>> #"
" 3. Forward to generator     >>> outputs = model(inputs=None, "
"context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores, decoder_input_ids=input_dict[\"labels\"])      >>>"
" # or directly generate     >>> generated = "
"model.generate(context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores)     >>> generated_string = "
"tokenizer.batch_decode(generated, skip_special_tokens=True)"
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:85
#: transformers.TFRagTokenForGeneration.call:85
msgid ""
"A "
":class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:89
#: transformers.TFRagTokenForGeneration.call:89
msgid ""
"**loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Language modeling loss."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:101
#: transformers.TFRagTokenForGeneration.call:101
msgid ""
"**retrieved_doc_ids** (:obj:`tf.Tensor` (int32) of shape "
":obj:`(batch_size, config.n_docs)`, `optional`, returned when "
"`output_retrieved=True`) -- The indexes of the embedded documents "
"retrieved by the retriever."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:102
#: transformers.TFRagTokenForGeneration.call:102
msgid ""
"**context_input_ids** (:obj:`tf.Tensor`(int32) of shape :obj:`(batch_size"
" * config.n_docs, config.max_combined_length)`, `optional`, returned when"
" `output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:103
#: transformers.TFRagTokenForGeneration.call:103
msgid ""
"**context_attention_mask** (:obj:`tf.Tensor` (int32) of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the retriever."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.call:162
#: transformers.TFRagTokenForGeneration.call:162
msgid ""
":class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`"
" or :obj:`tuple(tf.Tensor)`"
msgstr ""

#: of transformers.TFRagSequenceForGeneration.generate:1
msgid ""
"Implements RAG sequence \"thorough\" decoding. Read the "
":meth:`~transformers.generation_utils.GenerationMixin.generate`` "
"documentation for more information on how to set other generate input "
"parameters"
msgstr ""

#: of transformers.TFRagSequenceForGeneration.generate:8
msgid ""
"Mask to avoid performing attention on padding token indices. Mask values "
"selected in ``[0, 1]``: - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**. `What are attention masks? <../glossary.html"
"#attention-mask>`__"
msgstr ""

#: of transformers.TFRagSequenceForGeneration.generate:15
msgid ""
"Attention mask post-processed from the retrieved documents and the "
"question encoder :obj:`input_ids` by the retriever. If the model has is "
"not initialized with a ``retriever`` or ``input_ids`` is not given, "
":obj:`context_input_ids` and :obj:`context_attention_mask` have to be "
"provided to the forward pass. They are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.generate:20
msgid ""
"Score between each retrieved document embeddings (see "
":obj:`retrieved_doc_embeds`) and "
":obj:`question_encoder_last_hidden_state`. If the model has is not "
"initialized with a ``retriever`` or ``input_ids`` is not given, "
":obj:`doc_scores` has to be provided to the forward pass. "
":obj:`doc_scores` are returned by "
":meth:`~transformers.RagRetriever.__call__`."
msgstr ""

#: of transformers.TFRagSequenceForGeneration.generate:37
msgid ""
"Additional kwargs will be passed to "
":meth:`~transformers.generation_utils.GenerationMixin.generate`"
msgstr ""

#: of transformers.TFRagSequenceForGeneration.generate:42
#: transformers.TFRagTokenForGeneration.generate:86
msgid ""
":obj:`tf.Tensor` of shape :obj:`(batch_size * num_return_sequences, "
"sequence_length)`"
msgstr ""

#: ../../source/model_doc/rag.rst:115
msgid "TFRagTokenForGeneration"
msgstr ""

#: of transformers.TFRagTokenForGeneration:1
#: transformers.TFRagTokenForGeneration.call:1
msgid ""
"The :class:`~transformers.TFRagTokenForGeneration` forward method, "
"overrides the :func:`__call__` special method."
msgstr ""

#: of transformers.TFRagTokenForGeneration:8
msgid ""
"A TF RAG-token model implementation. It performs RAG-token specific "
"marginalization in the forward pass."
msgstr ""

#: of transformers.TFRagTokenForGeneration.call:75
msgid ""
"Labels for computing the cross entropy classification loss according to "
"Rag-Token model formulation See https://arxiv.org/pdf/2005.11401.pdf "
"Section 2.1 for details about Rag-Token formulation. Indices should be in"
" ``[0, ..., config.vocab_size - 1]``."
msgstr ""

#: of transformers.TFRagTokenForGeneration.call:85
msgid ""
"A "
":class:`~transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`"
" or a tuple of :obj:`tf.Tensor` (if ``return_dict=False`` is passed or "
"when ``config.return_dict=False``) comprising various elements depending "
"on the configuration (:class:`~transformers.RagConfig`) and inputs.  - "
"**loss** (:obj:`tf.Tensor` of shape :obj:`(1,)`, `optional`, returned "
"when :obj:`labels` is provided) -- Language modeling loss. - **logits** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"config.vocab_size)`) -- Prediction scores of the language modeling head. "
"The score is possibly marginalized over all documents for   each "
"vocabulary token. - **past_key_values** (:obj:`List[tf.Tensor]`, "
"`optional`, returned when ``use_cache=True`` is passed or when "
"``config.use_cache=True``) -- List of :obj:`tf.Tensor` of length "
":obj:`config.n_layers`, with each tensor of shape :obj:`(2, batch_size,"
"   num_heads, sequence_length, embed_size_per_head)`).    Contains "
"precomputed hidden-states (key and values in the attention blocks) of the"
" decoder that can be used   (see :obj:`past_key_values` input) to speed "
"up sequential decoding. - **doc_scores** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, config.n_docs)`) -- Score between each retrieved "
"document embeddings (see :obj:`retrieved_doc_embeds`) and   "
":obj:`question_encoder_last_hidden_state`. - **retrieved_doc_embeds** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, config.n_docs, "
"hidden_size)`, `optional`, returned when `output_retrieved=True`) -- "
"Embedded documents retrieved by the retriever. Is used with "
"``question_encoder_last_hidden_state`` to   compute the ``doc_scores``. -"
" **retrieved_doc_ids** (:obj:`tf.Tensor` (int32) of shape "
":obj:`(batch_size, config.n_docs)`, `optional`, returned when "
"`output_retrieved=True`) -- The indexes of the embedded documents "
"retrieved by the retriever. - **context_input_ids** "
"(:obj:`tf.Tensor`(int32) of shape :obj:`(batch_size * config.n_docs, "
"config.max_combined_length)`, `optional`, returned when "
"`output_retrieved=True`) -- Input ids post-processed from the retrieved "
"documents and the question encoder input_ids by the retriever. - "
"**context_attention_mask** (:obj:`tf.Tensor` (int32) of shape "
":obj:`(batch_size * config.n_docs, config.max_combined_length)`, "
"`optional`, returned when `output_retrieved=True`) -- Attention mask "
"post-processed from the retrieved documents and the question encoder "
":obj:`input_ids` by the   retriever. - "
"**question_encoder_last_hidden_state** (:obj:`tf.Tensor` of shape "
":obj:`(batch_size, sequence_length, hidden_size)`, `optional`) -- "
"Sequence of hidden states at the output of the last layer of the question"
" encoder pooled output of the   model. - **question_enc_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the question encoder at the output of each layer plus the "
"initial embedding outputs. - **question_enc_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the question encoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads. - **generator_enc_last_hidden_state** "
"(:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, "
"hidden_size)`, `optional`) -- Sequence of hidden-states at the output of "
"the last layer of the generator encoder of the model. - "
"**generator_enc_hidden_states** (:obj:`tuple(tf.Tensor)`, `optional`, "
"returned when ``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the generator encoder at the output of each layer plus the "
"initial embedding outputs. - **generator_enc_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the generator encoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads. - **generator_dec_hidden_states** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_hidden_states=True`` is passed or when "
"``config.output_hidden_states=True``) -- Tuple of :obj:`tf.Tensor` (one "
"for the output of the embeddings and one for the output of each layer) of"
"   shape :obj:`(batch_size, sequence_length, hidden_size)`.    Hidden "
"states of the generator decoder at the output of each layer plus the "
"initial embedding outputs. - **generator_dec_attentions** "
"(:obj:`tuple(tf.Tensor)`, `optional`, returned when "
"``output_attentions=True`` is passed or when "
"``config.output_attentions=True``) -- Tuple of :obj:`tf.Tensor` (one for "
"each layer) of shape :obj:`(batch_size, num_heads, sequence_length,   "
"sequence_length)`.    Attentions weights of the generator decoder, after "
"the attention softmax, used to compute the weighted   average in the "
"self-attention heads.   Example::      >>> from transformers import "
"RagTokenizer, RagRetriever, TFRagTokenForGeneration      >>> tokenizer = "
"RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")     >>> retriever"
" = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", "
"index_name=\"exact\", use_dummy_dataset=True)     >>> # initialize with "
"RagRetriever to do everything in one forward call     >>> model = "
"TFRagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", "
"retriever=retriever, from_pt=True)      >>> input_dict = "
"tokenizer.prepare_seq2seq_batch(\"How many people live in Paris?\", \"In "
"Paris, there are 10 million people.\", return_tensors=\"tf\")     >>> "
"outputs = model(input_dict, output_retrieved=True)      >>> # or use "
"retriever separately     >>> # 1. Encode     >>> input_ids = "
"input_dict[\"input_ids\"]     >>> question_hidden_states = "
"model.question_encoder(input_ids)[0]     >>> # 2. Retrieve     >>> "
"docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), "
"return_tensors=\"tf\")     >>> doc_scores = "
"tf.squeeze(tf.matmul(tf.expand_dims(question_hidden_states, axis=1), "
"docs_dict[\"retrieved_doc_embeds\"], transpose_b=True), axis=1)     >>> #"
" 3. Forward to generator     >>> outputs = model(inputs=None, "
"context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores, decoder_input_ids=input_dict[\"labels\"])      >>>"
" # or directly generate     >>> generated = "
"model.generate(context_input_ids=docs_dict[\"context_input_ids\"], "
"context_attention_mask=docs_dict[\"context_attention_mask\"], "
"doc_scores=doc_scores)     >>> generated_string = "
"tokenizer.batch_decode(generated, skip_special_tokens=True)"
msgstr ""

#: of transformers.TFRagTokenForGeneration.generate:1
msgid "Implements TFRAG token decoding."
msgstr ""

#: of transformers.TFRagTokenForGeneration.generate:71
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"See ``attentions`` under returned tensors for more details."
msgstr ""

#: of transformers.TFRagTokenForGeneration.generate:74
msgid ""
"Whether or not to return the hidden states of all layers. See "
"``hidden_states`` under returned tensors for more details."
msgstr ""

#: of transformers.TFRagTokenForGeneration.generate:77
msgid ""
"Whether or not to return the prediction scores. See ``scores`` under "
"returned tensors for more details."
msgstr ""

#: of transformers.TFRagTokenForGeneration.generate:79
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.TFRagTokenForGeneration.generate:81
msgid ""
"Additional model specific kwargs will be forwarded to the :obj:`forward` "
"function of the model."
msgstr ""


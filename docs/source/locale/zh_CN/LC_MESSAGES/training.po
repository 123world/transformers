# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/training.rst:14
msgid "Fine-tuning a pretrained model"
msgstr ""

#: ../../source/training.rst:16
msgid ""
"In this tutorial, we will show you how to fine-tune a pretrained model "
"from the Transformers library. In TensorFlow, models can be directly "
"trained using Keras and the :obj:`fit` method. In PyTorch, there is no "
"generic training loop so the ðŸ¤— Transformers library provides an API with "
"the class :class:`~transformers.Trainer` to let you fine-tune or train a "
"model from scratch easily. Then we will show you how to alternatively "
"write the whole training loop in PyTorch."
msgstr ""

#: ../../source/training.rst:21
msgid ""
"Before we can fine-tune a model, we need a dataset. In this tutorial, we "
"will show you how to fine-tune BERT on the `IMDB dataset "
"<https://www.imdb.com/interfaces/>`__: the task is to classify whether "
"movie reviews are positive or negative. For examples of other tasks, "
"refer to the :ref:`additional-resources` section!"
msgstr ""

#: ../../source/training.rst:28
msgid "Preparing the datasets"
msgstr ""

#: ../../source/training.rst:36
msgid ""
"We will use the `ðŸ¤— Datasets <https:/github.com/huggingface/datasets/>`__ "
"library to download and preprocess the IMDB datasets. We will go over "
"this part pretty quickly. Since the focus of this tutorial is on "
"training, you should refer to the ðŸ¤— Datasets `documentation "
"<https://huggingface.co/docs/datasets/>`__ or the :doc:`preprocessing` "
"tutorial for more information."
msgstr ""

#: ../../source/training.rst:41
msgid ""
"First, we can use the :obj:`load_dataset` function to download and cache "
"the dataset:"
msgstr ""

#: ../../source/training.rst:49
msgid ""
"This works like the :obj:`from_pretrained` method we saw for the models "
"and tokenizers (except the cache directory is "
"`~/.cache/huggingface/dataset` by default)."
msgstr ""

#: ../../source/training.rst:52
msgid ""
"The :obj:`raw_datasets` object is a dictionary with three keys: "
":obj:`\"train\"`, :obj:`\"test\"` and :obj:`\"unsupervised\"` (which "
"correspond to the three splits of that dataset). We will use the "
":obj:`\"train\"` split for training and the :obj:`\"test\"` split for "
"validation."
msgstr ""

#: ../../source/training.rst:56
msgid "To preprocess our data, we will need a tokenizer:"
msgstr ""

#: ../../source/training.rst:64
msgid ""
"As we saw in :doc:`preprocessing`, we can prepare the text inputs for the"
" model with the following command (this is an example, not a command you "
"can execute):"
msgstr ""

#: ../../source/training.rst:71
msgid ""
"This will make all the samples have the maximum length the model can "
"accept (here 512), either by padding or truncating them."
msgstr ""

#: ../../source/training.rst:74
msgid ""
"However, we can instead apply these preprocessing steps to all the splits"
" of our dataset at once by using the :obj:`map` method:"
msgstr ""

#: ../../source/training.rst:84
msgid ""
"You can learn more about the map method or the other ways to preprocess "
"the data in the ðŸ¤— Datasets `documentation "
"<https://huggingface.co/docs/datasets/>`__."
msgstr ""

#: ../../source/training.rst:87
msgid ""
"Next we will generate a small subset of the training and validation set, "
"to enable faster training:"
msgstr ""

#: ../../source/training.rst:96
msgid ""
"In all the examples below, we will always use :obj:`small_train_dataset` "
"and :obj:`small_eval_dataset`. Just replace them by their `full` "
"equivalent to train or evaluate on the full dataset."
msgstr ""

#: ../../source/training.rst:102
msgid "Fine-tuning in PyTorch with the Trainer API"
msgstr ""

#: ../../source/training.rst:110
msgid ""
"Since PyTorch does not provide a training loop, the ðŸ¤— Transformers "
"library provides a :class:`~transformers.Trainer` API that is optimized "
"for ðŸ¤— Transformers models, with a wide range of training options and with"
" built-in features like logging, gradient accumulation, and mixed "
"precision."
msgstr ""

#: ../../source/training.rst:114
msgid "First, let's define our model:"
msgstr ""

#: ../../source/training.rst:122
msgid ""
"This will issue a warning about some of the pretrained weights not being "
"used and some weights being randomly initialized. That's because we are "
"throwing away the pretraining head of the BERT model to replace it with a"
" classification head which is randomly initialized. We will fine-tune "
"this model on our task, transferring the knowledge of the pretrained "
"model to it (which is why doing this is called transfer learning)."
msgstr ""

#: ../../source/training.rst:127
msgid ""
"Then, to define our :class:`~transformers.Trainer`, we will need to "
"instantiate a :class:`~transformers.TrainingArguments`. This class "
"contains all the hyperparameters we can tune for the "
":class:`~transformers.Trainer` or the flags to activate the different "
"training options it supports. Let's begin by using all the defaults, the "
"only thing we then have to provide is a directory in which the "
"checkpoints will be saved:"
msgstr ""

#: ../../source/training.rst:138
msgid "Then we can instantiate a :class:`~transformers.Trainer` like this:"
msgstr ""

#: ../../source/training.rst:148
msgid "To fine-tune our model, we just need to call"
msgstr ""

#: ../../source/training.rst:154
msgid ""
"which will start a training that you can follow with a progress bar, "
"which should take a couple of minutes to complete (as long as you have "
"access to a GPU). It won't actually tell you anything useful about how "
"well (or badly) your model is performing however as by default, there is "
"no evaluation during training, and we didn't tell the "
":class:`~transformers.Trainer` to compute any metrics. Let's have a look "
"on how to do that now!"
msgstr ""

#: ../../source/training.rst:159
msgid ""
"To have the :class:`~transformers.Trainer` compute and report metrics, we"
" need to give it a :obj:`compute_metrics` function that takes predictions"
" and labels (grouped in a namedtuple called "
":class:`~transformers.EvalPrediction`) and return a dictionary with "
"string items (the metric names) and float values (the metric values)."
msgstr ""

#: ../../source/training.rst:163
msgid ""
"The ðŸ¤— Datasets library provides an easy way to get the common metrics "
"used in NLP with the :obj:`load_metric` function. here we simply use "
"accuracy. Then we define the :obj:`compute_metrics` function that just "
"convert logits to predictions (remember that all ðŸ¤— Transformers models "
"return the logits) and feed them to :obj:`compute` method of this metric."
msgstr ""

#: ../../source/training.rst:179
msgid ""
"The compute function needs to receive a tuple (with logits and labels) "
"and has to return a dictionary with string keys (the name of the metric) "
"and float values. It will be called at the end of each evaluation phase "
"on the whole arrays of predictions/labels."
msgstr ""

#: ../../source/training.rst:183
msgid ""
"To check if this works on practice, let's create a new "
":class:`~transformers.Trainer` with our fine-tuned model:"
msgstr ""

#: ../../source/training.rst:196
#, python-format
msgid "which showed an accuracy of 87.5% in our case."
msgstr ""

#: ../../source/training.rst:198
msgid ""
"If you want to fine-tune your model and regularly report the evaluation "
"metrics (for instance at the end of each epoch), here is how you should "
"define your training arguments:"
msgstr ""

#: ../../source/training.rst:207
msgid ""
"See the documentation of :class:`~transformers.TrainingArguments` for "
"more options."
msgstr ""

#: ../../source/training.rst:213
msgid "Fine-tuning with Keras"
msgstr ""

#: ../../source/training.rst:221
msgid ""
"Models can also be trained natively in TensorFlow using the Keras API. "
"First, let's define our model:"
msgstr ""

#: ../../source/training.rst:230
msgid ""
"Then we will need to convert our datasets from before in standard "
":obj:`tf.data.Dataset`. Since we have fixed shapes, it can easily be done"
" like this. First we remove the `\"text\"` column from our datasets and "
"set them in TensorFlow format:"
msgstr ""

#: ../../source/training.rst:239
msgid ""
"Then we convert everything in big tensors and use the "
":obj:`tf.data.Dataset.from_tensor_slices` method:"
msgstr ""

#: ../../source/training.rst:251
msgid ""
"With this done, the model can then be compiled and trained as any Keras "
"model:"
msgstr ""

#: ../../source/training.rst:263
msgid ""
"With the tight interoperability between TensorFlow and PyTorch models, "
"you can even save the model and then reload it as a PyTorch model (or "
"vice-versa):"
msgstr ""

#: ../../source/training.rst:276
msgid "Fine-tuning in native PyTorch"
msgstr ""

#: ../../source/training.rst:284
msgid ""
"You might need to restart your notebook at this stage to free some "
"memory, or excute the following code:"
msgstr ""

#: ../../source/training.rst:293
msgid ""
"Let's now see how to achieve the same results as in :ref:`trainer section"
" <trainer>` in PyTorch. First we need to define the dataloaders, which we"
" will use to iterate over batches. We just need to apply a bit of post-"
"processing to our :obj:`tokenized_datasets` before doing that to:"
msgstr ""

#: ../../source/training.rst:297
msgid ""
"remove the columns corresponding to values the model does not expect "
"(here the :obj:`\"text\"` column)"
msgstr ""

#: ../../source/training.rst:298
msgid ""
"rename the column :obj:`\"label\"` to :obj:`\"labels\"` (because the "
"model expect the argument to be named :obj:`labels`)"
msgstr ""

#: ../../source/training.rst:299
msgid ""
"set the format of the datasets so they return PyTorch Tensors instead of "
"lists."
msgstr ""

#: ../../source/training.rst:301
msgid "Our `tokenized_datasets` has one method for each of those steps:"
msgstr ""

#: ../../source/training.rst:312
msgid "Now that this is done, we can easily define our dataloaders:"
msgstr ""

#: ../../source/training.rst:321
msgid "Next, we define our model:"
msgstr ""

#: ../../source/training.rst:329
msgid ""
"We are almost ready to write our training loop, the only two things are "
"missing are an optimizer and a learning rate scheduler. The default "
"optimizer used by the :class:`~transformers.Trainer` is "
":class:`~transformers.AdamW`:"
msgstr ""

#: ../../source/training.rst:338
msgid ""
"Finally, the learning rate scheduler used by default it just a linear "
"decay form the maximum value (5e-5 here) to 0:"
msgstr ""

#: ../../source/training.rst:353
msgid ""
"One last thing, we will want to use the GPU if we have access to one "
"(otherwise training might take several hours instead of a couple of "
"minutes). To do this, we define a :obj:`device` we will put our model and"
" our batches on."
msgstr ""

#: ../../source/training.rst:363
msgid ""
"We now are ready to train! To get some sense of when it will be finished,"
" we add a progress bar over our number of training steps, using the "
"`tqdm` library."
msgstr ""

#: ../../source/training.rst:385
msgid ""
"Note that if you are used to freezing the body of your pretrained model "
"(like in computer vision) the above may seem a bit strange, as we are "
"directly fine-tuning the whole model without taking any precaution. It "
"actually works better this way for Transformers model (so this is not an "
"oversight on our side). If you're not familiar with what \"freezing the "
"body\" of the model means, forget you read this paragraph."
msgstr ""

#: ../../source/training.rst:390
msgid ""
"Now to check the results, we need to write the evaluation loop. Like in "
"the :ref:`trainer section <trainer>` we will use a metric from the "
"datasets library. Here we accumulate the predictions at each batch before"
" computing the final result when the loop is finished."
msgstr ""

#: ../../source/training.rst:413
msgid "Additional resources"
msgstr ""

#: ../../source/training.rst:415
msgid "To look at more fine-tuning examples you can refer to:"
msgstr ""

#: ../../source/training.rst:417
msgid ""
"`ðŸ¤— Transformers Examples "
"<https://github.com/huggingface/transformers/tree/master/examples>`__ "
"which includes scripts to train on all common NLP tasks in PyTorch and "
"TensorFlow."
msgstr ""

#: ../../source/training.rst:420
msgid ""
"`ðŸ¤— Transformers Notebooks <notebooks.html>`__ which contains various "
"notebooks and in particular one per task (look for the `how to finetune a"
" model on xxx`)."
msgstr ""


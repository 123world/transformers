# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/internal/generation_utils.rst:14
msgid "Utilities for Generation"
msgstr ""

#: ../../source/internal/generation_utils.rst:16
msgid ""
"This page lists all the utility functions used by "
":meth:`~transformers.generation_utils.GenerationMixin.generate`, "
":meth:`~transformers.generation_utils.GenerationMixin.greedy_search`, "
":meth:`~transformers.generation_utils.GenerationMixin.sample`, "
":meth:`~transformers.generation_utils.GenerationMixin.beam_search`, "
":meth:`~transformers.generation_utils.GenerationMixin.beam_sample`, and "
":meth:`~transformers.generation_utils.GenerationMixin.group_beam_search`."
msgstr ""

#: ../../source/internal/generation_utils.rst:23
msgid ""
"Most of those are only useful if you are studying the code of the "
"generate methods in the library."
msgstr ""

#: ../../source/internal/generation_utils.rst:26
msgid "Generate Outputs"
msgstr ""

#: ../../source/internal/generation_utils.rst:28
msgid ""
"The output of "
":meth:`~transformers.generation_utils.GenerationMixin.generate` is an "
"instance of a subclass of :class:`~transformers.file_utils.ModelOutput`. "
"This output is a data structure containing all the information returned "
"by :meth:`~transformers.generation_utils.GenerationMixin.generate`, but "
"that can also be used as tuple or dictionary."
msgstr ""

#: ../../source/internal/generation_utils.rst:32
msgid "Here's an example:"
msgstr ""

#: ../../source/internal/generation_utils.rst:44
msgid ""
"The ``generation_output`` object is a "
":class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`, as"
" we can see in the documentation of that class below, it means it has the"
" following attributes:"
msgstr ""

#: ../../source/internal/generation_utils.rst:47
msgid "``sequences``: the generated sequences of tokens"
msgstr ""

#: ../../source/internal/generation_utils.rst:48
msgid ""
"``scores`` (optional): the prediction scores of the language modelling "
"head, for each generation step"
msgstr ""

#: ../../source/internal/generation_utils.rst:49
msgid ""
"``hidden_states`` (optional): the hidden states of the model, for each "
"generation step"
msgstr ""

#: ../../source/internal/generation_utils.rst:50
msgid ""
"``attentions`` (optional): the attention weights of the model, for each "
"generation step"
msgstr ""

#: ../../source/internal/generation_utils.rst:52
msgid ""
"Here we have the ``scores`` since we passed along ``output_scores=True``,"
" but we don't have ``hidden_states`` and ``attentions`` because we didn't"
" pass ``output_hidden_states=True`` or ``output_attentions=True``."
msgstr ""

#: ../../source/internal/generation_utils.rst:55
msgid ""
"You can access each attribute as you would usually do, and if that "
"attribute has not been returned by the model, you will get ``None``. Here"
" for instance ``generation_output.scores`` are all the generated "
"prediction scores of the language modeling head, and "
"``generation_output.attentions`` is ``None``."
msgstr ""

#: ../../source/internal/generation_utils.rst:59
msgid ""
"When using our ``generation_output`` object as a tuple, it only keeps the"
" attributes that don't have ``None`` values. Here, for instance, it has "
"two elements, ``loss`` then ``logits``, so"
msgstr ""

#: ../../source/internal/generation_utils.rst:66
msgid ""
"will return the tuple ``(generation_output.sequences, "
"generation_output.scores)`` for instance."
msgstr ""

#: ../../source/internal/generation_utils.rst:68
msgid ""
"When using our ``generation_output`` object as a dictionary, it only "
"keeps the attributes that don't have ``None`` values. Here, for instance,"
" it has two keys that are ``sequences`` and ``scores``."
msgstr ""

#: ../../source/internal/generation_utils.rst:71
msgid "We document here all output types."
msgstr ""

#: ../../source/internal/generation_utils.rst:75
msgid "GreedySearchOutput"
msgstr ""

#: of transformers.generation_utils.GreedySearchDecoderOnlyOutput:1
msgid ""
"Base class for outputs of decoder-only generation models using greedy "
"search."
msgstr ""

#: of transformers.BeamScorer.finalize transformers.BeamScorer.process
#: transformers.BeamSearchScorer transformers.BeamSearchScorer.finalize
#: transformers.BeamSearchScorer.process
#: transformers.FlaxForcedBOSTokenLogitsProcessor
#: transformers.FlaxForcedBOSTokenLogitsProcessor.__call__
#: transformers.FlaxForcedEOSTokenLogitsProcessor
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__
#: transformers.FlaxLogitsProcessorList.__call__
#: transformers.FlaxMinLengthLogitsProcessor
#: transformers.FlaxMinLengthLogitsProcessor.__call__
#: transformers.FlaxTemperatureLogitsWarper
#: transformers.FlaxTemperatureLogitsWarper.__call__
#: transformers.FlaxTopKLogitsWarper transformers.FlaxTopKLogitsWarper.__call__
#: transformers.FlaxTopPLogitsWarper transformers.FlaxTopPLogitsWarper.__call__
#: transformers.ForcedBOSTokenLogitsProcessor
#: transformers.ForcedBOSTokenLogitsProcessor.__call__
#: transformers.ForcedEOSTokenLogitsProcessor
#: transformers.ForcedEOSTokenLogitsProcessor.__call__
#: transformers.HammingDiversityLogitsProcessor
#: transformers.HammingDiversityLogitsProcessor.__call__
#: transformers.InfNanRemoveLogitsProcessor.__call__
#: transformers.LogitsProcessorList.__call__ transformers.MaxLengthCriteria
#: transformers.MaxLengthCriteria.__call__ transformers.MaxTimeCriteria
#: transformers.MaxTimeCriteria.__call__ transformers.MinLengthLogitsProcessor
#: transformers.MinLengthLogitsProcessor.__call__
#: transformers.NoBadWordsLogitsProcessor
#: transformers.NoBadWordsLogitsProcessor.__call__
#: transformers.NoRepeatNGramLogitsProcessor
#: transformers.NoRepeatNGramLogitsProcessor.__call__
#: transformers.PrefixConstrainedLogitsProcessor
#: transformers.PrefixConstrainedLogitsProcessor.__call__
#: transformers.RepetitionPenaltyLogitsProcessor
#: transformers.RepetitionPenaltyLogitsProcessor.__call__
#: transformers.StoppingCriteria.__call__
#: transformers.StoppingCriteriaList.__call__
#: transformers.TemperatureLogitsWarper
#: transformers.TemperatureLogitsWarper.__call__ transformers.TopKLogitsWarper
#: transformers.TopKLogitsWarper.__call__ transformers.TopPLogitsWarper
#: transformers.TopPLogitsWarper.__call__
#: transformers.generation_flax_utils.FlaxGreedySearchOutput
#: transformers.generation_flax_utils.FlaxSampleOutput
#: transformers.generation_utils.BeamSampleDecoderOnlyOutput
#: transformers.generation_utils.BeamSampleEncoderDecoderOutput
#: transformers.generation_utils.BeamSearchDecoderOnlyOutput
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput
#: transformers.generation_utils.GreedySearchDecoderOnlyOutput
#: transformers.generation_utils.GreedySearchEncoderDecoderOutput
#: transformers.generation_utils.SampleDecoderOnlyOutput
#: transformers.generation_utils.SampleEncoderDecoderOutput
#: transformers.tf_top_k_top_p_filtering transformers.top_k_top_p_filtering
msgid "Parameters"
msgstr ""

#: of transformers.BeamScorer.finalize:20
#: transformers.BeamSearchScorer.finalize:20
#: transformers.generation_utils.BeamSampleDecoderOnlyOutput:3
#: transformers.generation_utils.BeamSampleEncoderDecoderOutput:5
#: transformers.generation_utils.BeamSearchDecoderOnlyOutput:3
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput:5
#: transformers.generation_utils.GreedySearchDecoderOnlyOutput:4
#: transformers.generation_utils.GreedySearchEncoderDecoderOutput:6
#: transformers.generation_utils.SampleDecoderOnlyOutput:4
#: transformers.generation_utils.SampleEncoderDecoderOutput:6
msgid ""
"The generated sequences. The second dimension (sequence_length) is either"
" equal to :obj:`max_length` or shorter if all batches finished early due "
"to the :obj:`eos_token_id`."
msgstr ""

#: of transformers.generation_utils.GreedySearchDecoderOnlyOutput:7
msgid ""
"Processed prediction scores of the language modeling head (scores for "
"each vocabulary token before SoftMax) at each generation step. :obj"
":`(max_length-input_ids.shape[-1],)`-shaped tuple of "
":obj:`torch.FloatTensor` with each tensor of shape :obj:`(batch_size, "
"config.vocab_size)`)."
msgstr ""

#: of transformers.generation_utils.BeamSampleEncoderDecoderOutput:25
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput:27
#: transformers.generation_utils.GreedySearchDecoderOnlyOutput:11
#: transformers.generation_utils.GreedySearchEncoderDecoderOutput:19
#: transformers.generation_utils.GreedySearchEncoderDecoderOutput:22
#: transformers.generation_utils.SampleEncoderDecoderOutput:23
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, num_heads, generated_length, sequence_length)`."
msgstr ""

#: of transformers.generation_utils.GreedySearchDecoderOnlyOutput:14
#: transformers.generation_utils.GreedySearchEncoderDecoderOutput:25
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size, generated_length, hidden_size)`."
msgstr ""

#: of transformers.generation_utils.GreedySearchEncoderDecoderOutput:1
msgid ""
"Base class for outputs of encoder-decoder generation models using greedy "
"search. Hidden states and attention weights of the decoder (respectively "
"the encoder) can be accessed via the encoder_attentions and the "
"encoder_hidden_states attributes (respectively the decoder_attentions and"
" the decoder_hidden_states attributes)"
msgstr ""

#: of transformers.generation_utils.GreedySearchEncoderDecoderOutput:9
msgid ""
"Processed prediction scores of the language modeling head (scores for "
"each vocabulary token before SoftMax) at each generation step. "
":obj:`(max_length-1,)`-shaped tuple of :obj:`torch.FloatTensor` with each"
" tensor of shape :obj:`(batch_size, config.vocab_size)`)."
msgstr ""

#: of transformers.generation_utils.BeamSampleEncoderDecoderOutput:15
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput:17
#: transformers.generation_utils.GreedySearchEncoderDecoderOutput:13
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of "
"shape :obj:`(batch_size, num_heads, sequence_length, sequence_length)`."
msgstr ""

#: of transformers.generation_utils.GreedySearchEncoderDecoderOutput:16
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size, "
"sequence_length, hidden_size)`."
msgstr ""

#: of transformers.generation_flax_utils.FlaxGreedySearchOutput:1
msgid ""
"Flax Base class for outputs of decoder-only generation models using "
"greedy search."
msgstr ""

#: of transformers.generation_flax_utils.FlaxGreedySearchOutput:4
#: transformers.generation_flax_utils.FlaxSampleOutput:4
msgid "The generated sequences."
msgstr ""

#: of transformers.generation_flax_utils.FlaxGreedySearchOutput.replace:1
#: transformers.generation_flax_utils.FlaxSampleOutput.replace:1
msgid "\"Returns a new object replacing the specified fields with new values."
msgstr ""

#: ../../source/internal/generation_utils.rst:88
msgid "SampleOutput"
msgstr ""

#: of transformers.generation_utils.SampleDecoderOnlyOutput:1
msgid "Base class for outputs of decoder-only generation models using sampling."
msgstr ""

#: of transformers.generation_utils.SampleDecoderOnlyOutput:7
msgid ""
"Processed prediction scores of the language modeling head (scores for "
"each vocabulary token before SoftMax) at each generation step. :obj"
":`(max_length-input_ids.shape[-1],)`-shaped tuple of "
":obj:`torch.FloatTensor` with each tensor of shape "
":obj:`(batch_size*num_return_sequences, config.vocab_size)`)."
msgstr ""

#: of transformers.generation_utils.SampleDecoderOnlyOutput:11
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(num_return_sequences*batch_size, num_heads, generated_length, "
"sequence_length)`."
msgstr ""

#: of transformers.generation_utils.SampleDecoderOnlyOutput:15
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(num_return_sequences*batch_size, generated_length, hidden_size)`."
msgstr ""

#: of transformers.generation_utils.SampleEncoderDecoderOutput:1
msgid ""
"Base class for outputs of encoder-decoder generation models using "
"sampling. Hidden states and attention weights of the decoder "
"(respectively the encoder) can be accessed via the encoder_attentions and"
" the encoder_hidden_states attributes (respectively the "
"decoder_attentions and the decoder_hidden_states attributes)"
msgstr ""

#: of transformers.generation_utils.SampleEncoderDecoderOutput:9
msgid ""
"Processed prediction scores of the language modeling head (scores for "
"each vocabulary token before SoftMax) at each generation step. "
":obj:`(max_length-1,)`-shaped tuple of :obj:`torch.FloatTensor` with each"
" tensor of shape :obj:`(batch_size*num_return_sequences, "
"config.vocab_size)`)."
msgstr ""

#: of transformers.generation_utils.SampleEncoderDecoderOutput:13
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of "
"shape :obj:`(batch_size*num_return_sequences, num_heads, sequence_length,"
" sequence_length)`."
msgstr ""

#: of transformers.generation_utils.SampleEncoderDecoderOutput:16
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape "
":obj:`(batch_size*num_return_sequences, sequence_length, hidden_size)`."
msgstr ""

#: of transformers.generation_utils.SampleEncoderDecoderOutput:19
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size*num_return_sequences, num_heads, generated_length, "
"sequence_length)`."
msgstr ""

#: of transformers.generation_utils.SampleEncoderDecoderOutput:26
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size*num_return_sequences, generated_length, hidden_size)`."
msgstr ""

#: of transformers.generation_flax_utils.FlaxSampleOutput:1
msgid ""
"Flax Base class for outputs of decoder-only generation models using "
"sampling."
msgstr ""

#: ../../source/internal/generation_utils.rst:101
msgid "BeamSearchOutput"
msgstr ""

#: of transformers.generation_utils.BeamSearchDecoderOnlyOutput:1
msgid ""
"Base class for outputs of decoder-only generation models using beam "
"search."
msgstr ""

#: of transformers.generation_utils.BeamSampleDecoderOnlyOutput:6
#: transformers.generation_utils.BeamSampleEncoderDecoderOutput:8
#: transformers.generation_utils.BeamSearchDecoderOnlyOutput:6
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput:8
msgid "Final beam scores of the generated ``sequences``."
msgstr ""

#: of transformers.generation_utils.BeamSampleDecoderOnlyOutput:8
#: transformers.generation_utils.BeamSearchDecoderOnlyOutput:8
msgid ""
"Processed beam scores for each vocabulary token at each generation step. "
"Beam scores consisting of log softmax scores for each vocabulary token "
"and sum of log softmax of previously generated tokens in this beam . :obj"
":`(max_length-input_ids.shape[-1],)`-shaped tuple of "
":obj:`torch.FloatTensor` with each tensor of shape "
":obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`)."
msgstr ""

#: of transformers.generation_utils.BeamSampleDecoderOnlyOutput:13
#: transformers.generation_utils.BeamSampleEncoderDecoderOutput:21
#: transformers.generation_utils.BeamSearchDecoderOnlyOutput:13
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size*num_beams, num_heads, generated_length, "
"sequence_length)`."
msgstr ""

#: of transformers.generation_utils.BeamSearchDecoderOnlyOutput:17
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput:30
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size*num_beams*num_return_sequences, generated_length, "
"hidden_size)`."
msgstr ""

#: of transformers.generation_utils.BeamSearchEncoderDecoderOutput:1
msgid ""
"Base class for outputs of encoder-decoder generation models using beam "
"search. Hidden states and attention weights of the decoder (respectively "
"the encoder) can be accessed via the encoder_attentions and the "
"encoder_hidden_states attributes (respectively the decoder_attentions and"
" the decoder_hidden_states attributes)"
msgstr ""

#: of transformers.generation_utils.BeamSampleEncoderDecoderOutput:10
#: transformers.generation_utils.BeamSearchEncoderDecoderOutput:10
msgid ""
"Processed beam scores for each vocabulary token at each generation step. "
"Beam scores consisting of log softmax scores for each vocabulary token "
"and sum of log softmax of previously generated tokens in this beam . "
":obj:`(max_length-1,)`-shaped tuple of :obj:`torch.FloatTensor` with each"
" tensor of shape :obj:`(batch_size*num_beams, config.vocab_size)`)."
msgstr ""

#: of transformers.generation_utils.BeamSearchEncoderDecoderOutput:20
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape "
":obj:`(batch_size*num_beams*num_return_sequences, sequence_length, "
"hidden_size)`."
msgstr ""

#: of transformers.generation_utils.BeamSearchEncoderDecoderOutput:23
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size*num_beams*num_return_sequences, num_heads, "
"generated_length, sequence_length)`."
msgstr ""

#: ../../source/internal/generation_utils.rst:111
msgid "BeamSampleOutput"
msgstr ""

#: of transformers.generation_utils.BeamSampleDecoderOnlyOutput:1
msgid ""
"Base class for outputs of decoder-only generation models using beam "
"sample."
msgstr ""

#: of transformers.generation_utils.BeamSampleDecoderOnlyOutput:17
#: transformers.generation_utils.BeamSampleEncoderDecoderOutput:28
msgid ""
"Tuple (one element for each generated token) of tuples (one element for "
"each layer of the decoder) of :obj:`torch.FloatTensor` of shape "
":obj:`(batch_size*num_beams, generated_length, hidden_size)`."
msgstr ""

#: of transformers.generation_utils.BeamSampleEncoderDecoderOutput:1
msgid ""
"Base class for outputs of encoder-decoder generation models using beam "
"sampling. Hidden states and attention weights of the decoder "
"(respectively the encoder) can be accessed via the encoder_attentions and"
" the encoder_hidden_states attributes (respectively the "
"decoder_attentions and the decoder_hidden_states attributes)"
msgstr ""

#: of transformers.generation_utils.BeamSampleEncoderDecoderOutput:18
msgid ""
"Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings +"
" one for the output of each layer) of shape :obj:`(batch_size*num_beams, "
"sequence_length, hidden_size)`."
msgstr ""

#: ../../source/internal/generation_utils.rst:121
msgid "LogitsProcessor"
msgstr ""

#: ../../source/internal/generation_utils.rst:123
msgid ""
"A :class:`~transformers.LogitsProcessor` can be used to modify the "
"prediction scores of a language model head for generation."
msgstr ""

#: of transformers.FlaxLogitsProcessor:1 transformers.LogitsProcessor:1
msgid ""
"Abstract base class for all logit processors that can be applied during "
"generation."
msgstr ""

#: of transformers.FlaxLogitsProcessor.__call__:14
#: transformers.FlaxLogitsWarper.__call__:14
#: transformers.LogitsProcessor.__call__:14
#: transformers.LogitsWarper.__call__:14
msgid "Args:"
msgstr ""

#: of transformers.LogitsProcessor.__call__:8
#: transformers.LogitsWarper.__call__:8
msgid ""
"input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, "
"sequence_length)`):"
msgstr ""

#: of transformers.BeamScorer.finalize:1 transformers.BeamScorer.process:1
#: transformers.BeamSearchScorer.finalize:1
#: transformers.BeamSearchScorer.process:1
#: transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:1
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:1
#: transformers.FlaxLogitsProcessor.__call__:3
#: transformers.FlaxLogitsProcessorList.__call__:1
#: transformers.FlaxLogitsWarper.__call__:3
#: transformers.FlaxMinLengthLogitsProcessor.__call__:1
#: transformers.FlaxTemperatureLogitsWarper.__call__:1
#: transformers.FlaxTopKLogitsWarper.__call__:1
#: transformers.FlaxTopPLogitsWarper.__call__:1
#: transformers.ForcedBOSTokenLogitsProcessor.__call__:1
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:1
#: transformers.HammingDiversityLogitsProcessor.__call__:1
#: transformers.InfNanRemoveLogitsProcessor.__call__:1
#: transformers.LogitsProcessor.__call__:3
#: transformers.LogitsProcessorList.__call__:1
#: transformers.LogitsWarper.__call__:3
#: transformers.MaxLengthCriteria.__call__:1
#: transformers.MaxTimeCriteria.__call__:1
#: transformers.MinLengthLogitsProcessor.__call__:1
#: transformers.NoBadWordsLogitsProcessor.__call__:1
#: transformers.NoRepeatNGramLogitsProcessor.__call__:1
#: transformers.PrefixConstrainedLogitsProcessor.__call__:1
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:1
#: transformers.StoppingCriteria.__call__:1
#: transformers.StoppingCriteriaList.__call__:1
#: transformers.TemperatureLogitsWarper.__call__:1
#: transformers.TopKLogitsWarper.__call__:1
#: transformers.TopPLogitsWarper.__call__:1
msgid "Indices of input sequence tokens in the vocabulary."
msgstr ""

#: of transformers.ForcedBOSTokenLogitsProcessor.__call__:3
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:3
#: transformers.HammingDiversityLogitsProcessor.__call__:3
#: transformers.InfNanRemoveLogitsProcessor.__call__:3
#: transformers.LogitsProcessor.__call__:5
#: transformers.LogitsProcessorList.__call__:3
#: transformers.LogitsWarper.__call__:5
#: transformers.MaxLengthCriteria.__call__:3
#: transformers.MaxTimeCriteria.__call__:3
#: transformers.MinLengthLogitsProcessor.__call__:3
#: transformers.NoBadWordsLogitsProcessor.__call__:3
#: transformers.NoRepeatNGramLogitsProcessor.__call__:3
#: transformers.PrefixConstrainedLogitsProcessor.__call__:3
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:3
#: transformers.StoppingCriteria.__call__:3
#: transformers.StoppingCriteriaList.__call__:3
#: transformers.TemperatureLogitsWarper.__call__:3
#: transformers.TopKLogitsWarper.__call__:3
#: transformers.TopPLogitsWarper.__call__:3
msgid ""
"Indices can be obtained using :class:`~transformers.BertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.BeamScorer.finalize:7 transformers.BeamScorer.process:7
#: transformers.BeamSearchScorer.finalize:7
#: transformers.BeamSearchScorer.process:7
#: transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:7
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:7
#: transformers.FlaxLogitsProcessor.__call__:9
#: transformers.FlaxLogitsProcessorList.__call__:7
#: transformers.FlaxLogitsWarper.__call__:9
#: transformers.FlaxMinLengthLogitsProcessor.__call__:7
#: transformers.FlaxTemperatureLogitsWarper.__call__:7
#: transformers.FlaxTopKLogitsWarper.__call__:7
#: transformers.FlaxTopPLogitsWarper.__call__:7
#: transformers.ForcedBOSTokenLogitsProcessor.__call__:7
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:7
#: transformers.HammingDiversityLogitsProcessor.__call__:7
#: transformers.InfNanRemoveLogitsProcessor.__call__:7
#: transformers.LogitsProcessor.__call__:9
#: transformers.LogitsProcessorList.__call__:7
#: transformers.LogitsWarper.__call__:9
#: transformers.MaxLengthCriteria.__call__:7
#: transformers.MaxTimeCriteria.__call__:7
#: transformers.MinLengthLogitsProcessor.__call__:7
#: transformers.NoBadWordsLogitsProcessor.__call__:7
#: transformers.NoRepeatNGramLogitsProcessor.__call__:7
#: transformers.PrefixConstrainedLogitsProcessor.__call__:7
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:7
#: transformers.StoppingCriteria.__call__:7
#: transformers.StoppingCriteriaList.__call__:7
#: transformers.TemperatureLogitsWarper.__call__:7
#: transformers.TopKLogitsWarper.__call__:7
#: transformers.TopPLogitsWarper.__call__:7
msgid "`What are input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.LogitsProcessor.__call__:11
#: transformers.LogitsWarper.__call__:11
msgid ""
"scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"config.vocab_size)`):"
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:9
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:9
#: transformers.FlaxLogitsProcessor.__call__:11
#: transformers.FlaxLogitsProcessorList.__call__:9
#: transformers.FlaxLogitsWarper.__call__:11
#: transformers.FlaxMinLengthLogitsProcessor.__call__:9
#: transformers.FlaxTemperatureLogitsWarper.__call__:9
#: transformers.FlaxTopKLogitsWarper.__call__:9
#: transformers.FlaxTopPLogitsWarper.__call__:9
#: transformers.ForcedBOSTokenLogitsProcessor.__call__:9
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:9
#: transformers.HammingDiversityLogitsProcessor.__call__:9
#: transformers.InfNanRemoveLogitsProcessor.__call__:9
#: transformers.LogitsProcessor.__call__:11
#: transformers.LogitsProcessorList.__call__:9
#: transformers.LogitsWarper.__call__:11
#: transformers.MinLengthLogitsProcessor.__call__:9
#: transformers.NoBadWordsLogitsProcessor.__call__:9
#: transformers.NoRepeatNGramLogitsProcessor.__call__:9
#: transformers.PrefixConstrainedLogitsProcessor.__call__:9
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:9
#: transformers.TemperatureLogitsWarper.__call__:9
#: transformers.TopKLogitsWarper.__call__:9
#: transformers.TopPLogitsWarper.__call__:9
msgid ""
"Prediction scores of a language modeling head. These can be logits for "
"each vocabulary when not using beam search or log softmax for each "
"vocabulary token when using beam search"
msgstr ""

#: of transformers.FlaxLogitsProcessor.__call__:14
#: transformers.FlaxLogitsWarper.__call__:14
#: transformers.LogitsProcessor.__call__:14
#: transformers.LogitsWarper.__call__:14
msgid "kwargs:"
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:12
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:12
#: transformers.FlaxLogitsProcessor.__call__:14
#: transformers.FlaxLogitsProcessorList.__call__:12
#: transformers.FlaxLogitsWarper.__call__:14
#: transformers.FlaxMinLengthLogitsProcessor.__call__:12
#: transformers.FlaxTemperatureLogitsWarper.__call__:12
#: transformers.FlaxTopKLogitsWarper.__call__:12
#: transformers.FlaxTopPLogitsWarper.__call__:12
#: transformers.ForcedBOSTokenLogitsProcessor.__call__:12
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:12
#: transformers.HammingDiversityLogitsProcessor.__call__:12
#: transformers.InfNanRemoveLogitsProcessor.__call__:12
#: transformers.LogitsProcessor.__call__:14
#: transformers.LogitsProcessorList.__call__:12
#: transformers.LogitsWarper.__call__:14
#: transformers.MinLengthLogitsProcessor.__call__:12
#: transformers.NoBadWordsLogitsProcessor.__call__:12
#: transformers.NoRepeatNGramLogitsProcessor.__call__:12
#: transformers.PrefixConstrainedLogitsProcessor.__call__:12
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:12
#: transformers.TemperatureLogitsWarper.__call__:12
#: transformers.TopKLogitsWarper.__call__:12
#: transformers.TopPLogitsWarper.__call__:12
msgid "Additional logits processor specific kwargs."
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:15
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:15
#: transformers.FlaxLogitsProcessor.__call__:17
#: transformers.FlaxLogitsWarper.__call__:17
#: transformers.FlaxMinLengthLogitsProcessor.__call__:15
#: transformers.FlaxTemperatureLogitsWarper.__call__:15
#: transformers.FlaxTopKLogitsWarper.__call__:15
#: transformers.FlaxTopPLogitsWarper.__call__:15
#: transformers.ForcedBOSTokenLogitsProcessor.__call__:15
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:15
#: transformers.HammingDiversityLogitsProcessor.__call__:15
#: transformers.InfNanRemoveLogitsProcessor.__call__:15
#: transformers.LogitsProcessor.__call__:17
#: transformers.LogitsWarper.__call__:17
#: transformers.MinLengthLogitsProcessor.__call__:15
#: transformers.NoBadWordsLogitsProcessor.__call__:15
#: transformers.NoRepeatNGramLogitsProcessor.__call__:15
#: transformers.PrefixConstrainedLogitsProcessor.__call__:15
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:15
#: transformers.TemperatureLogitsWarper.__call__:15
#: transformers.TopKLogitsWarper.__call__:15
#: transformers.TopPLogitsWarper.__call__:15
msgid "Return:"
msgstr ""

#: of transformers.ForcedBOSTokenLogitsProcessor.__call__:15
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:15
#: transformers.HammingDiversityLogitsProcessor.__call__:15
#: transformers.InfNanRemoveLogitsProcessor.__call__:15
#: transformers.LogitsProcessor.__call__:17
#: transformers.LogitsWarper.__call__:17
#: transformers.MinLengthLogitsProcessor.__call__:15
#: transformers.NoBadWordsLogitsProcessor.__call__:15
#: transformers.NoRepeatNGramLogitsProcessor.__call__:15
#: transformers.PrefixConstrainedLogitsProcessor.__call__:15
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:15
#: transformers.TemperatureLogitsWarper.__call__:15
#: transformers.TopKLogitsWarper.__call__:15
#: transformers.TopPLogitsWarper.__call__:15
msgid ""
":obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`:"
" The processed prediction scores."
msgstr ""

#: of transformers.ForcedBOSTokenLogitsProcessor.__call__:17
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:17
#: transformers.HammingDiversityLogitsProcessor.__call__:17
#: transformers.InfNanRemoveLogitsProcessor.__call__:17
#: transformers.LogitsProcessor.__call__:19
#: transformers.MinLengthLogitsProcessor.__call__:17
#: transformers.NoBadWordsLogitsProcessor.__call__:17
#: transformers.NoRepeatNGramLogitsProcessor.__call__:17
#: transformers.PrefixConstrainedLogitsProcessor.__call__:17
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:17
msgid "Torch method for processing logits."
msgstr ""

#: of transformers.LogitsProcessorList:1
msgid ""
"This class can be used to create a list of "
":class:`~transformers.LogitsProcessor` or "
":class:`~transformers.LogitsWarper` to subsequently process a "
":obj:`scores` input tensor. This class inherits from list and adds a "
"specific `__call__` method to apply each "
":class:`~transformers.LogitsProcessor` or "
":class:`~transformers.LogitsWarper` to the inputs."
msgstr ""

#: of transformers.ForcedBOSTokenLogitsProcessor.__call__:1
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:1
#: transformers.HammingDiversityLogitsProcessor.__call__:1
#: transformers.InfNanRemoveLogitsProcessor.__call__:1
#: transformers.LogitsProcessorList.__call__:1
#: transformers.MaxLengthCriteria.__call__:1
#: transformers.MaxTimeCriteria.__call__:1
#: transformers.MinLengthLogitsProcessor.__call__:1
#: transformers.NoBadWordsLogitsProcessor.__call__:1
#: transformers.NoRepeatNGramLogitsProcessor.__call__:1
#: transformers.PrefixConstrainedLogitsProcessor.__call__:1
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:1
#: transformers.StoppingCriteria.__call__:1
#: transformers.StoppingCriteriaList.__call__:1
#: transformers.TemperatureLogitsWarper.__call__:1
#: transformers.TopKLogitsWarper.__call__:1
#: transformers.TopPLogitsWarper.__call__:1
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.BertTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.BeamScorer.finalize transformers.BeamScorer.process
#: transformers.BeamSearchScorer.finalize transformers.BeamSearchScorer.process
#: transformers.FlaxLogitsProcessorList.__call__
#: transformers.LogitsProcessorList.__call__
#: transformers.MaxLengthCriteria.__call__
#: transformers.MaxTimeCriteria.__call__ transformers.StoppingCriteria.__call__
#: transformers.StoppingCriteriaList.__call__
msgid "Returns"
msgstr ""

#: of transformers.FlaxLogitsProcessorList.__call__:14
#: transformers.LogitsProcessorList.__call__:14
msgid "The processed prediction scores."
msgstr ""

#: of transformers.BeamScorer.finalize transformers.BeamScorer.process
#: transformers.BeamSearchScorer.finalize transformers.BeamSearchScorer.process
#: transformers.FlaxLogitsProcessorList.__call__
#: transformers.LogitsProcessorList.__call__
msgid "Return type"
msgstr ""

#: of transformers.LogitsProcessorList.__call__:15
msgid ":obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`"
msgstr ""

#: of transformers.FlaxLogitsWarper:1 transformers.LogitsWarper:1
msgid ""
"Abstract base class for all logit warpers that can be applied during "
"generation with multinomial sampling."
msgstr ""

#: of transformers.LogitsWarper.__call__:19
#: transformers.TemperatureLogitsWarper.__call__:17
#: transformers.TopKLogitsWarper.__call__:17
#: transformers.TopPLogitsWarper.__call__:17
msgid "Torch method for warping logits."
msgstr ""

#: of transformers.MinLengthLogitsProcessor:1
msgid ""
":class:`transformers.LogitsProcessor` enforcing a min-length by setting "
"EOS probability to 0."
msgstr ""

#: of transformers.FlaxMinLengthLogitsProcessor:3
#: transformers.MinLengthLogitsProcessor:3
msgid ""
"The minimum length below which the score of :obj:`eos_token_id` is set to"
" :obj:`-float(\"Inf\")`."
msgstr ""

#: of transformers.BeamScorer.finalize:17 transformers.BeamScorer.process:17
#: transformers.BeamSearchScorer.finalize:17
#: transformers.BeamSearchScorer.process:17
#: transformers.FlaxMinLengthLogitsProcessor:5
#: transformers.MinLengthLogitsProcessor:5
#: transformers.NoBadWordsLogitsProcessor:7
msgid "The id of the `end-of-sequence` token."
msgstr ""

#: of transformers.ForcedBOSTokenLogitsProcessor.__call__:12
#: transformers.ForcedEOSTokenLogitsProcessor.__call__:12
#: transformers.HammingDiversityLogitsProcessor.__call__:12
#: transformers.InfNanRemoveLogitsProcessor.__call__:12
#: transformers.MinLengthLogitsProcessor.__call__:12
#: transformers.NoBadWordsLogitsProcessor.__call__:12
#: transformers.NoRepeatNGramLogitsProcessor.__call__:12
#: transformers.PrefixConstrainedLogitsProcessor.__call__:12
#: transformers.RepetitionPenaltyLogitsProcessor.__call__:12
#: transformers.TemperatureLogitsWarper.__call__:12
#: transformers.TopKLogitsWarper.__call__:12
#: transformers.TopPLogitsWarper.__call__:12
msgid ""
"Additional logits processor specific kwargs.  Return:     "
":obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`:"
" The processed prediction scores."
msgstr ""

#: of transformers.FlaxTemperatureLogitsWarper:1
#: transformers.TemperatureLogitsWarper:1
msgid ""
":class:`transformers.LogitsWarper` for temperature (exponential scaling "
"output probability distribution)."
msgstr ""

#: of transformers.FlaxTemperatureLogitsWarper:3
#: transformers.TemperatureLogitsWarper:3
msgid "The value used to module the logits distribution."
msgstr ""

#: of transformers.RepetitionPenaltyLogitsProcessor:1
msgid ""
":class:`transformers.LogitsProcessor` enforcing an exponential penalty on"
" repeated sequences."
msgstr ""

#: of transformers.RepetitionPenaltyLogitsProcessor:3
msgid ""
"The parameter for repetition penalty. 1.0 means no penalty. See `this "
"paper <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details."
msgstr ""

#: of transformers.FlaxTopPLogitsWarper:1 transformers.TopPLogitsWarper:1
msgid ""
":class:`transformers.LogitsWarper` that performs top-p, i.e. restricting "
"to top tokens summing to prob_cut_off <= prob_cut_off."
msgstr ""

#: of transformers.FlaxTopPLogitsWarper:4 transformers.TopPLogitsWarper:4
msgid ""
"If set to < 1, only the most probable tokens with probabilities that add "
"up to :obj:`top_p` or higher are kept for generation."
msgstr ""

#: of transformers.FlaxTopKLogitsWarper:5 transformers.FlaxTopPLogitsWarper:7
#: transformers.TopKLogitsWarper:5 transformers.TopPLogitsWarper:7
msgid "All filtered values will be set to this float value."
msgstr ""

#: of transformers.FlaxTopKLogitsWarper:7 transformers.FlaxTopPLogitsWarper:9
#: transformers.TopKLogitsWarper:7 transformers.TopPLogitsWarper:9
msgid "Minimum number of tokens that cannot be filtered."
msgstr ""

#: of transformers.FlaxTopKLogitsWarper:1 transformers.TopKLogitsWarper:1
msgid ""
":class:`transformers.LogitsWarper` that performs top-k, i.e. restricting "
"to the k highest probability elements."
msgstr ""

#: of transformers.FlaxTopKLogitsWarper:3 transformers.TopKLogitsWarper:3
msgid ""
"The number of highest probability vocabulary tokens to keep for "
"top-k-filtering."
msgstr ""

#: of transformers.NoRepeatNGramLogitsProcessor:1
msgid ""
":class:`transformers.LogitsProcessor` that enforces no repetition of "
"n-grams. See `Fairseq "
"<https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345>`__."
msgstr ""

#: of transformers.NoRepeatNGramLogitsProcessor:4
msgid "All ngrams of size :obj:`ngram_size` can only occur once."
msgstr ""

#: of transformers.NoBadWordsLogitsProcessor:1
msgid ""
":class:`transformers.LogitsProcessor` that enforces that specified "
"sequences will never be sampled."
msgstr ""

#: of transformers.NoBadWordsLogitsProcessor:3
msgid ""
"List of list of token ids that are not allowed to be generated. In order "
"to get the tokens of the words that should not appear in the generated "
"text, use :obj:`tokenizer(bad_word, add_prefix_space=True).input_ids`."
msgstr ""

#: of transformers.PrefixConstrainedLogitsProcessor:1
msgid ""
":class:`transformers.LogitsProcessor` that enforces constrained "
"generation and is useful for prefix-conditioned constrained generation. "
"See `Autoregressive Entity Retrieval "
"<https://arxiv.org/abs/2010.00904>`__ for more information."
msgstr ""

#: of transformers.PrefixConstrainedLogitsProcessor:5
msgid ""
"(:obj:`Callable[[int, torch.Tensor], List[int]]`): This function "
"constraints the beam search to allowed tokens only at each step. This "
"function takes 2 arguments :obj:`inputs_ids` and the batch ID "
":obj:`batch_id`. It has to return a list with the allowed tokens for the "
"next generation step conditioned on the previously generated tokens "
":obj:`inputs_ids` and the batch ID :obj:`batch_id`."
msgstr ""

#: of transformers.HammingDiversityLogitsProcessor:1
msgid ""
":class:`transformers.LogitsProcessor` that enforces diverse beam search. "
"Note that this logits processor is only effective for "
":meth:`transformers.PreTrainedModel.group_beam_search`. See `Diverse Beam"
" Search: Decoding Diverse Solutions from Neural Sequence Models "
"<https://arxiv.org/pdf/1610.02424.pdf>`__ for more details."
msgstr ""

#: of transformers.HammingDiversityLogitsProcessor:5
msgid ""
"This value is subtracted from a beam's score if it generates a token same"
" as any beam from other group at a particular time. Note that "
":obj:`diversity_penalty` is only effective if ``group beam search`` is "
"enabled."
msgstr ""

#: of transformers.HammingDiversityLogitsProcessor:8
msgid ""
"Number of beams used for group beam search. See `this paper "
"<https://arxiv.org/pdf/1610.02424.pdf>`__ for more details."
msgstr ""

#: of transformers.BeamSearchScorer:27
#: transformers.HammingDiversityLogitsProcessor:11
msgid ""
"Number of groups to divide :obj:`num_beams` into in order to ensure "
"diversity among different groups of beams. See `this paper "
"<https://arxiv.org/pdf/1610.02424.pdf>`__ for more details."
msgstr ""

#: of transformers.ForcedBOSTokenLogitsProcessor:1
msgid ""
":class:`~transformers.LogitsProcessor` that enforces the specified token "
"as the first generated token."
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor:3
#: transformers.ForcedBOSTokenLogitsProcessor:3
msgid "The id of the token to force as the first generated token."
msgstr ""

#: of transformers.ForcedEOSTokenLogitsProcessor:1
msgid ""
":class:`~transformers.LogitsProcessor` that enforces the specified token "
"as the last generated token when :obj:`max_length` is reached."
msgstr ""

#: of transformers.BeamSearchScorer:11
#: transformers.FlaxForcedEOSTokenLogitsProcessor:4
#: transformers.ForcedEOSTokenLogitsProcessor:4
msgid "The maximum length of the sequence to be generated."
msgstr ""

#: of transformers.FlaxForcedEOSTokenLogitsProcessor:6
#: transformers.ForcedEOSTokenLogitsProcessor:6
msgid ""
"The id of the token to force as the last generated token when "
":obj:`max_length` is reached."
msgstr ""

#: of transformers.InfNanRemoveLogitsProcessor:1
msgid ""
":class:`~transformers.LogitsProcessor` that removes all :obj:`nan` and "
":obj:`inf` values to avoid the generation method to fail. Note that using"
" the logits processor should only be used if necessary since it can slow "
"down the generation method. :obj:`max_length` is reached."
msgstr ""

#: of transformers.FlaxLogitsProcessor.__call__:8
#: transformers.FlaxLogitsWarper.__call__:8
msgid ""
"input_ids (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"sequence_length)`):"
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:3
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:3
#: transformers.FlaxLogitsProcessor.__call__:5
#: transformers.FlaxLogitsProcessorList.__call__:3
#: transformers.FlaxLogitsWarper.__call__:5
#: transformers.FlaxMinLengthLogitsProcessor.__call__:3
#: transformers.FlaxTemperatureLogitsWarper.__call__:3
#: transformers.FlaxTopKLogitsWarper.__call__:3
#: transformers.FlaxTopPLogitsWarper.__call__:3
msgid ""
"Indices can be obtained using :class:`~transformers.PreTrainedTokenizer`."
" See :meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.FlaxLogitsProcessor.__call__:11
#: transformers.FlaxLogitsWarper.__call__:11
msgid ""
"scores (:obj:`jnp.ndarray` of shape :obj:`(batch_size, "
"config.vocab_size)`):"
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:15
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:15
#: transformers.FlaxLogitsProcessor.__call__:17
#: transformers.FlaxLogitsWarper.__call__:17
#: transformers.FlaxMinLengthLogitsProcessor.__call__:15
#: transformers.FlaxTemperatureLogitsWarper.__call__:15
#: transformers.FlaxTopKLogitsWarper.__call__:15
#: transformers.FlaxTopPLogitsWarper.__call__:15
msgid ""
":obj:`jnp.ndarray` of shape :obj:`(batch_size, config.vocab_size)`: The "
"processed prediction scores."
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:17
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:17
#: transformers.FlaxLogitsProcessor.__call__:19
#: transformers.FlaxMinLengthLogitsProcessor.__call__:17
msgid "Flax method for processing logits."
msgstr ""

#: of transformers.FlaxLogitsProcessorList:1
msgid ""
"This class can be used to create a list of "
":class:`~transformers.FlaxLogitsProcessor` or "
":class:`~transformers.FlaxLogitsWarper` to subsequently process a "
":obj:`scores` input tensor. This class inherits from list and adds a "
"specific `__call__` method to apply each "
":class:`~transformers.FlaxLogitsProcessor` or "
":class:`~transformers.FlaxLogitsWarper` to the inputs."
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:1
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:1
#: transformers.FlaxLogitsProcessorList.__call__:1
#: transformers.FlaxMinLengthLogitsProcessor.__call__:1
#: transformers.FlaxTemperatureLogitsWarper.__call__:1
#: transformers.FlaxTopKLogitsWarper.__call__:1
#: transformers.FlaxTopPLogitsWarper.__call__:1
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using :class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.FlaxLogitsProcessorList.__call__:15
msgid ":obj:`jnp.ndarray` of shape :obj:`(batch_size, config.vocab_size)`"
msgstr ""

#: of transformers.FlaxLogitsWarper.__call__:19
#: transformers.FlaxTemperatureLogitsWarper.__call__:17
#: transformers.FlaxTopKLogitsWarper.__call__:17
#: transformers.FlaxTopPLogitsWarper.__call__:17
msgid "Flax method for warping logits."
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor.__call__:12
#: transformers.FlaxForcedEOSTokenLogitsProcessor.__call__:12
#: transformers.FlaxMinLengthLogitsProcessor.__call__:12
#: transformers.FlaxTemperatureLogitsWarper.__call__:12
#: transformers.FlaxTopKLogitsWarper.__call__:12
#: transformers.FlaxTopPLogitsWarper.__call__:12
msgid ""
"Additional logits processor specific kwargs.  Return:     "
":obj:`jnp.ndarray` of shape :obj:`(batch_size, config.vocab_size)`: The "
"processed prediction scores."
msgstr ""

#: of transformers.FlaxForcedBOSTokenLogitsProcessor:1
msgid ""
":class:`~transformers.FlaxLogitsProcessor` that enforces the specified "
"token as the first generated token."
msgstr ""

#: of transformers.FlaxForcedEOSTokenLogitsProcessor:1
msgid ""
":class:`~transformers.FlaxLogitsProcessor` that enforces the specified "
"token as the last generated token when :obj:`max_length` is reached."
msgstr ""

#: of transformers.FlaxMinLengthLogitsProcessor:1
msgid ""
":class:`transformers.FlaxLogitsProcessor` enforcing a min-length by "
"setting EOS probability to 0."
msgstr ""

#: ../../source/internal/generation_utils.rst:200
msgid "StoppingCriteria"
msgstr ""

#: ../../source/internal/generation_utils.rst:202
msgid ""
"A :class:`~transformers.StoppingCriteria` can be used to change when to "
"stop generation (other than EOS token)."
msgstr ""

#: of transformers.StoppingCriteria:1
msgid ""
"Abstract base class for all stopping criteria that can be applied during "
"generation."
msgstr ""

#: of transformers.MaxLengthCriteria.__call__:9
#: transformers.MaxTimeCriteria.__call__:9
#: transformers.StoppingCriteria.__call__:9
#: transformers.StoppingCriteriaList.__call__:9
msgid ""
"Prediction scores of a language modeling head. These can be scores for "
"each vocabulary token before SoftMax or scores for each vocabulary token "
"after SoftMax."
msgstr ""

#: of transformers.MaxLengthCriteria.__call__:12
#: transformers.MaxTimeCriteria.__call__:12
#: transformers.StoppingCriteria.__call__:12
#: transformers.StoppingCriteriaList.__call__:12
msgid "Additional stopping criteria specific kwargs."
msgstr ""

#: of transformers.MaxLengthCriteria.__call__:14
#: transformers.MaxTimeCriteria.__call__:14
#: transformers.StoppingCriteria.__call__:14
#: transformers.StoppingCriteriaList.__call__:14
msgid ""
":obj:`bool`. :obj:`False` indicates we should continue, :obj:`True` "
"indicates we should stop."
msgstr ""

#: of transformers.MaxLengthCriteria:1
msgid ""
"This class can be used to stop generation whenever the full generated "
"number of tokens exceeds :obj:`max_length`. Keep in mind for decoder-only"
" type of transformers, this will include the initial prompted tokens."
msgstr ""

#: of transformers.MaxLengthCriteria:4
msgid "The maximum length that the output sequence can have in number of tokens."
msgstr ""

#: of transformers.MaxTimeCriteria:1
msgid ""
"This class can be used to stop generation whenever the full generation "
"exceeds some amount of time. By default, the time will start being "
"counted when you initialize this function. You can override this by "
"passing an :obj:`initial_time`."
msgstr ""

#: of transformers.MaxTimeCriteria:5
msgid "The maximum allowed time in seconds for the generation."
msgstr ""

#: of transformers.MaxTimeCriteria:7
msgid "The start of the generation allowed time."
msgstr ""

#: ../../source/internal/generation_utils.rst:217
msgid "BeamSearch"
msgstr ""

#: of transformers.BeamScorer:1
msgid ""
"Abstract base class for all beam scorers that are used for "
":meth:`~transformers.PreTrainedModel.beam_search` and "
":meth:`~transformers.PreTrainedModel.beam_sample`."
msgstr ""

#: of transformers.BeamScorer.finalize:1 transformers.BeamScorer.process:1
#: transformers.BeamSearchScorer.finalize:1
#: transformers.BeamSearchScorer.process:1
msgid ""
"Indices of input sequence tokens in the vocabulary.  Indices can be "
"obtained using any class inheriting from "
":class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details.  `What are"
" input IDs? <../glossary.html#input-ids>`__"
msgstr ""

#: of transformers.BeamScorer.finalize:3 transformers.BeamScorer.process:3
#: transformers.BeamSearchScorer.finalize:3
#: transformers.BeamSearchScorer.process:3
msgid ""
"Indices can be obtained using any class inheriting from "
":class:`~transformers.PreTrainedTokenizer`. See "
":meth:`transformers.PreTrainedTokenizer.encode` and "
":meth:`transformers.PreTrainedTokenizer.__call__` for details."
msgstr ""

#: of transformers.BeamScorer.finalize:9
#: transformers.BeamSearchScorer.finalize:9
msgid "The final scores of all non-finished beams."
msgstr ""

#: of transformers.BeamScorer.finalize:11
#: transformers.BeamSearchScorer.finalize:11
msgid "The last tokens to be added to the non-finished beam_hypotheses."
msgstr ""

#: of transformers.BeamScorer.finalize:13
#: transformers.BeamSearchScorer.finalize:13
msgid ""
"The beam indices indicating to which beam the :obj:`final_beam_tokens` "
"shall be added."
msgstr ""

#: of transformers.BeamScorer.finalize:15 transformers.BeamScorer.process:15
#: transformers.BeamSearchScorer.finalize:15
#: transformers.BeamSearchScorer.process:15
msgid "The id of the `padding` token."
msgstr ""

#: of transformers.BeamScorer.finalize:23
#: transformers.BeamSearchScorer.finalize:23
msgid ""
":obj:`torch.LongTensor` of shape :obj:`(batch_size * "
"num_return_sequences, sequence_length)`"
msgstr ""

#: of transformers.BeamScorer.process:9 transformers.BeamSearchScorer.process:9
msgid ""
"Current scores of the top :obj:`2 * num_beams` non-finished beam "
"hypotheses."
msgstr ""

#: of transformers.BeamScorer.process:11
#: transformers.BeamSearchScorer.process:11
msgid ""
":obj:`input_ids` of the tokens corresponding to the top :obj:`2 * "
"num_beams` non-finished beam hypotheses."
msgstr ""

#: of transformers.BeamScorer.process:13
#: transformers.BeamSearchScorer.process:13
msgid ""
"Beam indices indicating to which beam hypothesis the :obj:`next_tokens` "
"correspond."
msgstr ""

#: of transformers.BeamScorer.process:20
#: transformers.BeamSearchScorer.process:20
msgid ""
"A dictionary composed of the fields as defined above:      - "
"**next_beam_scores** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size"
" * num_beams)`) -- Updated       scores of all non-finished beams.     - "
"**next_beam_tokens** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size"
" * num_beams)`) -- Next tokens       to be added to the non-finished "
"beam_hypotheses.     - **next_beam_indices** (:obj:`torch.FloatTensor` of"
" shape :obj:`(batch_size * num_beams)`) -- Beam indices       indicating "
"to which beam the next tokens shall be added."
msgstr ""

#: of transformers.BeamScorer.process:22
#: transformers.BeamSearchScorer.process:22
msgid "A dictionary composed of the fields as defined above:"
msgstr ""

#: of transformers.BeamScorer.process:24
#: transformers.BeamSearchScorer.process:24
msgid ""
"**next_beam_scores** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size"
" * num_beams)`) -- Updated scores of all non-finished beams."
msgstr ""

#: of transformers.BeamScorer.process:26
#: transformers.BeamSearchScorer.process:26
msgid ""
"**next_beam_tokens** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size"
" * num_beams)`) -- Next tokens to be added to the non-finished "
"beam_hypotheses."
msgstr ""

#: of transformers.BeamScorer.process:28
#: transformers.BeamSearchScorer.process:28
msgid ""
"**next_beam_indices** (:obj:`torch.FloatTensor` of shape "
":obj:`(batch_size * num_beams)`) -- Beam indices indicating to which beam"
" the next tokens shall be added."
msgstr ""

#: of transformers.BeamScorer.process:30
#: transformers.BeamSearchScorer.process:30
msgid ":obj:`UserDict`"
msgstr ""

#: of transformers.BeamSearchScorer:1
msgid ""
":class:`transformers.BeamScorer` implementing standard beam search "
"decoding."
msgstr ""

#: of transformers.BeamSearchScorer:3
msgid ""
"Adapted in part from `Facebook's XLM beam search code "
"<https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529>`__."
msgstr ""

#: of transformers.BeamSearchScorer:6
msgid ""
"Reference for the diverse beam search algorithm and implementation "
"`Ashwin Kalyan's DBS implementation "
"<https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua>`__"
msgstr ""

#: of transformers.BeamSearchScorer:9
msgid ""
"Batch Size of :obj:`input_ids` for which standard beam search decoding is"
" run in parallel."
msgstr ""

#: of transformers.BeamSearchScorer:13
msgid "Number of beams for beam search."
msgstr ""

#: of transformers.BeamSearchScorer:15
msgid ""
"Defines the device type (*e.g.*, :obj:`\"cpu\"` or :obj:`\"cuda\"`) on "
"which this instance of :obj:`BeamSearchScorer` will be allocated."
msgstr ""

#: of transformers.BeamSearchScorer:18
msgid ""
"Exponential penalty to the length. 1.0 means no penalty. Set to values < "
"1.0 in order to encourage the model to generate shorter sequences, to a "
"value > 1.0 in order to encourage the model to produce longer sequences."
msgstr ""

#: of transformers.BeamSearchScorer:22
msgid ""
"Whether to stop the beam search when at least ``num_beams`` sentences are"
" finished per batch or not."
msgstr ""

#: of transformers.BeamSearchScorer:24
msgid ""
"The number of beam hypotheses that shall be returned upon calling "
":meth:`~transformer.BeamSearchScorer.finalize`."
msgstr ""

#: ../../source/internal/generation_utils.rst:226
msgid "Utilities"
msgstr ""

#: of transformers.tf_top_k_top_p_filtering:1
#: transformers.top_k_top_p_filtering:1
msgid ""
"Filter a distribution of logits using top-k and/or nucleus (top-p) "
"filtering"
msgstr ""

#: of transformers.tf_top_k_top_p_filtering:3
#: transformers.top_k_top_p_filtering:3
msgid "logits distribution shape (batch size, vocabulary size)"
msgstr ""

#: of transformers.tf_top_k_top_p_filtering:4
#: transformers.top_k_top_p_filtering:4
msgid "keep only top k tokens with highest probability (top-k filtering)."
msgstr ""

#: of transformers.tf_top_k_top_p_filtering:5
#: transformers.top_k_top_p_filtering:5
msgid ""
"keep the top tokens with cumulative probability >= top_p (nucleus "
"filtering). Nucleus filtering is described in Holtzman et al. "
"(http://arxiv.org/abs/1904.09751)"
msgstr ""

#: of transformers.tf_top_k_top_p_filtering:9
#: transformers.top_k_top_p_filtering:9
msgid "From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317"
msgstr ""


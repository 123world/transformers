# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/internal/modeling_utils.rst:14
msgid "Custom Layers and Utilities"
msgstr ""

#: ../../source/internal/modeling_utils.rst:16
msgid ""
"This page lists all the custom layers used by the library, as well as the"
" utility functions it provides for modeling."
msgstr ""

#: ../../source/internal/modeling_utils.rst:18
msgid ""
"Most of those are only useful if you are studying the code of the models "
"in the library."
msgstr ""

#: ../../source/internal/modeling_utils.rst:22
msgid "Pytorch custom modules"
msgstr ""

#: of transformers.modeling_tf_utils.TFConv1D:1
#: transformers.modeling_utils.Conv1D:1
msgid ""
"1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and "
"also used in GPT-2)."
msgstr ""

#: of transformers.modeling_tf_utils.TFConv1D:3
#: transformers.modeling_utils.Conv1D:3
msgid "Basically works like a linear layer but the weights are transposed."
msgstr ""

#: of transformers.apply_chunking_to_forward
#: transformers.modeling_tf_utils.TFConv1D
#: transformers.modeling_tf_utils.TFSequenceSummary
#: transformers.modeling_tf_utils.TFSequenceSummary.call
#: transformers.modeling_tf_utils.TFSharedEmbeddings
#: transformers.modeling_tf_utils.TFSharedEmbeddings.call
#: transformers.modeling_tf_utils.get_initializer
#: transformers.modeling_tf_utils.keras_serializable
#: transformers.modeling_tf_utils.shape_list transformers.modeling_utils.Conv1D
#: transformers.modeling_utils.PoolerAnswerClass
#: transformers.modeling_utils.PoolerAnswerClass.forward
#: transformers.modeling_utils.PoolerEndLogits
#: transformers.modeling_utils.PoolerEndLogits.forward
#: transformers.modeling_utils.PoolerStartLogits
#: transformers.modeling_utils.PoolerStartLogits.forward
#: transformers.modeling_utils.SQuADHead
#: transformers.modeling_utils.SequenceSummary
#: transformers.modeling_utils.SequenceSummary.forward
#: transformers.modeling_utils.SquadHeadOutput
#: transformers.modeling_utils.find_pruneable_heads_and_indices
#: transformers.modeling_utils.prune_conv1d_layer
#: transformers.modeling_utils.prune_layer
#: transformers.modeling_utils.prune_linear_layer
msgid "Parameters"
msgstr ""

#: of transformers.modeling_tf_utils.TFConv1D:5
#: transformers.modeling_utils.Conv1D:5
msgid "The number of output features."
msgstr ""

#: of transformers.modeling_tf_utils.TFConv1D:7
#: transformers.modeling_utils.Conv1D:7
msgid "The number of input features."
msgstr ""

#: of transformers.modeling_utils.PoolerStartLogits:1
msgid "Compute SQuAD start logits from sequence hidden states."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass:3
#: transformers.modeling_utils.PoolerStartLogits:3
msgid ""
"The config used by the model, will be used to grab the :obj:`hidden_size`"
" of the model."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:1
#: transformers.modeling_utils.PoolerEndLogits.forward:1
#: transformers.modeling_utils.PoolerStartLogits.forward:1
msgid "The final hidden states of the model."
msgstr ""

#: of transformers.modeling_utils.PoolerEndLogits.forward:7
#: transformers.modeling_utils.PoolerStartLogits.forward:3
#: transformers.modeling_utils.SQuADHead.forward:13
msgid ""
"Mask for tokens at invalid position, such as query and special symbols "
"(PAD, SEP, CLS). 1.0 means token should be masked."
msgstr ""

#: of transformers.apply_chunking_to_forward
#: transformers.modeling_tf_utils.TFSequenceSummary.call
#: transformers.modeling_tf_utils.TFSharedEmbeddings.call
#: transformers.modeling_tf_utils.get_initializer
#: transformers.modeling_tf_utils.keras_serializable
#: transformers.modeling_tf_utils.shape_list
#: transformers.modeling_utils.PoolerAnswerClass.forward
#: transformers.modeling_utils.PoolerEndLogits.forward
#: transformers.modeling_utils.PoolerStartLogits.forward
#: transformers.modeling_utils.SQuADHead.forward
#: transformers.modeling_utils.SequenceSummary.forward
#: transformers.modeling_utils.find_pruneable_heads_and_indices
#: transformers.modeling_utils.prune_conv1d_layer
#: transformers.modeling_utils.prune_layer
#: transformers.modeling_utils.prune_linear_layer
msgid "Returns"
msgstr ""

#: of transformers.modeling_utils.PoolerStartLogits.forward:7
msgid "The start logits for SQuAD."
msgstr ""

#: of transformers.apply_chunking_to_forward
#: transformers.modeling_tf_utils.TFSharedEmbeddings.call
#: transformers.modeling_tf_utils.get_initializer
#: transformers.modeling_tf_utils.shape_list
#: transformers.modeling_utils.PoolerAnswerClass.forward
#: transformers.modeling_utils.PoolerEndLogits.forward
#: transformers.modeling_utils.PoolerStartLogits.forward
#: transformers.modeling_utils.SQuADHead.forward
#: transformers.modeling_utils.SequenceSummary.forward
#: transformers.modeling_utils.find_pruneable_heads_and_indices
#: transformers.modeling_utils.prune_conv1d_layer
#: transformers.modeling_utils.prune_layer
#: transformers.modeling_utils.prune_linear_layer
msgid "Return type"
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:16
#: transformers.modeling_utils.PoolerEndLogits.forward:17
#: transformers.modeling_utils.PoolerStartLogits.forward:8
#: transformers.modeling_utils.SequenceSummary.forward:10
msgid ":obj:`torch.FloatTensor`"
msgstr ""

#: of transformers.modeling_utils.PoolerEndLogits:1
msgid "Compute SQuAD end logits from sequence hidden states."
msgstr ""

#: of transformers.modeling_utils.PoolerEndLogits:3
#: transformers.modeling_utils.SQuADHead:3
msgid ""
"The config used by the model, will be used to grab the :obj:`hidden_size`"
" of the model and the :obj:`layer_norm_eps` to use."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:3
#: transformers.modeling_utils.PoolerEndLogits.forward:3
msgid "The hidden states of the first tokens for the labeled span."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:5
#: transformers.modeling_utils.PoolerEndLogits.forward:5
msgid "The position of the first token for the labeled span."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:12
#: transformers.modeling_utils.PoolerEndLogits.forward:13
msgid ""
"One of ``start_states`` or ``start_positions`` should be not obj:`None`. "
"If both are set, ``start_positions`` overrides ``start_states``."
msgstr ""

#: of transformers.modeling_utils.PoolerEndLogits.forward:16
msgid "The end logits for SQuAD."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass:1
msgid ""
"Compute SQuAD 2.0 answer class from classification and start tokens "
"hidden states."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:7
#: transformers.modeling_utils.SQuADHead.forward:9
msgid ""
"Position of the CLS token for each sentence in the batch. If :obj:`None`,"
" takes the last token."
msgstr ""

#: of transformers.modeling_utils.PoolerAnswerClass.forward:15
msgid "The SQuAD 2.0 answer class."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:1
msgid ""
"Base class for outputs of question answering models using a "
":class:`~transformers.modeling_utils.SQuADHead`."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:3
msgid ""
"Classification loss as the sum of start token, end token (and "
"is_impossible if provided) classification losses."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:6
msgid ""
"Log probabilities for the top config.start_n_top start token "
"possibilities (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:8
msgid ""
"Indices for the top config.start_n_top start token possibilities (beam-"
"search)."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:10
msgid ""
"Log probabilities for the top ``config.start_n_top * config.end_n_top`` "
"end token possibilities (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:13
msgid ""
"Indices for the top ``config.start_n_top * config.end_n_top`` end token "
"possibilities (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SquadHeadOutput:15
msgid "Log probabilities for the ``is_impossible`` label of the answers."
msgstr ""

#: of transformers.modeling_utils.SQuADHead:1
msgid "A SQuAD head inspired by XLNet."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:17
msgid "Args:"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:2
msgid ""
"hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, "
"seq_len, hidden_size)`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:3
msgid "Final hidden states of the model on the sequence tokens."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:4
msgid ""
"start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, "
"`optional`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:5
msgid "Positions of the first token for the labeled span."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:6
msgid ""
"end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, "
"`optional`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:7
msgid "Positions of the last token for the labeled span."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:8
msgid ""
"cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, "
"`optional`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:10
msgid ""
"is_impossible (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, "
"`optional`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:11
msgid "Whether the question has a possible answer in the paragraph or not."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:13
msgid ""
"p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, "
"`optional`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:17
msgid "return_dict (:obj:`bool`, `optional`, defaults to :obj:`False`):"
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:16
msgid ""
"Whether or not to return a :class:`~transformers.file_utils.ModelOutput` "
"instead of a plain tuple."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:19
msgid ""
"A :class:`~transformers.modeling_utils.SquadHeadOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.<class "
"'transformers.configuration_utils.PretrainedConfig'>`) and inputs.  - "
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned if both :obj:`start_positions` and :obj:`end_positions` are "
"provided) -- Classification loss as the sum of start token, end token "
"(and is_impossible if provided) classification   losses. - "
"**start_top_log_probs** (``torch.FloatTensor`` of shape ``(batch_size, "
"config.start_n_top)``, `optional`, returned if ``start_positions`` or "
"``end_positions`` is not provided) -- Log probabilities for the top "
"config.start_n_top start token possibilities (beam-search). - "
"**start_top_index** (``torch.LongTensor`` of shape ``(batch_size, "
"config.start_n_top)``, `optional`, returned if ``start_positions`` or "
"``end_positions`` is not provided) -- Indices for the top "
"config.start_n_top start token possibilities (beam-search). - "
"**end_top_log_probs** (``torch.FloatTensor`` of shape ``(batch_size, "
"config.start_n_top * config.end_n_top)``, `optional`, returned if "
"``start_positions`` or ``end_positions`` is not provided) -- Log "
"probabilities for the top ``config.start_n_top * config.end_n_top`` end "
"token possibilities   (beam-search). - **end_top_index** "
"(``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * "
"config.end_n_top)``, `optional`, returned if ``start_positions`` or "
"``end_positions`` is not provided) -- Indices for the top "
"``config.start_n_top * config.end_n_top`` end token possibilities (beam-"
"search). - **cls_logits** (``torch.FloatTensor`` of shape "
"``(batch_size,)``, `optional`, returned if ``start_positions`` or "
"``end_positions`` is not provided) -- Log probabilities for the "
"``is_impossible`` label of the answers."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:19
msgid ""
"A :class:`~transformers.modeling_utils.SquadHeadOutput` or a tuple of "
":obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when "
"``config.return_dict=False``) comprising various elements depending on "
"the configuration (:class:`~transformers.<class "
"'transformers.configuration_utils.PretrainedConfig'>`) and inputs."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:23
msgid ""
"**loss** (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, "
"returned if both :obj:`start_positions` and :obj:`end_positions` are "
"provided) -- Classification loss as the sum of start token, end token "
"(and is_impossible if provided) classification losses."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:25
msgid ""
"**start_top_log_probs** (``torch.FloatTensor`` of shape ``(batch_size, "
"config.start_n_top)``, `optional`, returned if ``start_positions`` or "
"``end_positions`` is not provided) -- Log probabilities for the top "
"config.start_n_top start token possibilities (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:26
msgid ""
"**start_top_index** (``torch.LongTensor`` of shape ``(batch_size, "
"config.start_n_top)``, `optional`, returned if ``start_positions`` or "
"``end_positions`` is not provided) -- Indices for the top "
"config.start_n_top start token possibilities (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:27
msgid ""
"**end_top_log_probs** (``torch.FloatTensor`` of shape ``(batch_size, "
"config.start_n_top * config.end_n_top)``, `optional`, returned if "
"``start_positions`` or ``end_positions`` is not provided) -- Log "
"probabilities for the top ``config.start_n_top * config.end_n_top`` end "
"token possibilities (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:29
msgid ""
"**end_top_index** (``torch.LongTensor`` of shape ``(batch_size, "
"config.start_n_top * config.end_n_top)``, `optional`, returned if "
"``start_positions`` or ``end_positions`` is not provided) -- Indices for "
"the top ``config.start_n_top * config.end_n_top`` end token possibilities"
" (beam-search)."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:30
msgid ""
"**cls_logits** (``torch.FloatTensor`` of shape ``(batch_size,)``, "
"`optional`, returned if ``start_positions`` or ``end_positions`` is not "
"provided) -- Log probabilities for the ``is_impossible`` label of the "
"answers."
msgstr ""

#: of transformers.modeling_utils.SQuADHead.forward:31
msgid ""
":class:`~transformers.modeling_utils.SquadHeadOutput` or "
":obj:`tuple(torch.FloatTensor)`"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:1
#: transformers.modeling_utils.SequenceSummary:1
#: transformers.modeling_utils.SequenceSummary.forward:1
msgid "Compute a single vector summary of a sequence hidden states."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:3
#: transformers.modeling_utils.SequenceSummary:3
msgid ""
"The config used by the model. Relevant arguments in the config class of "
"the model are (refer to the actual config class of your model for the "
"default values it uses):  - **summary_type** (:obj:`str`) -- The method "
"to use to make this summary. Accepted values are:      - :obj:`\"last\"` "
"-- Take the last token hidden state (like XLNet)     - :obj:`\"first\"` "
"-- Take the first token hidden state (like Bert)     - :obj:`\"mean\"` --"
" Take the mean of all tokens hidden states     - :obj:`\"cls_index\"` -- "
"Supply a Tensor of classification token position (GPT/GPT-2)     - "
":obj:`\"attn\"` -- Not implemented now, use multi-head attention  - "
"**summary_use_proj** (:obj:`bool`) -- Add a projection after the vector "
"extraction. - **summary_proj_to_labels** (:obj:`bool`) -- If :obj:`True`,"
" the projection outputs to   :obj:`config.num_labels` classes (otherwise "
"to :obj:`config.hidden_size`). - **summary_activation** "
"(:obj:`Optional[str]`) -- Set to :obj:`\"tanh\"` to add a tanh activation"
" to the   output, another string or :obj:`None` will add no activation. -"
" **summary_first_dropout** (:obj:`float`) -- Optional dropout probability"
" before the projection and   activation. - **summary_last_dropout** "
"(:obj:`float`)-- Optional dropout probability after the projection and   "
"activation."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:3
#: transformers.modeling_utils.SequenceSummary:3
msgid ""
"The config used by the model. Relevant arguments in the config class of "
"the model are (refer to the actual config class of your model for the "
"default values it uses):"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:6
#: transformers.modeling_utils.SequenceSummary:6
msgid ""
"**summary_type** (:obj:`str`) -- The method to use to make this summary. "
"Accepted values are:"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:8
#: transformers.modeling_utils.SequenceSummary:8
msgid ":obj:`\"last\"` -- Take the last token hidden state (like XLNet)"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:9
#: transformers.modeling_utils.SequenceSummary:9
msgid ":obj:`\"first\"` -- Take the first token hidden state (like Bert)"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:10
#: transformers.modeling_utils.SequenceSummary:10
msgid ":obj:`\"mean\"` -- Take the mean of all tokens hidden states"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:11
#: transformers.modeling_utils.SequenceSummary:11
msgid ""
":obj:`\"cls_index\"` -- Supply a Tensor of classification token position "
"(GPT/GPT-2)"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:12
#: transformers.modeling_utils.SequenceSummary:12
msgid ":obj:`\"attn\"` -- Not implemented now, use multi-head attention"
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:14
#: transformers.modeling_utils.SequenceSummary:14
msgid ""
"**summary_use_proj** (:obj:`bool`) -- Add a projection after the vector "
"extraction."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:15
#: transformers.modeling_utils.SequenceSummary:15
msgid ""
"**summary_proj_to_labels** (:obj:`bool`) -- If :obj:`True`, the "
"projection outputs to :obj:`config.num_labels` classes (otherwise to "
":obj:`config.hidden_size`)."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:17
#: transformers.modeling_utils.SequenceSummary:17
msgid ""
"**summary_activation** (:obj:`Optional[str]`) -- Set to :obj:`\"tanh\"` "
"to add a tanh activation to the output, another string or :obj:`None` "
"will add no activation."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:19
#: transformers.modeling_utils.SequenceSummary:19
msgid ""
"**summary_first_dropout** (:obj:`float`) -- Optional dropout probability "
"before the projection and activation."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary:21
#: transformers.modeling_utils.SequenceSummary:21
msgid ""
"**summary_last_dropout** (:obj:`float`)-- Optional dropout probability "
"after the projection and activation."
msgstr ""

#: of transformers.modeling_utils.SequenceSummary.forward:3
msgid "The hidden states of the last layer."
msgstr ""

#: of transformers.modeling_utils.SequenceSummary.forward:5
msgid ""
"Used if :obj:`summary_type == \"cls_index\"` and takes the last token of "
"the sequence as classification token."
msgstr ""

#: of transformers.modeling_utils.SequenceSummary.forward:9
msgid "The summary of the sequence hidden states."
msgstr ""

#: ../../source/internal/modeling_utils.rst:45
msgid "PyTorch Helper Functions"
msgstr ""

#: of transformers.apply_chunking_to_forward:1
msgid ""
"This function chunks the :obj:`input_tensors` into smaller input tensor "
"parts of size :obj:`chunk_size` over the dimension :obj:`chunk_dim`. It "
"then applies a layer :obj:`forward_fn` to each chunk independently to "
"save memory."
msgstr ""

#: of transformers.apply_chunking_to_forward:4
msgid ""
"If the :obj:`forward_fn` is independent across the :obj:`chunk_dim` this "
"function will yield the same result as directly applying "
":obj:`forward_fn` to :obj:`input_tensors`."
msgstr ""

#: of transformers.apply_chunking_to_forward:7
msgid "The forward function of the model."
msgstr ""

#: of transformers.apply_chunking_to_forward:9
msgid ""
"The chunk size of a chunked tensor: :obj:`num_chunks = "
"len(input_tensors[0]) / chunk_size`."
msgstr ""

#: of transformers.apply_chunking_to_forward:11
msgid "The dimension over which the :obj:`input_tensors` should be chunked."
msgstr ""

#: of transformers.apply_chunking_to_forward:13
msgid "The input tensors of ``forward_fn`` which will be chunked"
msgstr ""

#: of transformers.apply_chunking_to_forward:16
msgid ""
"A tensor with the same shape as the :obj:`forward_fn` would have given if"
" applied`."
msgstr ""

#: of transformers.apply_chunking_to_forward:17
msgid ":obj:`torch.Tensor`"
msgstr ""

#: of transformers.apply_chunking_to_forward:19
msgid "Examples::"
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:1
msgid ""
"Finds the heads and their indices taking :obj:`already_pruned_heads` into"
" account."
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:3
msgid "List of the indices of heads to prune."
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:5
msgid "The number of heads in the model."
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:7
msgid "The size of each head."
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:9
msgid "A set of already pruned heads."
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:12
msgid "A tuple with the remaining heads and their corresponding indices."
msgstr ""

#: of transformers.modeling_utils.find_pruneable_heads_and_indices:13
msgid ":obj:`Tuple[Set[int], torch.LongTensor]`"
msgstr ""

#: of transformers.modeling_utils.prune_layer:1
msgid "Prune a Conv1D or linear layer to keep only entries in index."
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:4
#: transformers.modeling_utils.prune_layer:3
#: transformers.modeling_utils.prune_linear_layer:3
msgid "Used to remove heads."
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:6
#: transformers.modeling_utils.prune_layer:5
#: transformers.modeling_utils.prune_linear_layer:5
msgid "The layer to prune."
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:8
#: transformers.modeling_utils.prune_layer:7
#: transformers.modeling_utils.prune_linear_layer:7
msgid "The indices to keep in the layer."
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:10
#: transformers.modeling_utils.prune_layer:9
#: transformers.modeling_utils.prune_linear_layer:9
msgid "The dimension on which to keep the indices."
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:13
#: transformers.modeling_utils.prune_layer:12
#: transformers.modeling_utils.prune_linear_layer:12
msgid "The pruned layer as a new layer with :obj:`requires_grad=True`."
msgstr ""

#: of transformers.modeling_utils.prune_layer:14
msgid ":obj:`torch.nn.Linear` or :class:`~transformers.modeling_utils.Conv1D`"
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:1
msgid ""
"Prune a Conv1D layer to keep only entries in index. A Conv1D work as a "
"Linear layer (see e.g. BERT) but the weights are transposed."
msgstr ""

#: of transformers.modeling_utils.prune_conv1d_layer:14
msgid ":class:`~transformers.modeling_utils.Conv1D`"
msgstr ""

#: of transformers.modeling_utils.prune_linear_layer:1
msgid "Prune a linear layer to keep only entries in index."
msgstr ""

#: of transformers.modeling_utils.prune_linear_layer:13
msgid ":obj:`torch.nn.Linear`"
msgstr ""

#: ../../source/internal/modeling_utils.rst:58
msgid "TensorFlow custom layers"
msgstr ""

#: of transformers.modeling_tf_utils.TFConv1D:9
#: transformers.modeling_tf_utils.TFSequenceSummary:24
msgid "The standard deviation to use to initialize the weights."
msgstr ""

#: of transformers.modeling_tf_utils.TFConv1D:11
#: transformers.modeling_tf_utils.TFSequenceSummary:26
#: transformers.modeling_tf_utils.TFSharedEmbeddings:13
msgid ""
"Additional keyword arguments passed along to the :obj:`__init__` of "
":obj:`tf.keras.layers.Layer`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings:1
msgid "Construct shared token embeddings."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings:3
msgid ""
"The weights of the embedding layer is usually shared with the weights of "
"the linear decoder when doing language modeling."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings:6
msgid "The size of the vocabulary, e.g., the number of unique tokens."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings:8
msgid "The size of the embedding vectors."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings:10
msgid ""
"The standard deviation to use when initializing the weights. If no value "
"is provided, it will default to :math:`1/\\sqrt{hidden\\_size}`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:1
msgid "Get token embeddings of inputs or decode final hidden state."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:3
msgid ""
"In embedding mode, should be an int64 tensor with shape "
":obj:`[batch_size, length]`.  In linear mode, should be a float tensor "
"with shape :obj:`[batch_size, length, hidden_size]`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:3
msgid ""
"In embedding mode, should be an int64 tensor with shape "
":obj:`[batch_size, length]`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:5
msgid ""
"In linear mode, should be a float tensor with shape :obj:`[batch_size, "
"length, hidden_size]`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:7
msgid ""
"A valid value is either :obj:`\"embedding\"` or :obj:`\"linear\"`, the "
"first one indicates that the layer should be used as an embedding layer, "
"the second one that the layer should be used as a linear decoder."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:11
msgid ""
"In embedding mode, the output is a float32 embedding tensor, with shape "
":obj:`[batch_size, length, embedding_size]`.  In linear mode, the output "
"is a float32 with shape :obj:`[batch_size, length, vocab_size]`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:11
msgid ""
"In embedding mode, the output is a float32 embedding tensor, with shape "
":obj:`[batch_size, length, embedding_size]`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:14
msgid ""
"In linear mode, the output is a float32 with shape :obj:`[batch_size, "
"length, vocab_size]`."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:15
msgid ":obj:`tf.Tensor`"
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call
msgid "Raises"
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:17
msgid "if :obj:`mode` is not valid."
msgstr ""

#: of transformers.modeling_tf_utils.TFSharedEmbeddings.call:19
msgid ""
"Shared weights logic is adapted from `here "
"<https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24>`__."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary.call:1
msgid "This is where the layer's logic lives."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary.call:3
msgid ""
"Note here that `call()` method in `tf.keras` is little bit different from"
" `keras` API. In `keras` API, you can pass support masking for layers as "
"additional arguments. Whereas `tf.keras` has `compute_mask()` method to "
"support masking."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary.call:8
msgid "Input tensor, or list/tuple of input tensors."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary.call:9
msgid "Additional keyword arguments. Currently unused."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceSummary.call:11
msgid "A tensor or list/tuple of tensors."
msgstr ""

#: ../../source/internal/modeling_utils.rst:70
msgid "TensorFlow loss functions"
msgstr ""

#: of transformers.modeling_tf_utils.TFCausalLanguageModelingLoss:1
msgid ""
"Loss function suitable for causal language modeling (CLM), that is, the "
"task of guessing the next token."
msgstr ""

#: of transformers.modeling_tf_utils.TFCausalLanguageModelingLoss:5
#: transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss:5
#: transformers.modeling_tf_utils.TFTokenClassificationLoss:5
msgid ""
"Any label of -100 will be ignored (along with the corresponding logits) "
"in the loss computation."
msgstr ""

#: of transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss:1
msgid ""
"Loss function suitable for masked language modeling (MLM), that is, the "
"task of guessing the masked tokens."
msgstr ""

#: of transformers.modeling_tf_utils.TFMultipleChoiceLoss:1
msgid "Loss function suitable for multiple choice tasks."
msgstr ""

#: of transformers.modeling_tf_utils.TFQuestionAnsweringLoss:1
msgid "Loss function suitable for question answering."
msgstr ""

#: of transformers.modeling_tf_utils.TFSequenceClassificationLoss:1
msgid "Loss function suitable for sequence classification."
msgstr ""

#: of transformers.modeling_tf_utils.TFTokenClassificationLoss:1
msgid "Loss function suitable for token classification."
msgstr ""

#: ../../source/internal/modeling_utils.rst:92
msgid "TensorFlow Helper Functions"
msgstr ""

#: of transformers.modeling_tf_utils.get_initializer:1
msgid "Creates a :obj:`tf.initializers.TruncatedNormal` with the given range."
msgstr ""

#: of transformers.modeling_tf_utils.get_initializer:3
msgid "Standard deviation of the initializer range."
msgstr ""

#: of transformers.modeling_tf_utils.get_initializer:6
msgid "The truncated normal initializer."
msgstr ""

#: of transformers.modeling_tf_utils.get_initializer:7
msgid ":obj:`tf.initializers.TruncatedNormal`"
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:1
msgid "Decorate a Keras Layer class to support Keras serialization."
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:3
msgid "This is done by:"
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:5
msgid ""
"Adding a :obj:`transformers_config` dict to the Keras config dictionary "
"in :obj:`get_config` (called by Keras at serialization time."
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:7
msgid ""
"Wrapping :obj:`__init__` to accept that :obj:`transformers_config` dict "
"(passed by Keras at deserialization time) and convert it to a config "
"object for the actual layer initializer."
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:9
msgid ""
"Registering the class as a custom object in Keras (if the Tensorflow "
"version supports this), so that it does not need to be supplied in "
":obj:`custom_objects` in the call to :obj:`tf.keras.models.load_model`."
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:12
msgid ""
"Typically a :obj:`TF.MainLayer` class in this project, in general must "
"accept a :obj:`config` argument to its initializer."
msgstr ""

#: of transformers.modeling_tf_utils.keras_serializable:16
msgid "The same class object, with modifications for Keras deserialization."
msgstr ""

#: of transformers.modeling_tf_utils.shape_list:1
msgid "Deal with dynamic shape in tensorflow cleanly."
msgstr ""

#: of transformers.modeling_tf_utils.shape_list:3
msgid "The tensor we want the shape of."
msgstr ""

#: of transformers.modeling_tf_utils.shape_list:6
msgid "The shape of the tensor as a list."
msgstr ""

#: of transformers.modeling_tf_utils.shape_list:7
msgid ":obj:`List[int]`"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-10 04:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/index.rst:439
msgid "Get started"
msgstr ""

#: ../../source/index.rst:448
msgid "Using 🤗 Transformers"
msgstr ""

#: ../../source/index.rst:460
msgid "Advanced guides"
msgstr ""

#: ../../source/index.rst:482
msgid "Research"
msgstr ""

#: ../../source/index.rst:490
msgid "Main Classes"
msgstr ""

#: ../../source/index.rst:508
msgid "Models"
msgstr ""

#: ../../source/index.rst:587
msgid "Internal Helpers"
msgstr ""

#: ../../source/index.rst:2
msgid "Transformers"
msgstr ""

#: ../../source/index.rst:4
msgid ""
"State-of-the-art Natural Language Processing for Jax, Pytorch and "
"TensorFlow"
msgstr ""

#: ../../source/index.rst:6
msgid ""
"🤗 Transformers (formerly known as `pytorch-transformers` and `pytorch-"
"pretrained-bert`) provides general-purpose architectures (BERT, GPT-2, "
"RoBERTa, XLM, DistilBert, XLNet...) for Natural Language Understanding "
"(NLU) and Natural Language Generation (NLG) with over 32+ pretrained "
"models in 100+ languages and deep interoperability between Jax, PyTorch "
"and TensorFlow."
msgstr ""

#: ../../source/index.rst:11
msgid ""
"This is the documentation of our repository `transformers "
"<https://github.com/huggingface/transformers>`__. You can also follow our"
" `online course <https://huggingface.co/course>`__ that teaches how to "
"use this library, as well as the other libraries developed by Hugging "
"Face and the Hub."
msgstr ""

#: ../../source/index.rst:16
msgid "If you are looking for custom support from the Hugging Face team"
msgstr ""

#: ../../source/index.rst:25
msgid "Features"
msgstr ""

#: ../../source/index.rst:27
msgid "High performance on NLU and NLG tasks"
msgstr ""

#: ../../source/index.rst:28
msgid "Low barrier to entry for educators and practitioners"
msgstr ""

#: ../../source/index.rst:30
msgid "State-of-the-art NLP for everyone:"
msgstr ""

#: ../../source/index.rst:32
msgid "Deep learning researchers"
msgstr ""

#: ../../source/index.rst:33
msgid "Hands-on practitioners"
msgstr ""

#: ../../source/index.rst:34
msgid "AI/ML/NLP teachers and educators"
msgstr ""

#: ../../source/index.rst:48
msgid "Lower compute costs, smaller carbon footprint:"
msgstr ""

#: ../../source/index.rst:50
msgid "Researchers can share trained models instead of always retraining"
msgstr ""

#: ../../source/index.rst:51
msgid "Practitioners can reduce compute time and production costs"
msgstr ""

#: ../../source/index.rst:52
msgid ""
"8 architectures with over 30 pretrained models, some in more than 100 "
"languages"
msgstr ""

#: ../../source/index.rst:54
msgid "Choose the right framework for every part of a model's lifetime:"
msgstr ""

#: ../../source/index.rst:56
msgid "Train state-of-the-art models in 3 lines of code"
msgstr ""

#: ../../source/index.rst:57
msgid "Deep interoperability between Jax, Pytorch and TensorFlow models"
msgstr ""

#: ../../source/index.rst:58
msgid "Move a single model between Jax/PyTorch/TensorFlow frameworks at will"
msgstr ""

#: ../../source/index.rst:59
msgid "Seamlessly pick the right framework for training, evaluation, production"
msgstr ""

#: ../../source/index.rst:61
msgid ""
"The support for Jax is still experimental (with a few models right now), "
"expect to see it grow in the coming months!"
msgstr ""

#: ../../source/index.rst:63
msgid ""
"`All the model checkpoints <https://huggingface.co/models>`__ are "
"seamlessly integrated from the huggingface.co `model hub "
"<https://huggingface.co>`__ where they are uploaded directly by `users "
"<https://huggingface.co/users>`__ and `organizations "
"<https://huggingface.co/organizations>`__."
msgstr ""

#: ../../source/index.rst:67
msgid "Current number of checkpoints: |checkpoints|"
msgstr ""

#: ../../source/index.rst:72
msgid "Contents"
msgstr ""

#: ../../source/index.rst:74
msgid "The documentation is organized in five parts:"
msgstr ""

#: ../../source/index.rst:76
msgid ""
"**GET STARTED** contains a quick tour, the installation instructions and "
"some useful information about our philosophy and a glossary."
msgstr ""

#: ../../source/index.rst:78
msgid ""
"**USING 🤗 TRANSFORMERS** contains general tutorials on how to use the "
"library."
msgstr ""

#: ../../source/index.rst:79
msgid ""
"**ADVANCED GUIDES** contains more advanced guides that are more specific "
"to a given script or part of the library."
msgstr ""

#: ../../source/index.rst:80
msgid ""
"**RESEARCH** focuses on tutorials that have less to do with how to use "
"the library but more about general research in transformers model"
msgstr ""

#: ../../source/index.rst:82
msgid ""
"The three last section contain the documentation of each public class and"
" function, grouped in:"
msgstr ""

#: ../../source/index.rst:84
msgid ""
"**MAIN CLASSES** for the main classes exposing the important APIs of the "
"library."
msgstr ""

#: ../../source/index.rst:85
msgid ""
"**MODELS** for the classes and functions related to each model "
"implemented in the library."
msgstr ""

#: ../../source/index.rst:86
msgid "**INTERNAL HELPERS** for the classes and functions we use internally."
msgstr ""

#: ../../source/index.rst:88
msgid ""
"The library currently contains Jax, PyTorch and Tensorflow "
"implementations, pretrained model weights, usage scripts and conversion "
"utilities for the following models."
msgstr ""

#: ../../source/index.rst:92
msgid "Supported models"
msgstr ""

#: ../../source/index.rst:97
msgid ""
":doc:`ALBERT <model_doc/albert>` (from Google Research and the Toyota "
"Technological Institute at Chicago) released with the paper `ALBERT: A "
"Lite BERT for Self-supervised Learning of Language Representations "
"<https://arxiv.org/abs/1909.11942>`__, by Zhenzhong Lan, Mingda Chen, "
"Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut."
msgstr ""

#: ../../source/index.rst:101
msgid ""
":doc:`BART <model_doc/bart>` (from Facebook) released with the paper "
"`BART: Denoising Sequence-to-Sequence Pre-training for Natural Language "
"Generation, Translation, and Comprehension "
"<https://arxiv.org/pdf/1910.13461.pdf>`__ by Mike Lewis, Yinhan Liu, "
"Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves "
"Stoyanov and Luke Zettlemoyer."
msgstr ""

#: ../../source/index.rst:105
msgid ""
":doc:`BARThez <model_doc/barthez>` (from École polytechnique) released "
"with the paper `BARThez: a Skilled Pretrained French Sequence-to-Sequence"
" Model <https://arxiv.org/abs/2010.12321>`__ by Moussa Kamal Eddine, "
"Antoine J.-P. Tixier, Michalis Vazirgiannis."
msgstr ""

#: ../../source/index.rst:108
msgid ""
"`BEiT <https://huggingface.co/transformers/master/model_doc/beit.html>`__"
" (from Microsoft) released with the paper `BEiT: BERT Pre-Training of "
"Image Transformers <https://arxiv.org/abs/2106.08254>`__ by Hangbo Bao, "
"Li Dong, Furu Wei."
msgstr ""

#: ../../source/index.rst:111
msgid ""
":doc:`BERT <model_doc/bert>` (from Google) released with the paper `BERT:"
" Pre-training of Deep Bidirectional Transformers for Language "
"Understanding <https://arxiv.org/abs/1810.04805>`__ by Jacob Devlin, "
"Ming-Wei Chang, Kenton Lee and Kristina Toutanova."
msgstr ""

#: ../../source/index.rst:114
msgid ""
":doc:`BERT For Sequence Generation <model_doc/bertgeneration>` (from "
"Google) released with the paper `Leveraging Pre-trained Checkpoints for "
"Sequence Generation Tasks <https://arxiv.org/abs/1907.12461>`__ by Sascha"
" Rothe, Shashi Narayan, Aliaksei Severyn."
msgstr ""

#: ../../source/index.rst:117
msgid ""
":doc:`BigBird-RoBERTa <model_doc/bigbird>` (from Google Research) "
"released with the paper `Big Bird: Transformers for Longer Sequences "
"<https://arxiv.org/abs/2007.14062>`__ by Manzil Zaheer, Guru Guruganesh, "
"Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip "
"Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed."
msgstr ""

#: ../../source/index.rst:120
msgid ""
":doc:`BigBird-Pegasus <model_doc/bigbird_pegasus>` (from Google Research)"
" released with the paper `Big Bird: Transformers for Longer Sequences "
"<https://arxiv.org/abs/2007.14062>`__ by Manzil Zaheer, Guru Guruganesh, "
"Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip "
"Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed."
msgstr ""

#: ../../source/index.rst:123
msgid ""
":doc:`Blenderbot <model_doc/blenderbot>` (from Facebook) released with "
"the paper `Recipes for building an open-domain chatbot "
"<https://arxiv.org/abs/2004.13637>`__ by Stephen Roller, Emily Dinan, "
"Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt "
"Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston."
msgstr ""

#: ../../source/index.rst:126
msgid ""
":doc:`BlenderbotSmall <model_doc/blenderbot_small>` (from Facebook) "
"released with the paper `Recipes for building an open-domain chatbot "
"<https://arxiv.org/abs/2004.13637>`__ by Stephen Roller, Emily Dinan, "
"Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt "
"Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston."
msgstr ""

#: ../../source/index.rst:129
msgid ""
":doc:`BORT <model_doc/bort>` (from Alexa) released with the paper "
"`Optimal Subarchitecture Extraction For BERT "
"<https://arxiv.org/abs/2010.10499>`__ by Adrian de Wynter and Daniel J. "
"Perry."
msgstr ""

#: ../../source/index.rst:131
msgid ""
":doc:`ByT5 <model_doc/byt5>` (from Google Research) released with the "
"paper `ByT5: Towards a token-free future with pre-trained byte-to-byte "
"models <https://arxiv.org/abs/2105.13626>`__ by Linting Xue, Aditya "
"Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam "
"Roberts, Colin Raffel."
msgstr ""

#: ../../source/index.rst:134
msgid ""
":doc:`CamemBERT <model_doc/camembert>` (from Inria/Facebook/Sorbonne) "
"released with the paper `CamemBERT: a Tasty French Language Model "
"<https://arxiv.org/abs/1911.03894>`__ by Louis Martin*, Benjamin Muller*,"
" Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric "
"Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot."
msgstr ""

#: ../../source/index.rst:137
msgid ""
":doc:`CANINE <model_doc/canine>` (from Google Research) released with the"
" paper `CANINE: Pre-training an Efficient Tokenization-Free Encoder for "
"Language Representation <https://arxiv.org/abs/2103.06874>`__ by Jonathan"
" H. Clark, Dan Garrette, Iulia Turc, John Wieting."
msgstr ""

#: ../../source/index.rst:140
msgid ""
":doc:`CLIP <model_doc/clip>` (from OpenAI) released with the paper "
"`Learning Transferable Visual Models From Natural Language Supervision "
"<https://arxiv.org/abs/2103.00020>`__ by Alec Radford, Jong Wook Kim, "
"Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish "
"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya"
" Sutskever."
msgstr ""

#: ../../source/index.rst:144
msgid ""
":doc:`ConvBERT <model_doc/convbert>` (from YituTech) released with the "
"paper `ConvBERT: Improving BERT with Span-based Dynamic Convolution "
"<https://arxiv.org/abs/2008.02496>`__ by Zihang Jiang, Weihao Yu, Daquan "
"Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan."
msgstr ""

#: ../../source/index.rst:147
msgid ""
":doc:`CPM <model_doc/cpm>` (from Tsinghua University) released with the "
"paper `CPM: A Large-scale Generative Chinese Pre-trained Language Model "
"<https://arxiv.org/abs/2012.00413>`__ by Zhengyan Zhang, Xu Han, Hao "
"Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, "
"Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi "
"Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, "
"Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun."
msgstr ""

#: ../../source/index.rst:152
msgid ""
":doc:`CTRL <model_doc/ctrl>` (from Salesforce) released with the paper "
"`CTRL: A Conditional Transformer Language Model for Controllable "
"Generation <https://arxiv.org/abs/1909.05858>`__ by Nitish Shirish "
"Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard "
"Socher."
msgstr ""

#: ../../source/index.rst:155
msgid ""
":doc:`DeBERTa <model_doc/deberta>` (from Microsoft) released with the "
"paper `DeBERTa: Decoding-enhanced BERT with Disentangled Attention "
"<https://arxiv.org/abs/2006.03654>`__ by Pengcheng He, Xiaodong Liu, "
"Jianfeng Gao, Weizhu Chen."
msgstr ""

#: ../../source/index.rst:158
msgid ""
":doc:`DeBERTa-v2 <model_doc/deberta_v2>` (from Microsoft) released with "
"the paper `DeBERTa: Decoding-enhanced BERT with Disentangled Attention "
"<https://arxiv.org/abs/2006.03654>`__ by Pengcheng He, Xiaodong Liu, "
"Jianfeng Gao, Weizhu Chen."
msgstr ""

#: ../../source/index.rst:161
msgid ""
":doc:`DeiT <model_doc/deit>` (from Facebook) released with the paper "
"`Training data-efficient image transformers & distillation through "
"attention <https://arxiv.org/abs/2012.12877>`__ by Hugo Touvron, Matthieu"
" Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé "
"Jégou."
msgstr ""

#: ../../source/index.rst:164
msgid ""
":doc:`DETR <model_doc/detr>` (from Facebook) released with the paper "
"`End-to-End Object Detection with Transformers "
"<https://arxiv.org/abs/2005.12872>`__ by Nicolas Carion, Francisco Massa,"
" Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko."
msgstr ""

#: ../../source/index.rst:167
msgid ""
":doc:`DialoGPT <model_doc/dialogpt>` (from Microsoft Research) released "
"with the paper `DialoGPT: Large-Scale Generative Pre-training for "
"Conversational Response Generation <https://arxiv.org/abs/1911.00536>`__ "
"by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, "
"Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan."
msgstr ""

#: ../../source/index.rst:170
msgid ""
":doc:`DistilBERT <model_doc/distilbert>` (from HuggingFace), released "
"together with the paper `DistilBERT, a distilled version of BERT: "
"smaller, faster, cheaper and lighter "
"<https://arxiv.org/abs/1910.01108>`__ by Victor Sanh, Lysandre Debut and "
"Thomas Wolf. The same method has been applied to compress GPT2 into "
"`DistilGPT2 "
"<https://github.com/huggingface/transformers/tree/master/examples/distillation>`__,"
" RoBERTa into `DistilRoBERTa "
"<https://github.com/huggingface/transformers/tree/master/examples/distillation>`__,"
" Multilingual BERT into `DistilmBERT "
"<https://github.com/huggingface/transformers/tree/master/examples/distillation>`__"
" and a German version of DistilBERT."
msgstr ""

#: ../../source/index.rst:177
msgid ""
":doc:`DPR <model_doc/dpr>` (from Facebook) released with the paper `Dense"
" Passage Retrieval for Open-Domain Question Answering "
"<https://arxiv.org/abs/2004.04906>`__ by Vladimir Karpukhin, Barlas Oğuz,"
" Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-"
"tau Yih."
msgstr ""

#: ../../source/index.rst:180
msgid ""
":doc:`ELECTRA <model_doc/electra>` (from Google Research/Stanford "
"University) released with the paper `ELECTRA: Pre-training text encoders "
"as discriminators rather than generators "
"<https://arxiv.org/abs/2003.10555>`__ by Kevin Clark, Minh-Thang Luong, "
"Quoc V. Le, Christopher D. Manning."
msgstr ""

#: ../../source/index.rst:183
msgid ""
":doc:`FlauBERT <model_doc/flaubert>` (from CNRS) released with the paper "
"`FlauBERT: Unsupervised Language Model Pre-training for French "
"<https://arxiv.org/abs/1912.05372>`__ by Hang Le, Loïc Vial, Jibril Frej,"
" Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre "
"Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab."
msgstr ""

#: ../../source/index.rst:186
msgid ""
":doc:`Funnel Transformer <model_doc/funnel>` (from CMU/Google Brain) "
"released with the paper `Funnel-Transformer: Filtering out Sequential "
"Redundancy for Efficient Language Processing "
"<https://arxiv.org/abs/2006.03236>`__ by Zihang Dai, Guokun Lai, Yiming "
"Yang, Quoc V. Le."
msgstr ""

#: ../../source/index.rst:189
msgid ""
":doc:`GPT <model_doc/gpt>` (from OpenAI) released with the paper "
"`Improving Language Understanding by Generative Pre-Training "
"<https://blog.openai.com/language-unsupervised/>`__ by Alec Radford, "
"Karthik Narasimhan, Tim Salimans and Ilya Sutskever."
msgstr ""

#: ../../source/index.rst:192
msgid ""
":doc:`GPT-2 <model_doc/gpt2>` (from OpenAI) released with the paper "
"`Language Models are Unsupervised Multitask Learners "
"<https://blog.openai.com/better-language-models/>`__ by Alec Radford*, "
"Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya "
"Sutskever**."
msgstr ""

#: ../../source/index.rst:195
msgid ""
":doc:`GPT Neo <model_doc/gpt_neo>` (from EleutherAI) released in the "
"repository `EleutherAI/gpt-neo <https://github.com/EleutherAI/gpt-neo>`__"
" by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy."
msgstr ""

#: ../../source/index.rst:197
msgid ""
":doc:`Hubert <model_doc/hubert>` (from Facebook) released with the paper "
"`HuBERT: Self-Supervised Speech Representation Learning by Masked "
"Prediction of Hidden Units <https://arxiv.org/abs/2106.07447>`__ by Wei-"
"Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan "
"Salakhutdinov, Abdelrahman Mohamed."
msgstr ""

#: ../../source/index.rst:200
msgid ""
":doc:`I-BERT <model_doc/ibert>` (from Berkeley) released with the paper "
"`I-BERT: Integer-only BERT Quantization "
"<https://arxiv.org/abs/2101.01321>`__ by Sehoon Kim, Amir Gholami, Zhewei"
" Yao, Michael W. Mahoney, Kurt Keutzer"
msgstr ""

#: ../../source/index.rst:202
msgid ""
":doc:`LayoutLM <model_doc/layoutlm>` (from Microsoft Research Asia) "
"released with the paper `LayoutLM: Pre-training of Text and Layout for "
"Document Image Understanding <https://arxiv.org/abs/1912.13318>`__ by "
"Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou."
msgstr ""

#: ../../source/index.rst:205
msgid ""
":doc:`LED <model_doc/led>` (from AllenAI) released with the paper "
"`Longformer: The Long-Document Transformer "
"<https://arxiv.org/abs/2004.05150>`__ by Iz Beltagy, Matthew E. Peters, "
"Arman Cohan."
msgstr ""

#: ../../source/index.rst:207
msgid ""
":doc:`Longformer <model_doc/longformer>` (from AllenAI) released with the"
" paper `Longformer: The Long-Document Transformer "
"<https://arxiv.org/abs/2004.05150>`__ by Iz Beltagy, Matthew E. Peters, "
"Arman Cohan."
msgstr ""

#: ../../source/index.rst:209
msgid ""
":doc:`LUKE <model_doc/luke>` (from Studio Ousia) released with the paper "
"`LUKE: Deep Contextualized Entity Representations with Entity-aware Self-"
"attention <https://arxiv.org/abs/2010.01057>`__ by Ikuya Yamada, Akari "
"Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto."
msgstr ""

#: ../../source/index.rst:212
msgid ""
":doc:`LXMERT <model_doc/lxmert>` (from UNC Chapel Hill) released with the"
" paper `LXMERT: Learning Cross-Modality Encoder Representations from "
"Transformers for Open-Domain Question Answering "
"<https://arxiv.org/abs/1908.07490>`__ by Hao Tan and Mohit Bansal."
msgstr ""

#: ../../source/index.rst:215
msgid ""
":doc:`M2M100 <model_doc/m2m_100>` (from Facebook) released with the paper"
" `Beyond English-Centric Multilingual Machine Translation "
"<https://arxiv.org/abs/2010.11125>`__ by by Angela Fan, Shruti Bhosale, "
"Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep "
"Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, "
"Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael "
"Auli, Armand Joulin."
msgstr ""

#: ../../source/index.rst:219
msgid ""
":doc:`MarianMT <model_doc/marian>` Machine translation models trained "
"using `OPUS <http://opus.nlpl.eu/>`__ data by Jörg Tiedemann. The `Marian"
" Framework <https://marian-nmt.github.io/>`__ is being developed by the "
"Microsoft Translator Team."
msgstr ""

#: ../../source/index.rst:222
msgid ""
":doc:`MBart <model_doc/mbart>` (from Facebook) released with the paper "
"`Multilingual Denoising Pre-training for Neural Machine Translation "
"<https://arxiv.org/abs/2001.08210>`__ by Yinhan Liu, Jiatao Gu, Naman "
"Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke "
"Zettlemoyer."
msgstr ""

#: ../../source/index.rst:225
msgid ""
":doc:`MBart-50 <model_doc/mbart>` (from Facebook) released with the paper"
" `Multilingual Translation with Extensible Multilingual Pretraining and "
"Finetuning <https://arxiv.org/abs/2008.00401>`__ by Yuqing Tang, Chau "
"Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, "
"Angela Fan."
msgstr ""

#: ../../source/index.rst:228
msgid ""
":doc:`Megatron-BERT <model_doc/megatron_bert>` (from NVIDIA) released "
"with the paper `Megatron-LM: Training Multi-Billion Parameter Language "
"Models Using Model Parallelism <https://arxiv.org/abs/1909.08053>`__ by "
"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared "
"Casper and Bryan Catanzaro."
msgstr ""

#: ../../source/index.rst:231
msgid ""
":doc:`Megatron-GPT2 <model_doc/megatron_gpt2>` (from NVIDIA) released "
"with the paper `Megatron-LM: Training Multi-Billion Parameter Language "
"Models Using Model Parallelism <https://arxiv.org/abs/1909.08053>`__ by "
"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared "
"Casper and Bryan Catanzaro."
msgstr ""

#: ../../source/index.rst:234
msgid ""
":doc:`MPNet <model_doc/mpnet>` (from Microsoft Research) released with "
"the paper `MPNet: Masked and Permuted Pre-training for Language "
"Understanding <https://arxiv.org/abs/2004.09297>`__ by Kaitao Song, Xu "
"Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu."
msgstr ""

#: ../../source/index.rst:237
msgid ""
":doc:`MT5 <model_doc/mt5>` (from Google AI) released with the paper `mT5:"
" A massively multilingual pre-trained text-to-text transformer "
"<https://arxiv.org/abs/2010.11934>`__ by Linting Xue, Noah Constant, Adam"
" Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin "
"Raffel."
msgstr ""

#: ../../source/index.rst:240
msgid ""
":doc:`Pegasus <model_doc/pegasus>` (from Google) released with the paper "
"`PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive "
"Summarization <https://arxiv.org/abs/1912.08777>`__> by Jingqing Zhang, "
"Yao Zhao, Mohammad Saleh and Peter J. Liu."
msgstr ""

#: ../../source/index.rst:243
msgid ""
":doc:`ProphetNet <model_doc/prophetnet>` (from Microsoft Research) "
"released with the paper `ProphetNet: Predicting Future N-gram for "
"Sequence-to-Sequence Pre-training <https://arxiv.org/abs/2001.04063>`__ "
"by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,"
" Ruofei Zhang and Ming Zhou."
msgstr ""

#: ../../source/index.rst:246
msgid ""
":doc:`Reformer <model_doc/reformer>` (from Google Research) released with"
" the paper `Reformer: The Efficient Transformer "
"<https://arxiv.org/abs/2001.04451>`__ by Nikita Kitaev, Łukasz Kaiser, "
"Anselm Levskaya."
msgstr ""

#: ../../source/index.rst:248
msgid ""
":doc:`RemBERT <model_doc/rembert>` (from Google Research) released with "
"the paper `Rethinking embedding coupling in pre-trained language models "
"<https://arxiv.org/pdf/2010.12821.pdf>`__ by Hyung Won Chung, Thibault "
"Févry, Henry Tsai, M. Johnson, Sebastian Ruder."
msgstr ""

#: ../../source/index.rst:251
msgid ""
":doc:`RoBERTa <model_doc/roberta>` (from Facebook), released together "
"with the paper a `Robustly Optimized BERT Pretraining Approach "
"<https://arxiv.org/abs/1907.11692>`__ by Yinhan Liu, Myle Ott, Naman "
"Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke "
"Zettlemoyer, Veselin Stoyanov."
msgstr ""

#: ../../source/index.rst:254
msgid ""
":doc:`RoFormer <model_doc/roformer>` (from ZhuiyiTechnology), released "
"together with the paper a `RoFormer: Enhanced Transformer with Rotary "
"Position Embedding <https://arxiv.org/pdf/2104.09864v1.pdf>`__ by Jianlin"
" Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu."
msgstr ""

#: ../../source/index.rst:257
msgid ""
":doc:`SpeechToTextTransformer <model_doc/speech_to_text>` (from "
"Facebook), released together with the paper `fairseq S2T: Fast Speech-to-"
"Text Modeling with fairseq <https://arxiv.org/abs/2010.05171>`__ by "
"Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino."
msgstr ""

#: ../../source/index.rst:260
msgid ""
":doc:`SqueezeBert <model_doc/squeezebert>` released with the paper "
"`SqueezeBERT: What can computer vision teach NLP about efficient neural "
"networks? <https://arxiv.org/abs/2006.11316>`__ by Forrest N. Iandola, "
"Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer."
msgstr ""

#: ../../source/index.rst:263
msgid ""
":doc:`T5 <model_doc/t5>` (from Google AI) released with the paper "
"`Exploring the Limits of Transfer Learning with a Unified Text-to-Text "
"Transformer <https://arxiv.org/abs/1910.10683>`__ by Colin Raffel and "
"Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and "
"Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu."
msgstr ""

#: ../../source/index.rst:266
msgid ""
":doc:`TAPAS <model_doc/tapas>` (from Google AI) released with the paper "
"`TAPAS: Weakly Supervised Table Parsing via Pre-training "
"<https://arxiv.org/abs/2004.02349>`__ by Jonathan Herzig, Paweł Krzysztof"
" Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos."
msgstr ""

#: ../../source/index.rst:269
msgid ""
":doc:`Transformer-XL <model_doc/transformerxl>` (from Google/CMU) "
"released with the paper `Transformer-XL: Attentive Language Models Beyond"
" a Fixed-Length Context <https://arxiv.org/abs/1901.02860>`__ by Zihang "
"Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan "
"Salakhutdinov."
msgstr ""

#: ../../source/index.rst:272
msgid ""
":doc:`Vision Transformer (ViT) <model_doc/vit>` (from Google AI) released"
" with the paper `An Image is Worth 16x16 Words: Transformers for Image "
"Recognition at Scale <https://arxiv.org/abs/2010.11929>`__ by Alexey "
"Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua"
" Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg "
"Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby."
msgstr ""

#: ../../source/index.rst:276
msgid ""
":doc:`VisualBERT <model_doc/visual_bert>` (from UCLA NLP) released with "
"the paper `VisualBERT: A Simple and Performant Baseline for Vision and "
"Language <https://arxiv.org/pdf/1908.03557>`__ by Liunian Harold Li, Mark"
" Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang."
msgstr ""

#: ../../source/index.rst:279
msgid ""
":doc:`Wav2Vec2 <model_doc/wav2vec2>` (from Facebook AI) released with the"
" paper `wav2vec 2.0: A Framework for Self-Supervised Learning of Speech "
"Representations <https://arxiv.org/abs/2006.11477>`__ by Alexei Baevski, "
"Henry Zhou, Abdelrahman Mohamed, Michael Auli."
msgstr ""

#: ../../source/index.rst:282
msgid ""
":doc:`XLM <model_doc/xlm>` (from Facebook) released together with the "
"paper `Cross-lingual Language Model Pretraining "
"<https://arxiv.org/abs/1901.07291>`__ by Guillaume Lample and Alexis "
"Conneau."
msgstr ""

#: ../../source/index.rst:284
msgid ""
":doc:`XLM-ProphetNet <model_doc/xlmprophetnet>` (from Microsoft Research)"
" released with the paper `ProphetNet: Predicting Future N-gram for "
"Sequence-to-Sequence Pre-training <https://arxiv.org/abs/2001.04063>`__ "
"by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,"
" Ruofei Zhang and Ming Zhou."
msgstr ""

#: ../../source/index.rst:287
msgid ""
":doc:`XLM-RoBERTa <model_doc/xlmroberta>` (from Facebook AI), released "
"together with the paper `Unsupervised Cross-lingual Representation "
"Learning at Scale <https://arxiv.org/abs/1911.02116>`__ by Alexis "
"Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume"
" Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and "
"Veselin Stoyanov."
msgstr ""

#: ../../source/index.rst:291
msgid ""
":doc:`XLNet <model_doc/xlnet>` (from Google/CMU) released with the paper "
"`​XLNet: Generalized Autoregressive Pretraining for Language "
"Understanding <https://arxiv.org/abs/1906.08237>`__ by Zhilin Yang*, "
"Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. "
"Le."
msgstr ""

#: ../../source/index.rst:294
msgid ""
":doc:`XLSR-Wav2Vec2 <model_doc/xlsr_wav2vec2>` (from Facebook AI) "
"released with the paper `Unsupervised Cross-Lingual Representation "
"Learning For Speech Recognition <https://arxiv.org/abs/2006.13979>`__ by "
"Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, "
"Michael Auli."
msgstr ""

#: ../../source/index.rst:300
msgid "Supported frameworks"
msgstr ""

#: ../../source/index.rst:302
msgid ""
"The table below represents the current support in the library for each of"
" those models, whether they have a Python tokenizer (called \"slow\"). A "
"\"fast\" tokenizer backed by the 🤗 Tokenizers library, whether they have "
"support in Jax (via Flax), PyTorch, and/or TensorFlow."
msgstr ""

#: ../../source/index.rst:312
msgid "Model"
msgstr ""

#: ../../source/index.rst:312
msgid "Tokenizer slow"
msgstr ""

#: ../../source/index.rst:312
msgid "Tokenizer fast"
msgstr ""

#: ../../source/index.rst:312
msgid "PyTorch support"
msgstr ""

#: ../../source/index.rst:312
msgid "TensorFlow support"
msgstr ""

#: ../../source/index.rst:312
msgid "Flax Support"
msgstr ""

#: ../../source/index.rst:314
msgid "ALBERT"
msgstr ""

#: ../../source/index.rst:314 ../../source/index.rst:316
#: ../../source/index.rst:318 ../../source/index.rst:320
#: ../../source/index.rst:322 ../../source/index.rst:324
#: ../../source/index.rst:326 ../../source/index.rst:328
#: ../../source/index.rst:330 ../../source/index.rst:332
#: ../../source/index.rst:334 ../../source/index.rst:336
#: ../../source/index.rst:338 ../../source/index.rst:340
#: ../../source/index.rst:342 ../../source/index.rst:344
#: ../../source/index.rst:346 ../../source/index.rst:348
#: ../../source/index.rst:350 ../../source/index.rst:352
#: ../../source/index.rst:354 ../../source/index.rst:356
#: ../../source/index.rst:358 ../../source/index.rst:360
#: ../../source/index.rst:362 ../../source/index.rst:364
#: ../../source/index.rst:366 ../../source/index.rst:368
#: ../../source/index.rst:370 ../../source/index.rst:372
#: ../../source/index.rst:374 ../../source/index.rst:376
#: ../../source/index.rst:378 ../../source/index.rst:380
#: ../../source/index.rst:382 ../../source/index.rst:384
#: ../../source/index.rst:386 ../../source/index.rst:388
#: ../../source/index.rst:390 ../../source/index.rst:392
#: ../../source/index.rst:394 ../../source/index.rst:396
#: ../../source/index.rst:398 ../../source/index.rst:400
#: ../../source/index.rst:402 ../../source/index.rst:404
#: ../../source/index.rst:406 ../../source/index.rst:408
#: ../../source/index.rst:410 ../../source/index.rst:412
#: ../../source/index.rst:414 ../../source/index.rst:416
#: ../../source/index.rst:418 ../../source/index.rst:420
#: ../../source/index.rst:422 ../../source/index.rst:424
#: ../../source/index.rst:426 ../../source/index.rst:428
#: ../../source/index.rst:430 ../../source/index.rst:432
#: ../../source/index.rst:434 ../../source/index.rst:436
msgid "✅"
msgstr ""

#: ../../source/index.rst:314 ../../source/index.rst:320
#: ../../source/index.rst:322 ../../source/index.rst:324
#: ../../source/index.rst:326 ../../source/index.rst:328
#: ../../source/index.rst:330 ../../source/index.rst:332
#: ../../source/index.rst:334 ../../source/index.rst:336
#: ../../source/index.rst:338 ../../source/index.rst:340
#: ../../source/index.rst:342 ../../source/index.rst:344
#: ../../source/index.rst:346 ../../source/index.rst:348
#: ../../source/index.rst:350 ../../source/index.rst:352
#: ../../source/index.rst:356 ../../source/index.rst:358
#: ../../source/index.rst:360 ../../source/index.rst:362
#: ../../source/index.rst:364 ../../source/index.rst:366
#: ../../source/index.rst:368 ../../source/index.rst:370
#: ../../source/index.rst:372 ../../source/index.rst:374
#: ../../source/index.rst:376 ../../source/index.rst:378
#: ../../source/index.rst:380 ../../source/index.rst:382
#: ../../source/index.rst:384 ../../source/index.rst:386
#: ../../source/index.rst:388 ../../source/index.rst:390
#: ../../source/index.rst:394 ../../source/index.rst:396
#: ../../source/index.rst:398 ../../source/index.rst:400
#: ../../source/index.rst:402 ../../source/index.rst:404
#: ../../source/index.rst:408 ../../source/index.rst:410
#: ../../source/index.rst:412 ../../source/index.rst:416
#: ../../source/index.rst:418 ../../source/index.rst:420
#: ../../source/index.rst:422 ../../source/index.rst:424
#: ../../source/index.rst:426 ../../source/index.rst:428
#: ../../source/index.rst:430 ../../source/index.rst:432
msgid "❌"
msgstr ""

#: ../../source/index.rst:316
msgid "BART"
msgstr ""

#: ../../source/index.rst:318
msgid "BERT"
msgstr ""

#: ../../source/index.rst:320
msgid "BeiT"
msgstr ""

#: ../../source/index.rst:322
msgid "Bert Generation"
msgstr ""

#: ../../source/index.rst:324
msgid "BigBird"
msgstr ""

#: ../../source/index.rst:326
msgid "BigBirdPegasus"
msgstr ""

#: ../../source/index.rst:328
msgid "Blenderbot"
msgstr ""

#: ../../source/index.rst:330
msgid "BlenderbotSmall"
msgstr ""

#: ../../source/index.rst:332
msgid "CLIP"
msgstr ""

#: ../../source/index.rst:334
msgid "CTRL"
msgstr ""

#: ../../source/index.rst:336
msgid "CamemBERT"
msgstr ""

#: ../../source/index.rst:338
msgid "Canine"
msgstr ""

#: ../../source/index.rst:340
msgid "ConvBERT"
msgstr ""

#: ../../source/index.rst:342
msgid "DETR"
msgstr ""

#: ../../source/index.rst:344
msgid "DPR"
msgstr ""

#: ../../source/index.rst:346
msgid "DeBERTa"
msgstr ""

#: ../../source/index.rst:348
msgid "DeBERTa-v2"
msgstr ""

#: ../../source/index.rst:350
msgid "DeiT"
msgstr ""

#: ../../source/index.rst:352
msgid "DistilBERT"
msgstr ""

#: ../../source/index.rst:354
msgid "ELECTRA"
msgstr ""

#: ../../source/index.rst:356
msgid "Encoder decoder"
msgstr ""

#: ../../source/index.rst:358
msgid "FairSeq Machine-Translation"
msgstr ""

#: ../../source/index.rst:360
msgid "FlauBERT"
msgstr ""

#: ../../source/index.rst:362
msgid "Funnel Transformer"
msgstr ""

#: ../../source/index.rst:364
msgid "GPT Neo"
msgstr ""

#: ../../source/index.rst:366
msgid "Hubert"
msgstr ""

#: ../../source/index.rst:368
msgid "I-BERT"
msgstr ""

#: ../../source/index.rst:370
msgid "LED"
msgstr ""

#: ../../source/index.rst:372
msgid "LUKE"
msgstr ""

#: ../../source/index.rst:374
msgid "LXMERT"
msgstr ""

#: ../../source/index.rst:376
msgid "LayoutLM"
msgstr ""

#: ../../source/index.rst:378
msgid "Longformer"
msgstr ""

#: ../../source/index.rst:380
msgid "M2M100"
msgstr ""

#: ../../source/index.rst:382
msgid "MPNet"
msgstr ""

#: ../../source/index.rst:384
msgid "Marian"
msgstr ""

#: ../../source/index.rst:386
msgid "MegatronBert"
msgstr ""

#: ../../source/index.rst:388
msgid "MobileBERT"
msgstr ""

#: ../../source/index.rst:390
msgid "OpenAI GPT"
msgstr ""

#: ../../source/index.rst:392
msgid "OpenAI GPT-2"
msgstr ""

#: ../../source/index.rst:394
msgid "Pegasus"
msgstr ""

#: ../../source/index.rst:396
msgid "ProphetNet"
msgstr ""

#: ../../source/index.rst:398
msgid "RAG"
msgstr ""

#: ../../source/index.rst:400
msgid "Reformer"
msgstr ""

#: ../../source/index.rst:402
msgid "RemBERT"
msgstr ""

#: ../../source/index.rst:404
msgid "RetriBERT"
msgstr ""

#: ../../source/index.rst:406
msgid "RoBERTa"
msgstr ""

#: ../../source/index.rst:408
msgid "RoFormer"
msgstr ""

#: ../../source/index.rst:410
msgid "Speech2Text"
msgstr ""

#: ../../source/index.rst:412
msgid "SqueezeBERT"
msgstr ""

#: ../../source/index.rst:414
msgid "T5"
msgstr ""

#: ../../source/index.rst:416
msgid "TAPAS"
msgstr ""

#: ../../source/index.rst:418
msgid "Transformer-XL"
msgstr ""

#: ../../source/index.rst:420
msgid "ViT"
msgstr ""

#: ../../source/index.rst:422
msgid "VisualBert"
msgstr ""

#: ../../source/index.rst:424
msgid "Wav2Vec2"
msgstr ""

#: ../../source/index.rst:426
msgid "XLM"
msgstr ""

#: ../../source/index.rst:428
msgid "XLM-RoBERTa"
msgstr ""

#: ../../source/index.rst:430
msgid "XLMProphetNet"
msgstr ""

#: ../../source/index.rst:432
msgid "XLNet"
msgstr ""

#: ../../source/index.rst:434
msgid "mBART"
msgstr ""

#: ../../source/index.rst:436
msgid "mT5"
msgstr ""


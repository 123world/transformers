# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/main_classes/data_collator.rst:14
msgid "Data Collator"
msgstr ""

#: ../../source/main_classes/data_collator.rst:16
msgid ""
"Data collators are objects that will form a batch by using a list of "
"dataset elements as input. These elements are of the same type as the "
"elements of :obj:`train_dataset` or :obj:`eval_dataset`."
msgstr ""

#: ../../source/main_classes/data_collator.rst:19
msgid ""
"To be able to build batches, data collators may apply some processing "
"(like padding). Some of them (like "
":class:`~transformers.DataCollatorForLanguageModeling`) also apply some "
"random data augmentation (like random masking) oin the formed batch."
msgstr ""

#: ../../source/main_classes/data_collator.rst:23
msgid ""
"Examples of use can be found in the :doc:`example scripts <../examples>` "
"or :doc:`example notebooks <../notebooks>`."
msgstr ""

#: ../../source/main_classes/data_collator.rst:27
msgid "Default data collator"
msgstr ""

#: of transformers.data.data_collator.default_data_collator:1
msgid ""
"Very simple data collator that simply collates batches of dict-like "
"objects and performs special handling for potential keys named:"
msgstr ""

#: of transformers.data.data_collator.default_data_collator:4
msgid "``label``: handles a single value (int or float) per object"
msgstr ""

#: of transformers.data.data_collator.default_data_collator:5
msgid "``label_ids``: handles a list of values per object"
msgstr ""

#: of transformers.data.data_collator.default_data_collator:7
msgid ""
"Does not do any additional preprocessing: property names of the input "
"object will be used as corresponding inputs to the model. See glue and "
"ner for example of how it's useful."
msgstr ""

#: ../../source/main_classes/data_collator.rst:33
msgid "DataCollatorWithPadding"
msgstr ""

#: of transformers.data.data_collator.DataCollatorWithPadding:1
msgid "Data collator that will dynamically pad the inputs received."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling
#: transformers.data.data_collator.DataCollatorForSeq2Seq
#: transformers.data.data_collator.DataCollatorForTokenClassification
#: transformers.data.data_collator.DataCollatorWithPadding
msgid "Parameters"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling:4
#: transformers.data.data_collator.DataCollatorForSeq2Seq:3
#: transformers.data.data_collator.DataCollatorForTokenClassification:3
#: transformers.data.data_collator.DataCollatorWithPadding:3
msgid "The tokenizer used for encoding the data."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForTokenClassification:5
#: transformers.data.data_collator.DataCollatorWithPadding:5
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:  * :obj:`True` or "
":obj:`'longest'`: Pad to the longest sequence in the batch (or no padding"
" if only a single   sequence if provided). * :obj:`'max_length'`: Pad to "
"a maximum length specified with the argument :obj:`max_length` or to the"
"   maximum acceptable input length for the model if that argument is not "
"provided. * :obj:`False` or :obj:`'do_not_pad'` (default): No padding "
"(i.e., can output a batch with sequences of   different lengths)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:10
#: transformers.data.data_collator.DataCollatorForTokenClassification:5
#: transformers.data.data_collator.DataCollatorWithPadding:5
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForTokenClassification:8
#: transformers.data.data_collator.DataCollatorWithPadding:8
msgid ""
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single sequence if provided)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:15
#: transformers.data.data_collator.DataCollatorForTokenClassification:10
#: transformers.data.data_collator.DataCollatorWithPadding:10
msgid ""
":obj:`'max_length'`: Pad to a maximum length specified with the argument "
":obj:`max_length` or to the maximum acceptable input length for the model"
" if that argument is not provided."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:17
#: transformers.data.data_collator.DataCollatorForTokenClassification:12
#: transformers.data.data_collator.DataCollatorWithPadding:12
msgid ""
":obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can "
"output a batch with sequences of different lengths)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:20
#: transformers.data.data_collator.DataCollatorForTokenClassification:15
#: transformers.data.data_collator.DataCollatorWithPadding:15
msgid ""
"Maximum length of the returned list and optionally padding length (see "
"above)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:22
#: transformers.data.data_collator.DataCollatorForTokenClassification:17
#: transformers.data.data_collator.DataCollatorWithPadding:17
msgid ""
"If set will pad the sequence to a multiple of the provided value.  This "
"is especially useful to enable the use of Tensor Cores on NVIDIA hardware"
" with compute capability >= 7.5 (Volta)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling:12
#: transformers.data.data_collator.DataCollatorForSeq2Seq:22
#: transformers.data.data_collator.DataCollatorForTokenClassification:17
#: transformers.data.data_collator.DataCollatorWithPadding:17
msgid "If set will pad the sequence to a multiple of the provided value."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:24
#: transformers.data.data_collator.DataCollatorForTokenClassification:19
#: transformers.data.data_collator.DataCollatorWithPadding:19
msgid ""
"This is especially useful to enable the use of Tensor Cores on NVIDIA "
"hardware with compute capability >= 7.5 (Volta)."
msgstr ""

#: ../../source/main_classes/data_collator.rst:40
msgid "DataCollatorForTokenClassification"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:1
#: transformers.data.data_collator.DataCollatorForTokenClassification:1
msgid ""
"Data collator that will dynamically pad the inputs received, as well as "
"the labels."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForTokenClassification:22
msgid ""
"The id to use when padding the labels (-100 will be automatically ignore "
"by PyTorch loss functions)."
msgstr ""

#: ../../source/main_classes/data_collator.rst:47
msgid "DataCollatorForSeq2Seq"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:5
msgid ""
"The model that is being trained. If set and has the "
"`prepare_decoder_input_ids_from_labels`, use it to prepare the "
"`decoder_input_ids`  This is useful when using `label_smoothing` to avoid"
" calculating loss twice."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:5
msgid ""
"The model that is being trained. If set and has the "
"`prepare_decoder_input_ids_from_labels`, use it to prepare the "
"`decoder_input_ids`"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:8
msgid ""
"This is useful when using `label_smoothing` to avoid calculating loss "
"twice."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:10
msgid ""
"Select a strategy to pad the returned sequences (according to the model's"
" padding side and padding index) among:  * :obj:`True` or "
":obj:`'longest'`: Pad to the longest sequence in the batch (or no padding"
" if only a single   sequence is provided). * :obj:`'max_length'`: Pad to "
"a maximum length specified with the argument :obj:`max_length` or to the"
"   maximum acceptable input length for the model if that argument is not "
"provided. * :obj:`False` or :obj:`'do_not_pad'` (default): No padding "
"(i.e., can output a batch with sequences of   different lengths)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:13
msgid ""
":obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch"
" (or no padding if only a single sequence is provided)."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForSeq2Seq:27
msgid ""
"The id to use when padding the labels (-100 will be automatically ignored"
" by PyTorch loss functions)."
msgstr ""

#: ../../source/main_classes/data_collator.rst:54
msgid "DataCollatorForLanguageModeling"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling:1
msgid ""
"Data collator used for language modeling. Inputs are dynamically padded "
"to the maximum length of a batch if they are not all of the same length."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling:6
msgid ""
"Whether or not to use masked language modeling. If set to :obj:`False`, "
"the labels are the same as the inputs with the padding tokens ignored (by"
" setting them to -100). Otherwise, the labels are -100 for non-masked "
"tokens and the value to predict for the masked token."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling:10
msgid ""
"The probability with which to (randomly) mask tokens in the input, when "
":obj:`mlm` is set to :obj:`True`."
msgstr ""

#: of transformers.data.data_collator.DataCollatorForLanguageModeling:17
msgid ""
"For best performance, this data collator should be used with a dataset "
"having items that are dictionaries or BatchEncoding, with the "
":obj:`\"special_tokens_mask\"` key, as returned by a "
":class:`~transformers.PreTrainedTokenizer` or a "
":class:`~transformers.PreTrainedTokenizerFast` with the argument "
":obj:`return_special_tokens_mask=True`."
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForLanguageModeling.mask_tokens:1
#, python-format
msgid ""
"Prepare masked tokens inputs/labels for masked language modeling: 80% "
"MASK, 10% random, 10% original."
msgstr ""

#: ../../source/main_classes/data_collator.rst:61
msgid "DataCollatorForWholeWordMask"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForWholeWordMask:1
msgid "Data collator used for language modeling that masks entire words."
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling:3
#: transformers.data.data_collator.DataCollatorForWholeWordMask:3
msgid "collates batches of tensors, honoring their tokenizer's pad_token"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForWholeWordMask:4
msgid "preprocesses batches for masked language modeling"
msgstr ""

#: of transformers.data.data_collator.DataCollatorForWholeWordMask:8
msgid ""
"This collator relies on details of the implementation of subword "
"tokenization by :class:`~transformers.BertTokenizer`, specifically that "
"subword tokens are prefixed with `##`. For tokenizers that do not adhere "
"to this scheme, this collator will produce an output that is roughly "
"equivalent to :class:`.DataCollatorForLanguageModeling`."
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForWholeWordMask.mask_tokens:1
#, python-format
msgid ""
"Prepare masked tokens inputs/labels for masked language modeling: 80% "
"MASK, 10% random, 10% original. Set 'mask_labels' means we use whole word"
" mask (wwm), we directly mask idxs according to it's ref."
msgstr ""

#: ../../source/main_classes/data_collator.rst:68
msgid "DataCollatorForPermutationLanguageModeling"
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling:1
msgid "Data collator used for permutation language modeling."
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling:4
msgid ""
"preprocesses batches for permutation language modeling with procedures "
"specific to XLNet"
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens:1
msgid ""
"The masked tokens to be predicted for a particular sequence are "
"determined by the following algorithm:"
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens:3
msgid ""
"Start from the beginning of the sequence by setting ``cur_len = 0`` "
"(number of tokens processed so far)."
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens:4
msgid ""
"Sample a ``span_length`` from the interval ``[1, max_span_length]`` "
"(length of span of tokens to be masked)"
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens:6
msgid ""
"Reserve a context of length ``context_length = span_length / "
"plm_probability`` to surround span to be masked"
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens:8
msgid ""
"Sample a starting point ``start_index`` from the interval ``[cur_len, "
"cur_len + context_length - span_length]`` and mask tokens "
"``start_index:start_index + span_length``"
msgstr ""

#: of
#: transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens:10
msgid ""
"Set ``cur_len = cur_len + context_length``. If ``cur_len < max_len`` "
"(i.e. there are tokens remaining in the sequence to be processed), repeat"
" from Step 1."
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/main_classes/deepspeed.rst:15
msgid "DeepSpeed Integration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:18
msgid ""
"`DeepSpeed <https://github.com/microsoft/DeepSpeed>`__ implements "
"everything described in the `ZeRO paper "
"<https://arxiv.org/abs/1910.02054>`__. Currently it provides full support"
" for:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:21
msgid "Optimizer state partitioning (ZeRO stage 1)"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:22
msgid "Gradient partitioning (ZeRO stage 2)"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:23
msgid "Parameter partitioning (ZeRO stage 3)"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:24
msgid "Custom mixed precision training handling"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:25
msgid "A range of fast CUDA-extension-based optimizers"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:26
msgid "ZeRO-Offload to CPU and NVMe"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:28
msgid ""
"ZeRO-Offload has its own dedicated paper: `ZeRO-Offload: Democratizing "
"Billion-Scale Model Training <https://arxiv.org/abs/2101.06840>`__. And "
"NVMe-support is described in the paper `ZeRO-Infinity: Breaking the GPU "
"Memory Wall for Extreme Scale Deep Learning "
"<https://arxiv.org/abs/2104.07857>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:32
msgid ""
"DeepSpeed ZeRO-2 is primarily used only for training, as its features are"
" of no use to inference."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:34
msgid ""
"DeepSpeed ZeRO-3 can be used for inference as well, since it allows huge "
"models to be loaded on multiple GPUs, which won't be possible on a single"
" GPU."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:39
msgid ""
"ðŸ¤— Transformers integrates `DeepSpeed "
"<https://github.com/microsoft/DeepSpeed>`__ via 2 options:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:41
msgid ""
"Integration of the core DeepSpeed features via "
":class:`~transformers.Trainer`. This is everything done for you type of "
"integration - just supply your custom config file or use our template and"
" you have nothing else to do. Most of this document is focused on this "
"feature."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:44
msgid ""
"If you don't use :class:`~transformers.Trainer` and want to use your own "
"Trainer where you integrated DeepSpeed yourself, core functionality "
"functions like ``from_pretrained`` and ``from_config`` include "
"integration of essential parts of DeepSpeed like ``zero.Init`` for ZeRO "
"stage 3 and higher. To tap into this feature read the docs on :ref"
":`deepspeed-non-trainer-integration`."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:56
msgid "Trainer Deepspeed Integration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:62
msgid "Installation"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:64
msgid "Install the library via pypi:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:70
msgid "or via ``transformers``' ``extras``:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:76
msgid ""
"or find more details on `the DeepSpeed's GitHub page "
"<https://github.com/microsoft/deepspeed#installation>`__ and `advanced "
"install <https://www.deepspeed.ai/tutorials/advanced-install/>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:79
msgid ""
"If you're still struggling with the build, first make sure to read :ref"
":`zero-install-notes`."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:81
msgid ""
"If you don't prebuild the extensions and rely on them to be built at run "
"time and you tried all of the above solutions to no avail, the next thing"
" to try is to pre-build the modules before installing them."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:84
msgid "To make a local build for DeepSpeed:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:95
msgid ""
"If you intend to use NVMe offload you will need to also include "
"``DS_BUILD_AIO=1`` in the instructions above (and also install `libaio-"
"dev` system-wide)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:98
msgid ""
"Edit ``TORCH_CUDA_ARCH_LIST`` to insert the code for the architectures of"
" the GPU cards you intend to use. Assuming all your cards are the same "
"you can get the arch via:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:105
msgid ""
"So if you get ``8, 6``, then use ``TORCH_CUDA_ARCH_LIST=\"8.6\"``. If you"
" have multiple different cards, you can list all of them like so "
"``TORCH_CUDA_ARCH_LIST=\"6.1;8.6\"``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:108
msgid ""
"If you need to use the same setup on multiple machines, make a binary "
"wheel:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:118
msgid ""
"it will generate something like ``dist/deepspeed-0.3.13+8cd046f-"
"cp38-cp38-linux_x86_64.whl`` which now you can install as ``pip install "
"deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`` locally or on any "
"other machine."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:121
msgid ""
"Again, remember to ensure to adjust ``TORCH_CUDA_ARCH_LIST`` to the "
"target architectures."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:123
msgid ""
"You can find the complete list of NVIDIA GPUs and their corresponding "
"**Compute Capabilities** (same as arch in this context) `here "
"<https://developer.nvidia.com/cuda-gpus>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:126
msgid "You can check the archs pytorch was built with using:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:132
msgid ""
"Here is how to find out the arch for one of the installed GPU. For "
"example, for GPU 0:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:139
msgid "If the output is:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:145
msgid "then you know that this card's arch is ``8.6``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:147
msgid ""
"You can also leave ``TORCH_CUDA_ARCH_LIST`` out completely and then the "
"build program will automatically query the architecture of the GPUs the "
"build is made on. This may or may not match the GPUs on the target "
"machines, that's why it's best to specify the desired archs explicitly."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:151
msgid ""
"If after trying everything suggested you still encounter build issues, "
"please, proceed with the GitHub Issue of `Deepspeed "
"<https://github.com/microsoft/DeepSpeed/issues>`__,"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:159
msgid "Deployment with multiple GPUs"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:161
msgid ""
"To deploy this feature with multiple GPUs adjust the "
":class:`~transformers.Trainer` command line arguments as following:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:164
msgid "replace ``python -m torch.distributed.launch`` with ``deepspeed``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:165
msgid ""
"add a new argument ``--deepspeed ds_config.json``, where "
"``ds_config.json`` is the DeepSpeed configuration file as documented "
"`here <https://www.deepspeed.ai/docs/config-json/>`__. The file naming is"
" up to you."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:168
msgid "Therefore, if your original command line looked as following:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:174
msgid "Now it should be:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:180
msgid ""
"Unlike, ``torch.distributed.launch`` where you have to specify how many "
"GPUs to use with ``--nproc_per_node``, with the ``deepspeed`` launcher "
"you don't have to use the corresponding ``--num_gpus`` if you want all of"
" your GPUs used. The full details on how to configure various nodes and "
"GPUs can be found `here <https://www.deepspeed.ai/getting-started"
"/#resource-configuration-multi-node>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:185
msgid ""
"In fact, you can continue using ``-m torch.distributed.launch`` with "
"DeepSpeed as long as you don't need to use ``deepspeed`` launcher-"
"specific arguments. Typically if you don't need a multi-node setup you're"
" not required to use the ``deepspeed`` launcher. But since in the "
"DeepSpeed documentation it'll be used everywhere, for consistency we will"
" use it here as well."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:190
msgid ""
"Here is an example of running ``run_translation.py`` under DeepSpeed "
"deploying all available GPUs:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:203
msgid ""
"Note that in the DeepSpeed documentation you are likely to see "
"``--deepspeed --deepspeed_config ds_config.json`` - i.e. two DeepSpeed-"
"related arguments, but for the sake of simplicity, and since there are "
"already so many arguments to deal with, we combined the two into a single"
" argument."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:207
msgid ""
"For some practical usage examples, please, see this `post "
"<https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:215
msgid "Deployment with one GPU"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:217
msgid ""
"To deploy DeepSpeed with one GPU adjust the "
":class:`~transformers.Trainer` command line arguments as following:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:229
msgid ""
"This is almost the same as with multiple-GPUs, but here we tell DeepSpeed"
" explicitly to use just one GPU via ``--num_gpus=1``. By default, "
"DeepSpeed deploys all GPUs it can see on the given node. If you have only"
" 1 GPU to start with, then you don't need this argument. The following "
"`documentation <https://www.deepspeed.ai/getting-started/#resource-"
"configuration-multi-node>`__ discusses the launcher options."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:234
msgid "Why would you want to use DeepSpeed with just one GPU?"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:236
msgid ""
"It has a ZeRO-offload feature which can delegate some computations and "
"memory to the host's CPU and RAM, and thus leave more GPU resources for "
"model's needs - e.g. larger batch size, or enabling a fitting of a very "
"big model which normally won't fit."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:239
msgid ""
"It provides a smart GPU memory management system, that minimizes memory "
"fragmentation, which again allows you to fit bigger models and data "
"batches."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:242
msgid ""
"While we are going to discuss the configuration in details next, the key "
"to getting a huge improvement on a single GPU with DeepSpeed is to have "
"at least the following configuration in the configuration file:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:263
msgid ""
"which enables optimizer offload and some other important features. You "
"may experiment with the buffer sizes, you will find more details in the "
"discussion below."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:266
msgid ""
"For a practical usage example of this type of deployment, please, see "
"this `post "
"<https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:269
msgid ""
"You may also try the ZeRO-3 with CPU and NVMe offload as explained "
"further in this document."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:271
msgid ""
"<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3"
" vs. ZeRO-2 on a single GPU, and then recommend ZeRO-3 config as starting"
" one. -->"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:274
msgid "Notes:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:276
msgid ""
"if you need to run on a specific GPU, which is different from GPU 0, you "
"can't use ``CUDA_VISIBLE_DEVICES`` to limit the visible scope of "
"available GPUs. Instead, you have to use the following syntax:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:283
msgid "In this example, we tell DeepSpeed to use GPU 1 (second gpu)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:290
msgid "Deployment in Notebooks"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:292
msgid ""
"The problem with running notebook cells as a script is that there is no "
"normal ``deepspeed`` launcher to rely on, so under certain setups we have"
" to emulate it."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:295
msgid ""
"If you're using only 1 GPU, here is how you'd have to adjust your "
"training code in the notebook to use DeepSpeed."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:313
msgid ""
"Note: ``...`` stands for the normal arguments that you'd pass to the "
"functions."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:315
msgid ""
"If you want to use more than 1 GPU, you must use a multi-process "
"environment for DeepSpeed to work. That is, you have to use the launcher "
"for that purpose and this cannot be accomplished by emulating the "
"distributed environment presented at the beginning of this section."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:319
msgid ""
"If you want to create the config file on the fly in the notebook in the "
"current directory, you could have a dedicated cell with:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:386
msgid ""
"If the training script is in a normal file and not in the notebook cells,"
" you can launch ``deepspeed`` normally via shell from a cell. For "
"example, to use ``run_translation.py`` you would launch it with:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:394
#, python-format
msgid ""
"or with ``%%bash`` magic, where you can write a multi-line code for the "
"shell program to run:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:404
msgid ""
"In such case you don't need any of the code presented at the beginning of"
" this section."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:406
#, python-format
msgid ""
"Note: While ``%%bash`` magic is neat, but currently it buffers the output"
" so you won't see the logs until the process completes."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:415
msgid "Configuration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:417
msgid ""
"For the complete guide to the DeepSpeed configuration options that can be"
" used in its configuration file please refer to the `following "
"documentation <https://www.deepspeed.ai/docs/config-json/>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:420
msgid ""
"You can find dozens of DeepSpeed configuration examples that address "
"various practical needs in `the DeepSpeedExamples repo "
"<https://github.com/microsoft/DeepSpeedExamples>`__:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:429
msgid ""
"Continuing the code from above, let's say you're looking to configure the"
" Lamb optimizer. So you can search through the example ``.json`` files "
"with:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:436
msgid ""
"Some more examples are to be found in the `main repo "
"<https://github.com/microsoft/DeepSpeed>`__ as well."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:438
msgid ""
"When using DeepSpeed you always need to supply a DeepSpeed configuration "
"file, yet some configuration parameters have to be configured via the "
"command line. You will find the nuances in the rest of this guide."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:441
msgid ""
"To get an idea of what DeepSpeed configuration file looks like, here is "
"one that activates ZeRO stage 2 features, including optimizer states cpu "
"offload, uses ``AdamW`` optimizer and ``WarmupLR`` scheduler and will "
"enable mixed precision training if ``--fp16`` is passed:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:496
msgid ""
"When you execute the program, DeepSpeed will log the configuration it "
"received from the :class:`~transformers.Trainer` to the console, so you "
"can see exactly what was the final configuration passed to it."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:504
msgid "Passing Configuration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:506
msgid ""
"As discussed in this document normally the DeepSpeed configuration is "
"passed as a path to a json file, but if you're not using the command line"
" interface to configure the training, and instead instantiate the "
":class:`~transformers.Trainer` via "
":class:`~transformers.TrainingArguments` then for the ``deepspeed`` "
"argument you can pass a nested ``dict``. This allows you to create the "
"configuration on the fly and doesn't require you to write it to the file "
"system before passing it to :class:`~transformers.TrainingArguments`."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:512
msgid "To summarize you can do:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:518
msgid "or:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:530
msgid "Shared Configuration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:535
msgid "This section is a must-read"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:537
msgid ""
"Some configuration values are required by both the "
":class:`~transformers.Trainer` and DeepSpeed to function correctly, "
"therefore, to prevent conflicting definitions, which could lead to hard "
"to detect errors, we chose to configure those via the "
":class:`~transformers.Trainer` command line arguments."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:541
msgid ""
"Additionally, some configuration values are derived automatically based "
"on the model's configuration, so instead of remembering to manually "
"adjust multiple values, it's the best to let the "
":class:`~transformers.Trainer` do the majority of configuration for you."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:545
msgid ""
"Therefore, in the rest of this guide you will find a special "
"configuration value: ``auto``, which when set will be automatically "
"replaced with the correct or most efficient value. Please feel free to "
"choose to ignore this recommendation and set the values explicitly, in "
"which case be very careful that your the :class:`~transformers.Trainer` "
"arguments and DeepSpeed configurations agree. For example, are you using "
"the same learning rate, or batch size, or gradient accumulation settings?"
" if these mismatch the training may fail in very difficult to detect "
"ways. You have been warned."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:552
msgid ""
"There are multiple other values that are specific to DeepSpeed-only and "
"those you will have to set manually to suit your needs."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:555
msgid ""
"In your own programs, you can also use the following approach if you'd "
"like to modify the DeepSpeed config as a master and configure "
":class:`~transformers.TrainingArguments` based on that. The steps are:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:558
msgid ""
"Create or load the DeepSpeed configuration to be used as a master "
"configuration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:559
msgid ""
"Create the :class:`~transformers.TrainingArguments` object based on these"
" values"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:561
msgid ""
"Do note that some values, such as :obj:`scheduler.params.total_num_steps`"
" are calculated by :class:`~transformers.Trainer` during ``train``, but "
"you can of course do the math yourself."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:567
msgid "ZeRO"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:569
msgid ""
"`Zero Redundancy Optimizer (ZeRO) "
"<https://www.deepspeed.ai/tutorials/zero/>`__ is the workhorse of "
"DeepSpeed. It support 3 different levels (stages) of optimization. The "
"first one is not quite interesting for scalability purposes, therefore "
"this document focuses on stages 2 and 3. Stage 3 is further improved by "
"the latest addition of ZeRO-Infinity. You will find more indepth "
"information in the DeepSpeed documentation."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:574
msgid ""
"The ``zero_optimization`` section of the configuration file is the most "
"important part (`docs <https://www.deepspeed.ai/docs/config-json/#zero-"
"optimizations-for-fp16-training>`__), since that is where you define "
"which ZeRO stages you want to enable and how to configure them. You will "
"find the explanation for each parameter in the DeepSpeed docs."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:579
msgid ""
"This section has to be configured exclusively via DeepSpeed configuration"
" - the :class:`~transformers.Trainer` provides no equivalent command line"
" arguments."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:582
msgid ""
"Note: currently DeepSpeed doesn't validate parameter names, so if you "
"misspell any, it'll use the default setting for the parameter that got "
"misspelled. You can watch the DeepSpeed engine start up log messages to "
"see what values it is going to use."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:591
msgid "ZeRO-2 Config"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:593
msgid "The following is an example configuration for ZeRO stage 2:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:613
#: ../../source/main_classes/deepspeed.rst:668
msgid "**Performance tuning:**"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:615
msgid ""
"enabling ``offload_optimizer`` should reduce GPU RAM usage (it requires "
"``\"stage\": 2``)"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:616
msgid ""
"``\"overlap_comm\": true`` trades off increased GPU RAM usage to lower "
"all-reduce latency. ``overlap_comm`` uses 4.5x the "
"``allgather_bucket_size`` and ``reduce_bucket_size`` values. So if they "
"are set to 5e8, this requires a 9GB footprint (``5e8 x 2Bytes x 2 x "
"4.5``). Therefore, if you have a GPU with 8GB or less RAM, to avoid "
"getting OOM-errors you will need to reduce those parameters to about "
"``2e8``, which would require 3.6GB. You will want to do the same on "
"larger capacity GPU as well, if you're starting to hit OOM."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:621
msgid ""
"when reducing these buffers you're trading communication speed to avail "
"more GPU RAM. The smaller the buffer size, the slower the communication, "
"and the more GPU RAM will be available to other tasks. So if a bigger "
"batch size is important, getting a slightly slower training time could be"
" a good trade."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:630
msgid "ZeRO-3 Config"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:632
msgid "The following is an example configuration for ZeRO stage 3:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:659
msgid ""
"If you are getting OOMs, because your model or activations don't fit into"
" the GPU memory and you have unutilized CPU memory offloading the "
"optimizer states and parameters to CPU memory with ``\"device\": "
"\"cpu\"`` may solve this limitation. If you don't want to offload to CPU "
"memory, use ``none`` instead of ``cpu`` for the ``device`` entry. "
"Offloading to NVMe is discussed further down."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:664
msgid ""
"Pinned memory is enabled with ``pin_memory`` set to ``true``. This "
"feature can improve the throughput at the cost of making less memory "
"available to other processes. Pinned memory is set aside to the specific "
"process that requested it and its typically accessed much faster than "
"normal CPU memory."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:670
msgid "``stage3_max_live_parameters``: ``1e9``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:671
msgid "``stage3_max_reuse_distance``: ``1e9``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:673
msgid ""
"If hitting OOM reduce ``stage3_max_live_parameters`` and "
"``stage3_max_reuse_distance``. They should have minimal impact on "
"performance unless you are doing activation checkpointing. ``1e9`` would "
"consume ~2GB. The memory is shared by ``stage3_max_live_parameters`` and "
"``stage3_max_reuse_distance``, so its not additive, its just 2GB total."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:677
msgid ""
"``stage3_max_live_parameters`` is the upper limit on how many full "
"parameters you want to keep on the GPU at any given time. \"reuse "
"distance\" is a metric we are using to figure out when will a parameter "
"be used again in the future, and we use the ``stage3_max_reuse_distance``"
" to decide whether to throw away the parameter or to keep it. If a "
"parameter is going to be used again in near future (less than "
"``stage3_max_reuse_distance``) then we keep it to reduce communication "
"overhead. This is super helpful when you have activation checkpointing "
"enabled, where we do a forward recompute and backward passes a a single "
"layer granularity and want to keep the parameter in the forward recompute"
" till the backward"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:684
msgid "The following configuration values depend on the model's hidden size:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:686
msgid "``reduce_bucket_size``: ``hidden_size*hidden_size``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:687
msgid "``stage3_prefetch_bucket_size``: ``0.9 * hidden_size * hidden_size``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:688
msgid "``stage3_param_persistence_threshold``: ``10 * hidden_size``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:690
msgid ""
"therefore set these values to ``auto`` and the "
":class:`~transformers.Trainer` will automatically assign the recommended "
"values. But, of course, feel free to set these explicitly as well."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:693
msgid ""
"``stage3_gather_fp16_weights_on_model_save`` enables model fp16 weights "
"consolidation when model gets saved. With large models and multiple GPUs "
"this is an expensive operation both in terms of memory and speed. It's "
"currently required if you plan to resume the training. Watch out for "
"future updates that will remove this limitation and make things more "
"flexible."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:698
msgid ""
"If you're migrating from ZeRO-2 configuration note that "
"``allgather_partitions``, ``allgather_bucket_size`` and "
"``reduce_scatter`` configuration parameters are not used in ZeRO-3. If "
"you keep these in the config file they will just be ignored."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:702
msgid "``sub_group_size``: ``1e9``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:704
msgid ""
"``sub_group_size`` controls the granularity in which parameters are "
"updated during optimizer steps. Parameters are grouped into buckets of "
"``sub_group_size`` and each buckets is updated one at a time. When used "
"with NVMe offload in ZeRO-Infinity, ``sub_group_size`` therefore controls"
" the granularity in which model states are moved in and out of CPU memory"
" from NVMe during the optimizer step. This prevents running out of CPU "
"memory for extremely large models."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:709
msgid ""
"You can leave ``sub_group_size`` to its default value of `1e9` when not "
"using NVMe offload. You may want to change its default value in the "
"following cases:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:712
msgid ""
"Running into OOM during optimizer step: Reduce ``sub_group_size`` to "
"reduce memory utilization of temporary buffers"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:713
msgid ""
"Optimizer Step is taking a long time: Increase ``sub_group_size`` to "
"improve bandwidth utilization as a result of the increased data buffers."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:720
msgid "NVMe Support"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:722
msgid ""
"ZeRO-Infinity allows for training incredibly large models by extending "
"GPU and CPU memory with NVMe memory. Thanks to smart partitioning and "
"tiling algorithms each GPU needs to send and receive very small amounts "
"of data during offloading so modern NVMe proved to be fit to allow for an"
" even larger total memory pool available to your training process. ZeRO-"
"Infinity requires ZeRO-3 enabled."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:727
msgid ""
"The following configuration example enables NVMe to offload both "
"optimizer states and the params:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:768
msgid ""
"You can choose to offload both optimizer states and params to NVMe, or "
"just one of them or none. For example, if you have copious amounts of CPU"
" memory available, by all means offload to CPU memory only as it'd be "
"faster (hint: `\"device\": \"cpu\"`)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:772
msgid ""
"Here is the full documentation for offloading `optimizer states "
"<https://www.deepspeed.ai/docs/config-json/#optimizer-offloading>`__ and "
"`parameters <https://www.deepspeed.ai/docs/config-json/#parameter-"
"offloading>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:776
msgid ""
"Make sure that your ``nvme_path`` is actually an NVMe, since it will work"
" with the normal hard drive or SSD, but it'll be much much slower. The "
"fast scalable training was designed with modern NVMe transfer speeds in "
"mind (as of this writing one can have ~3.5GB/s read, ~3GB/s write peak "
"speeds)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:780
msgid ""
"In order to figure out the optimal ``aio`` configuration block you must "
"run a benchmark on your target setup, as `explained here "
"<https://github.com/microsoft/DeepSpeed/issues/998>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:788
msgid "ZeRO-2 vs ZeRO-3 Performance"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:790
msgid ""
"ZeRO-3 is likely to be slower than ZeRO-2 if everything else is "
"configured the same because the former has to gather model weights in "
"addition to what ZeRO-2 does. If ZeRO-2 meets your needs and you don't "
"need to scale beyond a few GPUs then you may choose to stick to it. It's "
"important to understand that ZeRO-3 enables a much higher scalability "
"capacity at a cost of speed."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:795
msgid ""
"It's possible to adjust ZeRO-3 configuration to make it perform closer to"
" ZeRO-2:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:797
msgid ""
"set ``stage3_param_persistence_threshold`` to a very large number - "
"larger than the largest parameter, e.g., ``6 * hidden_size * "
"hidden_size``. This will keep the parameters on the GPUs."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:799
msgid "turn off ``offload_params`` since ZeRO-2 doesn't have that option."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:801
msgid ""
"The performance will likely improve significantly with just "
"``offload_params`` turned off, even if you don't change "
"``stage3_param_persistence_threshold``. Of course, these changes will "
"impact the size of the model you can train. So these help you to trade "
"scalability for speed depending on your needs."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:810
msgid "ZeRO-2 Example"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:812
msgid "Here is a full ZeRO-2 auto-configuration file ``ds_config_zero2.json``:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:868
msgid ""
"Here is a full ZeRO-2 all-enabled manually set configuration file. It is "
"here mainly for you to see what the typical values look like, but we "
"highly recommend using the one with multiple ``auto`` settings in it."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:925
msgid "ZeRO-3 Example"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:927
msgid "Here is a full ZeRO-3 auto-configuration file ``ds_config_zero3.json``:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:990
msgid ""
"Here is a full ZeRO-3 all-enabled manually set configuration file. It is "
"here mainly for you to see what the typical values look like, but we "
"highly recommend using the one with multiple ``auto`` settings in it."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1051
msgid "Optimizer and Scheduler"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1053
msgid ""
"As long as you don't enable ``offload_optimizer`` you can mix and match "
"DeepSpeed and HuggingFace schedulers and optimizers, with the exception "
"of using the combination of HuggingFace scheduler and DeepSpeed "
"optimizer:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1057
msgid "Combos"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1057
msgid "HF Scheduler"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1057
msgid "DS Scheduler"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1059
msgid "HF Optimizer"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1059
#: ../../source/main_classes/deepspeed.rst:1061
msgid "Yes"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1061
msgid "DS Optimizer"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1061
msgid "No"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1064
msgid ""
"It is possible to use a non-DeepSpeed optimizer when "
"``offload_optimizer`` is enabled, as long as it has both CPU and GPU "
"implementation (except LAMB)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1073
msgid "Optimizer"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1076
msgid ""
"DeepSpeed's main optimizers are Adam, AdamW, OneBitAdam, and Lamb. These "
"have been thoroughly tested with ZeRO and are thus recommended to be "
"used. It, however, can import other optimizers from ``torch``. The full "
"documentation is `here <https://www.deepspeed.ai/docs/config-json"
"/#optimizer-parameters>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1080
msgid ""
"If you don't configure the ``optimizer`` entry in the configuration file,"
" the :class:`~transformers.Trainer` will automatically set it to "
"``AdamW`` and will use the supplied values or the defaults for the "
"following command line arguments: ``--learning_rate``, ``--adam_beta1``, "
"``--adam_beta2``, ``--adam_epsilon`` and ``--weight_decay``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1084
msgid ""
"Here is an example of the auto-configured ``optimizer`` entry for "
"``AdamW``:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1101
msgid ""
"Note that the command line arguments will set the values in the "
"configuration file. This is so that there is one definitive source of the"
" values and to avoid hard to find errors when for example, the learning "
"rate is set to different values in different places. Command line rules. "
"The values that get overridden are:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1105
msgid "``lr`` with the value of ``--learning_rate``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1106
msgid "``betas`` with the value of ``--adam_beta1 --adam_beta2``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1107
msgid "``eps`` with the value of ``--adam_epsilon``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1108
msgid "``weight_decay`` with the value of ``--weight_decay``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1110
msgid ""
"Therefore please remember to tune the shared hyperparameters on the "
"command line."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1112
#: ../../source/main_classes/deepspeed.rst:1357
msgid "You can also set the values explicitly:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1128
#: ../../source/main_classes/deepspeed.rst:1204
#: ../../source/main_classes/deepspeed.rst:1301
#: ../../source/main_classes/deepspeed.rst:1331
#: ../../source/main_classes/deepspeed.rst:1366
#: ../../source/main_classes/deepspeed.rst:1394
#: ../../source/main_classes/deepspeed.rst:1422
msgid ""
"But then you're on your own synchronizing the "
":class:`~transformers.Trainer` command line arguments and the DeepSpeed "
"configuration."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1131
msgid ""
"If you want to use another optimizer which is not listed above, you will "
"have to add to the top level configuration."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1139
msgid ""
"Similarly to ``AdamW``, you can configure other officially supported "
"optimizers. Just remember that may have different config values. e.g. for"
" Adam you will want ``weight_decay`` around ``0.01``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1147
msgid "Scheduler"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1149
msgid ""
"DeepSpeed supports ``LRRangeTest``, ``OneCycle``, ``WarmupLR`` and "
"``WarmupDecayLR`` learning rate schedulers. The full documentation is "
"`here <https://www.deepspeed.ai/docs/config-json/#scheduler-"
"parameters>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1152
msgid "Here is where the schedulers overlap between ðŸ¤— Transformers and DeepSpeed:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1154
msgid "``WarmupLR`` via ``--lr_scheduler_type constant_with_warmup``"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1155
msgid ""
"``WarmupDecayLR`` via ``--lr_scheduler_type linear``. This is also the "
"default value for ``--lr_scheduler_type``, therefore, if you don't "
"configure the scheduler this is scheduler that will get configured by "
"default."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1158
msgid ""
"If you don't configure the ``scheduler`` entry in the configuration file,"
" the :class:`~transformers.Trainer` will use the values of "
"``--lr_scheduler_type``, ``--learning_rate`` and ``--warmup_steps`` or "
"``--warmup_ratio`` to configure a ðŸ¤— Transformers version of it."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1162
msgid ""
"Here is an example of the auto-configured ``scheduler`` entry for "
"``WarmupLR``:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1177
msgid ""
"Since `\"auto\"` is used the :class:`~transformers.Trainer` arguments "
"will set the correct values in the configuration file. This is so that "
"there is one definitive source of the values and to avoid hard to find "
"errors when, for example, the learning rate is set to different values in"
" different places. Command line rules. The values that get set are:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1181
msgid "``warmup_min_lr`` with the value of ``0``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1182
msgid "``warmup_max_lr`` with the value of ``--learning_rate``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1183
msgid ""
"``warmup_num_steps`` with the value of ``--warmup_steps`` if provided. "
"Otherwise will use ``--warmup_ratio`` multiplied by the number of "
"training steps and rounded up."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1185
msgid ""
"``total_num_steps`` with either the value of ``--max_steps`` or if it is "
"not provided, derived automatically at run time based on the environment "
"and the size of the dataset and other command line arguments (needed for "
"``WarmupDecayLR``)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1189
msgid ""
"You can, of course, take over any or all of the configuration values and "
"set those yourself:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1207
msgid "For example, for ``WarmupDecayLR``, you can use the following entry:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1224
msgid ""
"and ``total_num_steps`, ``warmup_max_lr``, ``warmup_num_steps`` and "
"``total_num_steps`` will be set at loading time."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1232
msgid "fp32 Precision"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1234
msgid "Deepspeed supports the full fp32 and the fp16 mixed precision."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1236
msgid ""
"Because of the much reduced memory needs and faster speed one gets with "
"the fp16 mixed precision, the only time you will want to not use it is "
"when the model you're using doesn't behave well under this training mode."
" Typically this happens when the model wasn't pretrained in the fp16 "
"mixed precision (e.g. often this happens with bf16-pretrained models). "
"Such models may overflow or underflow leading to ``NaN`` loss. If this is"
" your case then you will want to use the full fp32 mode, by explicitly "
"disabling the otherwise default fp16 mixed precision mode with:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1250
msgid ""
"If you're using the Ampere-architecture based GPU, pytorch version 1.7 "
"and higher will automatically switch to using the much more efficient "
"tf32 format for some operations, but the results will still be in fp32. "
"For details and benchmarks, please, see `TensorFloat-32(TF32) on Ampere "
"devices "
"<https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-"
"ampere-devices>`__. The document includes instructions on how to disable "
"this automatic conversion if for some reason you prefer not to use it."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1262
msgid "Automatic Mixed Precision"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1264
msgid ""
"You can use automatic mixed precision with either a pytorch-like AMP way "
"or the apex-like way:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1266
msgid "To configure pytorch AMP-like mode set:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1281
msgid ""
"and the :class:`~transformers.Trainer` will automatically enable or "
"disable it based on the value of ``args.fp16_backend``. The rest of "
"config values are up to you."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1284
msgid ""
"This mode gets enabled when ``--fp16 --fp16_backend amp`` command line "
"args are passed."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1286
msgid "You can also enable/disable this mode explicitly:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1304
msgid ""
"Here is the `documentation <https://www.deepspeed.ai/docs/config-"
"json/#fp16-training-options>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1306
msgid "To configure apex AMP-like mode set:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1315
msgid ""
"and the :class:`~transformers.Trainer` will automatically configure it "
"based on the values of ``args.fp16_backend`` and ``args.fp16_opt_level``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1318
msgid ""
"This mode gets enabled when ``--fp16 --fp16_backend apex --fp16_opt_level"
" 01`` command line args are passed."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1320
msgid "You can also configure this mode explicitly:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1334
msgid ""
"Here is the `documentation <https://www.deepspeed.ai/docs/config-json"
"/#automatic-mixed-precision-amp-training-options>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1342
msgid "Batch Size"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1344
msgid "To configure batch size, use:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1353
msgid ""
"and the :class:`~transformers.Trainer` will automatically set "
"``train_micro_batch_size_per_gpu`` to the value of "
"``args.per_device_train_batch_size`` and ``train_batch_size`` to "
"``args.world_size * args.per_device_train_batch_size * "
"args.gradient_accumulation_steps``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1374
msgid "Gradient Accumulation"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1376
msgid "To configure gradient accumulation set:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1384
msgid ""
"and the :class:`~transformers.Trainer` will automatically set it to the "
"value of ``args.gradient_accumulation_steps``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1386
#: ../../source/main_classes/deepspeed.rst:1414
msgid "You can also set the value explicitly:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1402
msgid "Gradient Clipping"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1404
msgid "To configure gradient gradient clipping set:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1412
msgid ""
"and the :class:`~transformers.Trainer` will automatically set it to the "
"value of ``args.max_grad_norm``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1430
msgid "Getting The Model Weights Out"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1432
msgid ""
"As long as you continue training and resuming using DeepSpeed you don't "
"need to worry about anything. DeepSpeed stores fp32 master weights in its"
" custom checkpoint optimizer files, which are "
"``global_step*/*optim_states.pt`` (this is glob pattern), and are saved "
"under the normal checkpoint."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1436
msgid "**FP16 Weights:**"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1438
msgid ""
"When a model is saved under ZeRO-2, you end up having the normal "
"``pytorch_model.bin`` file with the model weights, but they are only the "
"fp16 version of the weights."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1441
msgid ""
"Under ZeRO-3, things are much more complicated, since the model weights "
"are partitioned out over multiple GPUs, therefore "
"``\"stage3_gather_fp16_weights_on_model_save\": true`` is required to get"
" the ``Trainer`` to save the fp16 version of the weights. If this setting"
" is ``False`` ``pytorch_model.bin`` won't be created. This is because by "
"default DeepSpeed's ``state_dict`` contains a placeholder and not the "
"real weights. If we were to save this ``state_dict`` it won't be possible"
" to load it back."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1457
msgid "**FP32 Weights:**"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1459
msgid ""
"While the fp16 weights are fine for resuming training, if you finished "
"finetuning your model and want to upload it to the `models hub "
"<https://huggingface.co/models>`__ or pass it to someone else you most "
"likely will want to get the fp32 weights. This ideally shouldn't be done "
"during training since this is a process that requires a lot of memory, "
"and therefore best to be performed offline after the training is "
"complete. But if desired and you have plenty of free CPU memory it can be"
" done in the same training script. The following sections will discuss "
"both approaches."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1466
msgid "**Live FP32 Weights Recovery:**"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1468
msgid ""
"This approach may not work if you model is large and you have little free"
" CPU memory left, at the end of the training."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1470
msgid ""
"If you have saved at least one checkpoint, and you want to use the latest"
" one, you can do the following:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1479
msgid ""
"If you're using the ``--load_best_model_at_end`` "
"class:`~transformers.TrainingArguments` argument (to track the best "
"checkpoint), then you can finish the training by first saving the final "
"model explicitly and then do the same as above:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1491
msgid ""
"Note, that once ``load_state_dict_from_zero_checkpoint`` was run, the "
"``model`` will no longer be useable in the DeepSpeed context of the same "
"application. i.e. you will need to re-initialize the deepspeed engine, "
"since ``model.load_state_dict(state_dict)`` will remove all the DeepSpeed"
" magic from it. So do this only at the very end of the training."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1496
msgid ""
"Of course, you don't have to use class:`~transformers.Trainer` and you "
"can adjust the examples above to your own trainer."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1499
msgid ""
"If for some reason you want more refinement, you can also extract the "
"fp32 ``state_dict`` of the weights and apply these yourself as is shown "
"in the following example:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1510
msgid "**Offline FP32 Weights Recovery:**"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1512
msgid ""
"DeepSpeed creates a special conversion script ``zero_to_fp32.py`` which "
"it places in the top-level of the checkpoint folder. Using this script "
"you can extract the weights at any point. The script is standalone and "
"you no longer need to have the configuration file or a ``Trainer`` to do "
"the extraction."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1516
msgid "Let's say your checkpoint folder looks like this:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1534
msgid ""
"In this example there is just one DeepSpeed checkpoint sub-folder "
"`global_step1`. Therefore to reconstruct the fp32 weights just run:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1541
msgid ""
"This is it. ``pytorch_model.bin`` will now contain the full fp32 model "
"weights consolidated from multiple GPUs."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1543
msgid ""
"The script will automatically be able to handle either a ZeRO-2 or ZeRO-3"
" checkpoint."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1545
msgid "``python zero_to_fp32.py -h`` will give you usage details."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1547
msgid ""
"The script will auto-discover the deepspeed sub-folder using the contents"
" of the file ``latest``, which in the current example will contain "
"``global_step1``."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1550
msgid ""
"Note: currently the script requires 2x general RAM of the final fp32 "
"model weights."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1554
msgid "ZeRO-3 and Infinity Nuances"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1556
msgid ""
"ZeRO-3 is quite different from ZeRO-2 because of its param sharding "
"feature."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1558
msgid ""
"ZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple "
"other speed and scalability improvements."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1560
msgid ""
"While all the efforts were made for things to just work without needing "
"any special changes to your models, in certain circumstances you may find"
" the following information to be needed."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1566
msgid "Constructing Massive Models"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1568
msgid ""
"DeepSpeed/ZeRO-3 can handle models with Trillions of parameters which may"
" not fit onto the existing RAM. In such cases, but also if you want the "
"initialization to happen much faster, initialize the model using "
"`deepspeed.zero.Init()` context manager (which is also a function "
"decorator), like so:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1580
msgid "As you can see this gives you a randomly initialized model."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1582
msgid ""
"If you want to use a pretrained model, ``model_class.from_pretrained`` "
"will activate this feature as long as ``is_deepspeed_zero3_enabled()`` "
"returns ``True``, which currently is setup by the "
"class:`~transformers.TrainingArguments` object if the passed DeepSpeed "
"configuration file contains ZeRO-3 config section. Thus you must create "
"the :class:`~transformers.TrainingArguments` object **before** calling "
"``from_pretrained``. Here is an example of a possible sequence:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1595
msgid ""
"If you're using the official example scripts and your command line "
"arguments include ``--deepspeed ds_config.json`` with ZeRO-3 config "
"enabled, then everything is already done for you, since this is how "
"example scripts are written."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1598
msgid ""
"Note: If the fp16 weights of the model can't fit onto the memory of a "
"single GPU this feature must be used."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1600
msgid ""
"For full details on this method and other related features please refer "
"to `Constructing Massive Models "
"<https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-"
"massive-models>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1603
msgid ""
"Also when loading fp16-pretrained models, you will want to tell "
"``from_pretrained`` to use ``torch_dtype=torch.float16``. For details, "
"please, see :ref:`from_pretrained-torch-dtype`."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1608
msgid "Gathering Parameters"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1610
msgid ""
"Under ZeRO-3 on multiple GPUs no single GPU has all the parameters unless"
" it's the parameters for the currently executing layer. So if you need to"
" access all parameters from all layers at once there is a specific method"
" to do it. Most likely you won't need it, but if you do please refer to "
"`Gathering Parameters "
"<https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-"
"coordination>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1615
msgid ""
"We do however use it internally in several places, one such example is "
"when loading pretrained model weights in ``from_pretrained``. We load one"
" layer at a time and immediately partition it to all participating GPUs, "
"as for very large models it won't be possible to load it on one GPU and "
"then spread it out to multiple GPUs, due to memory limitations."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1620
msgid ""
"Also under ZeRO-3, if you write your own code and run into a model "
"parameter weight that looks like:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1626
msgid ""
"stress on ``tensor([1.])``, or if you get an error where it says the "
"parameter is of size ``1``, instead of some much larger multi-dimensional"
" shape, this means that the parameter is partitioned and what you see is "
"a ZeRO-3 placeholder."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1633
msgid "Filing Issues"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1635
msgid ""
"Here is how to file an issue so that we could quickly get to the bottom "
"of the issue and help you to unblock your work."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1637
msgid "In your report please always include:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1639
msgid "the full Deepspeed config file in the report"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1641
msgid ""
"either the command line arguments if you were using the "
":class:`~transformers.Trainer` or "
":class:`~transformers.TrainingArguments` arguments if you were scripting "
"the Trainer setup yourself. Please do not dump the "
":class:`~transformers.TrainingArguments` as it has dozens of entries that"
" are irrelevant."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1645
msgid "Output of:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1653
msgid ""
"If possible include a link to a Google Colab notebook that we can "
"reproduce the problem with. You can use this `notebook "
"<https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb>`__"
" as a starting point."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1657
msgid ""
"Unless it's impossible please always use a standard dataset that we can "
"use and not something custom."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1659
msgid ""
"If possible try to use one of the existing `examples "
"<https://github.com/huggingface/transformers/tree/master/examples/pytorch>`__"
" to reproduce the problem with."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1662
msgid "Things to consider:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1664
msgid "Deepspeed is often not the cause of the problem."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1666
msgid ""
"Some of the filed issues proved to be Deepspeed-unrelated. That is once "
"Deepspeed was removed from the setup, the problem was still there."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1669
msgid ""
"Therefore, if it's not absolutely obvious it's a DeepSpeed-related "
"problem, as in you can see that there is an exception and you can see "
"that DeepSpeed modules are involved, first re-test your setup without "
"DeepSpeed in it. And only if the problem persists then do mentioned "
"Deepspeed and supply all the required details."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1673
msgid ""
"If it's clear to you that the issue is in the DeepSpeed core and not the "
"integration part, please file the Issue directly with `Deepspeed "
"<https://github.com/microsoft/DeepSpeed/>`__. If you aren't sure, please "
"do not worry, either Issue tracker will do, we will figure it out once "
"you posted it and redirect you to another Issue tracker if need be."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1681
msgid "Troubleshooting"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1683
msgid "``deepspeed`` process gets killed at startup without a traceback"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1685
msgid ""
"If the ``deepspeed`` process gets killed at launch time without a "
"traceback, that usually means that the program tried to allocate more CPU"
" memory than your system has or your process is allowed to allocate and "
"the OS kernel killed that process. This is because your configuration "
"file most likely has either ``offload_optimizer`` or ``offload_param`` or"
" both configured to offload to ``cpu``. If you have NVMe, experiment with"
" offloading to NVMe if you're running under ZeRO-3."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1691
msgid ""
"Work is being done to enable estimating how much memory is needed for a "
"specific model: `PR <https://github.com/microsoft/DeepSpeed/pull/965>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1700
msgid "Notes"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1702
msgid ""
"DeepSpeed works with the PyTorch :class:`~transformers.Trainer` but not "
"TF :class:`~transformers.TFTrainer`."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1703
msgid ""
"While DeepSpeed has a pip installable PyPI package, it is highly "
"recommended that it gets installed from `source "
"<https://github.com/microsoft/deepspeed#installation>`__ to best match "
"your hardware and also if you need to enable certain features, like 1-bit"
" Adam, which aren't available in the pypi distribution."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1706
msgid ""
"You don't have to use the :class:`~transformers.Trainer` to use DeepSpeed"
" with ðŸ¤— Transformers - you can use any model with your own trainer, and "
"you will have to adapt the latter according to `the DeepSpeed integration"
" instructions <https://www.deepspeed.ai/getting-started/#writing-"
"deepspeed-models>`__."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1716
msgid "Non-Trainer Deepspeed Integration"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1718
msgid ""
"The :class:`~transformers.integrations.HfDeepSpeedConfig` is used to "
"integrate Deepspeed into the ðŸ¤— Transformers core functionality, when "
":class:`~transformers.Trainer` is not used."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1721
msgid ""
"When using :class:`~transformers.Trainer` everything is automatically "
"taken care of."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1723
msgid ""
"When not using :class:`~transformers.Trainer`, to efficiently deploy "
"DeepSpeed stage 3, you must instantiate the "
":class:`~transformers.integrations.HfDeepSpeedConfig` object before "
"instantiating the model."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1726
msgid "For example for a pretrained model:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1739
msgid "or for non-pretrained model:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1755
msgid "HfDeepSpeedConfig"
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig:1
msgid ""
"This object contains a DeepSpeed configuration dictionary and can be "
"quickly queried for things like zero stage."
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig:3
msgid ""
"A ``weakref`` of this object is stored in the module's globals to be able"
" to access the config from areas where things like the Trainer object is "
"not available (e.g. ``from_pretrained`` and ``_get_resized_embeddings``)."
" Therefore it's important that this object remains alive while the "
"program is still running."
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig:7
msgid ""
":class:`~transformers.Trainer` uses the ``HfTrainerDeepSpeedConfig`` "
"subclass instead. That subclass has logic to sync the configuration with "
"values of :class:`~transformers.TrainingArguments` by replacing special "
"placeholder values: ``\"auto\"``. Without this special logic the "
"DeepSpeed configuration is not modified in any way."
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig
msgid "Parameters"
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig.get_value:1
msgid "Returns the set value or ``default`` if no value is set"
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig.is_false:1
msgid ""
"Returns :obj:`True`/:obj:`False` only if the value is set, always "
":obj:`False` otherwise. So use this method to ask the very specific "
"question of whether the value is set to :obj:`False` (and it's not set to"
" :obj:`True` or isn't set)."
msgstr ""

#: of transformers.deepspeed.HfDeepSpeedConfig.is_true:1
msgid ""
"Returns :obj:`True`/:obj:`False` only if the value is set, always "
":obj:`False` otherwise. So use this method to ask the very specific "
"question of whether the value is set to :obj:`True` (and it's not set to "
":obj:`False` or isn't set)."
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1763
msgid "Main DeepSpeed Resources"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1765
msgid "`Project's github <https://github.com/microsoft/deepspeed>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1766
msgid "`Usage docs <https://www.deepspeed.ai/getting-started/>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1767
msgid "`API docs <https://deepspeed.readthedocs.io/en/latest/index.html>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1768
msgid ""
"`Blog posts <https://www.microsoft.com/en-"
"us/research/search/?q=deepspeed>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1770
msgid "Papers:"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1772
msgid ""
"`ZeRO: Memory Optimizations Toward Training Trillion Parameter Models "
"<https://arxiv.org/abs/1910.02054>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1773
msgid ""
"`ZeRO-Offload: Democratizing Billion-Scale Model Training "
"<https://arxiv.org/abs/2101.06840>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1774
msgid ""
"`ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep "
"Learning <https://arxiv.org/abs/2104.07857>`__"
msgstr ""

#: ../../source/main_classes/deepspeed.rst:1776
msgid ""
"Finally, please, remember that, HuggingFace "
":class:`~transformers.Trainer` only integrates DeepSpeed, therefore if "
"you have any problems or questions with regards to DeepSpeed usage, "
"please, file an issue with `DeepSpeed GitHub "
"<https://github.com/microsoft/DeepSpeed/issues>`__."
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/main_classes/configuration.rst:14
msgid "Configuration"
msgstr ""

#: ../../source/main_classes/configuration.rst:16
msgid ""
"The base class :class:`~transformers.PretrainedConfig` implements the "
"common methods for loading/saving a configuration either from a local "
"file or directory, or from a pretrained model configuration provided by "
"the library (downloaded from HuggingFace's AWS S3 repository)."
msgstr ""

#: ../../source/main_classes/configuration.rst:22
msgid "PretrainedConfig"
msgstr ""

#: of transformers.PretrainedConfig:1
msgid ""
"Base class for all configuration classes. Handles a few parameters common"
" to all models' configurations as well as methods for "
"loading/downloading/saving configurations."
msgstr ""

#: of transformers.PretrainedConfig:6
msgid ""
"A configuration file can be loaded and saved to disk. Loading the "
"configuration file and using this file to initialize a model does **not**"
" load the model weights. It only affects the model's configuration."
msgstr ""

#: of transformers.PretrainedConfig:9
msgid "Class attributes (overridden by derived classes)"
msgstr ""

#: of transformers.PretrainedConfig:11
msgid ""
"**model_type** (:obj:`str`) -- An identifier for the model type, "
"serialized into the JSON file, and used to recreate the correct object in"
" :class:`~transformers.AutoConfig`."
msgstr ""

#: of transformers.PretrainedConfig:13
msgid ""
"**is_composition** (:obj:`bool`) -- Whether the config class is composed "
"of multiple sub-configs. In this case the config has to be initialized "
"from two or more configs of type :class:`~transformers.PretrainedConfig` "
"like: :class:`~transformers.EncoderDecoderConfig` or :class:`~RagConfig`."
msgstr ""

#: of transformers.PretrainedConfig:17
msgid ""
"**keys_to_ignore_at_inference** (:obj:`List[str]`) -- A list of keys to "
"ignore by default when looking at dictionary outputs of the model during "
"inference."
msgstr ""

#: of transformers.PretrainedConfig:20
msgid "Common attributes (present in all subclasses)"
msgstr ""

#: of transformers.PretrainedConfig:22
msgid ""
"**vocab_size** (:obj:`int`) -- The number of tokens in the vocabulary, "
"which is also the first dimension of the embeddings matrix (this "
"attribute may be missing for models that don't have a text modality like "
"ViT)."
msgstr ""

#: of transformers.PretrainedConfig:24
msgid "**hidden_size** (:obj:`int`) -- The hidden size of the model."
msgstr ""

#: of transformers.PretrainedConfig:25
msgid ""
"**num_attention_heads** (:obj:`int`) -- The number of attention heads "
"used in the multi-head attention layers of the model."
msgstr ""

#: of transformers.PretrainedConfig:27
msgid "**num_hidden_layers** (:obj:`int`) -- The number of blocks in the model."
msgstr ""

#: of transformers.PretrainedConfig transformers.PretrainedConfig.from_dict
#: transformers.PretrainedConfig.from_json_file
#: transformers.PretrainedConfig.from_pretrained
#: transformers.PretrainedConfig.get_config_dict
#: transformers.PretrainedConfig.push_to_hub
#: transformers.PretrainedConfig.save_pretrained
#: transformers.PretrainedConfig.to_json_file
#: transformers.PretrainedConfig.to_json_string
#: transformers.PretrainedConfig.update
#: transformers.PretrainedConfig.update_from_string
msgid "Parameters"
msgstr ""

#: of transformers.PretrainedConfig:29
msgid ""
"Store the string that was passed to "
":func:`~transformers.PreTrainedModel.from_pretrained` or "
":func:`~transformers.TFPreTrainedModel.from_pretrained` as "
"``pretrained_model_name_or_path`` if the configuration was created with "
"such a method."
msgstr ""

#: of transformers.PretrainedConfig:33
msgid "Whether or not the model should return all hidden-states."
msgstr ""

#: of transformers.PretrainedConfig:35
msgid "Whether or not the model should returns all attentions."
msgstr ""

#: of transformers.PretrainedConfig:37
msgid ""
"Whether or not the model should return a "
":class:`~transformers.file_utils.ModelOutput` instead of a plain tuple."
msgstr ""

#: of transformers.PretrainedConfig:40
msgid "Whether the model is used as an encoder/decoder or not."
msgstr ""

#: of transformers.PretrainedConfig:42
msgid ""
"Whether the model is used as decoder or not (in which case it's used as "
"an encoder)."
msgstr ""

#: of transformers.PretrainedConfig:44
msgid ""
"Whether cross-attention layers should be added to the model. Note, this "
"option is only relevant for models that can be used as decoder models "
"within the `:class:~transformers.EncoderDecoderModel` class, which "
"consists of all models in ``AUTO_MODELS_FOR_CAUSAL_LM``."
msgstr ""

#: of transformers.PretrainedConfig:48
msgid ""
"Whether all encoder weights should be tied to their equivalent decoder "
"weights. This requires the encoder and decoder model to have the exact "
"same parameter names."
msgstr ""

#: of transformers.PretrainedConfig:51
msgid ""
"Pruned heads of the model. The keys are the selected layer indices and "
"the associated values, the list of heads to prune in said layer.  For "
"instance ``{1: [0, 2], 2: [2, 3]}`` will prune heads 0 and 2 on layer 1 "
"and heads 2 and 3 on layer 2."
msgstr ""

#: of transformers.PretrainedConfig:51
msgid ""
"Pruned heads of the model. The keys are the selected layer indices and "
"the associated values, the list of heads to prune in said layer."
msgstr ""

#: of transformers.PretrainedConfig:54
msgid ""
"For instance ``{1: [0, 2], 2: [2, 3]}`` will prune heads 0 and 2 on layer"
" 1 and heads 2 and 3 on layer 2."
msgstr ""

#: of transformers.PretrainedConfig:56
msgid ""
"The chunk size of all feed forward layers in the residual attention "
"blocks. A chunk size of :obj:`0` means that the feed forward layer is not"
" chunked. A chunk size of n means that the feed forward layer processes "
":obj:`n` < sequence_length embeddings at a time. For more information on "
"feed forward chunking, see `How does Feed Forward Chunking work? "
"<../glossary.html#feed-forward-chunking>`__ ."
msgstr ""

#: of transformers.PretrainedConfig:62
msgid "Parameters for sequence generation"
msgstr ""

#: of transformers.PretrainedConfig:64
msgid ""
"**max_length** (:obj:`int`, `optional`, defaults to 20) -- Maximum length"
" that will be used by default in the :obj:`generate` method of the model."
msgstr ""

#: of transformers.PretrainedConfig:66
msgid ""
"**min_length** (:obj:`int`, `optional`, defaults to 10) -- Minimum length"
" that will be used by default in the :obj:`generate` method of the model."
msgstr ""

#: of transformers.PretrainedConfig:68
msgid ""
"**do_sample** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Flag"
" that will be used by default in the :obj:`generate` method of the model."
" Whether or not to use sampling ; use greedy decoding otherwise."
msgstr ""

#: of transformers.PretrainedConfig:70
msgid ""
"**early_stopping** (:obj:`bool`, `optional`, defaults to :obj:`False`) --"
" Flag that will be used by default in the :obj:`generate` method of the "
"model. Whether to stop the beam search when at least ``num_beams`` "
"sentences are finished per batch or not."
msgstr ""

#: of transformers.PretrainedConfig:73
msgid ""
"**num_beams** (:obj:`int`, `optional`, defaults to 1) -- Number of beams "
"for beam search that will be used by default in the :obj:`generate` "
"method of the model. 1 means no beam search."
msgstr ""

#: of transformers.PretrainedConfig:75
msgid ""
"**num_beam_groups** (:obj:`int`, `optional`, defaults to 1) -- Number of "
"groups to divide :obj:`num_beams` into in order to ensure diversity among"
" different groups of beams that will be used by default in the "
":obj:`generate` method of the model. 1 means no group beam search."
msgstr ""

#: of transformers.PretrainedConfig:78
msgid ""
"**diversity_penalty** (:obj:`float`, `optional`, defaults to 0.0) -- "
"Value to control diversity for group beam search. that will be used by "
"default in the :obj:`generate` method of the model. 0 means no diversity "
"penalty. The higher the penalty, the more diverse are the outputs."
msgstr ""

#: of transformers.PretrainedConfig:81
msgid ""
"**temperature** (:obj:`float`, `optional`, defaults to 1) -- The value "
"used to module the next token probabilities that will be used by default "
"in the :obj:`generate` method of the model. Must be strictly positive."
msgstr ""

#: of transformers.PretrainedConfig:84
msgid ""
"**top_k** (:obj:`int`, `optional`, defaults to 50) -- Number of highest "
"probability vocabulary tokens to keep for top-k-filtering that will be "
"used by default in the :obj:`generate` method of the model."
msgstr ""

#: of transformers.PretrainedConfig:86
msgid ""
"**top_p** (:obj:`float`, `optional`, defaults to 1) -- Value that will be"
" used by default in the :obj:`generate` method of the model for "
"``top_p``. If set to float < 1, only the most probable tokens with "
"probabilities that add up to ``top_p`` or higher are kept for generation."
msgstr ""

#: of transformers.PretrainedConfig:89
msgid ""
"**repetition_penalty** (:obj:`float`, `optional`, defaults to 1) -- "
"Parameter for repetition penalty that will be used by default in the "
":obj:`generate` method of the model. 1.0 means no penalty."
msgstr ""

#: of transformers.PretrainedConfig:91
msgid ""
"**length_penalty** (:obj:`float`, `optional`, defaults to 1) -- "
"Exponential penalty to the length that will be used by default in the "
":obj:`generate` method of the model."
msgstr ""

#: of transformers.PretrainedConfig:93
msgid ""
"**no_repeat_ngram_size** (:obj:`int`, `optional`, defaults to 0) -- Value"
" that will be used by default in the :obj:`generate` method of the model "
"for ``no_repeat_ngram_size``. If set to int > 0, all ngrams of that size "
"can only occur once."
msgstr ""

#: of transformers.PretrainedConfig:96
msgid ""
"**encoder_no_repeat_ngram_size** (:obj:`int`, `optional`, defaults to 0) "
"-- Value that will be used by default in the :obj:`generate` method of "
"the model for ``encoder_no_repeat_ngram_size``. If set to int > 0, all "
"ngrams of that size that occur in the ``encoder_input_ids`` cannot occur "
"in the ``decoder_input_ids``."
msgstr ""

#: of transformers.PretrainedConfig:99
msgid ""
"**bad_words_ids** (:obj:`List[int]`, `optional`) -- List of token ids "
"that are not allowed to be generated that will be used by default in the "
":obj:`generate` method of the model. In order to get the tokens of the "
"words that should not appear in the generated text, use "
":obj:`tokenizer.encode(bad_word, add_prefix_space=True)`."
msgstr ""

#: of transformers.PretrainedConfig:103
msgid ""
"**num_return_sequences** (:obj:`int`, `optional`, defaults to 1) -- "
"Number of independently computed returned sequences for each element in "
"the batch that will be used by default in the :obj:`generate` method of "
"the model."
msgstr ""

#: of transformers.PretrainedConfig:106
msgid ""
"**output_scores** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- "
"Whether the model should return the logits when used for generation"
msgstr ""

#: of transformers.PretrainedConfig:108
msgid ""
"**return_dict_in_generate** (:obj:`bool`, `optional`, defaults to "
":obj:`False`) -- Whether the model should return a "
":class:`~transformers.file_utils.ModelOutput` instead of a "
":obj:`torch.LongTensor`"
msgstr ""

#: of transformers.PretrainedConfig:110
msgid ""
"**forced_bos_token_id** (:obj:`int`, `optional`) -- The id of the token "
"to force as the first generated token after the "
":obj:`decoder_start_token_id`. Useful for multilingual models like "
":doc:`mBART <../model_doc/mbart>` where the first generated token needs "
"to be the target language token."
msgstr ""

#: of transformers.PretrainedConfig:113
msgid ""
"**forced_eos_token_id** (:obj:`int`, `optional`) -- The id of the token "
"to force as the last generated token when :obj:`max_length` is reached."
msgstr ""

#: of transformers.PretrainedConfig:115
msgid ""
"**remove_invalid_values** (:obj:`bool`, `optional`) -- Whether to remove "
"possible `nan` and `inf` outputs of the model to prevent the generation "
"method to crash. Note that using ``remove_invalid_values`` can slow down "
"generation."
msgstr ""

#: of transformers.PretrainedConfig:120
msgid "Parameters for fine-tuning tasks"
msgstr ""

#: of transformers.PretrainedConfig:122
msgid ""
"**architectures** (:obj:`List[str]`, `optional`) -- Model architectures "
"that can be used with the model pretrained weights."
msgstr ""

#: of transformers.PretrainedConfig:124
msgid ""
"**finetuning_task** (:obj:`str`, `optional`) -- Name of the task used to "
"fine-tune the model. This can be used when converting from an original "
"(TensorFlow or PyTorch) checkpoint."
msgstr ""

#: of transformers.PretrainedConfig:126
msgid ""
"**id2label** (:obj:`Dict[int, str]`, `optional`) -- A map from index (for"
" instance prediction index, or target index) to label."
msgstr ""

#: of transformers.PretrainedConfig:128
msgid ""
"**label2id** (:obj:`Dict[str, int]`, `optional`) -- A map from label to "
"index for the model."
msgstr ""

#: of transformers.PretrainedConfig:129
msgid ""
"**num_labels** (:obj:`int`, `optional`) -- Number of labels to use in the"
" last layer added to the model, typically for a classification task."
msgstr ""

#: of transformers.PretrainedConfig:131
msgid ""
"**task_specific_params** (:obj:`Dict[str, Any]`, `optional`) -- "
"Additional keyword arguments to store for the current task."
msgstr ""

#: of transformers.PretrainedConfig:133
msgid ""
"**problem_type** (:obj:`str`, `optional`) -- Problem type for "
":obj:`XxxForSequenceClassification` models. Can be one of "
"(:obj:`\"regression\"`, :obj:`\"single_label_classification\"`, "
":obj:`\"multi_label_classification\"`). Please note that this parameter "
"is only available in the following models: "
"`AlbertForSequenceClassification`, `BertForSequenceClassification`, "
"`BigBirdForSequenceClassification`, `ConvBertForSequenceClassification`, "
"`DistilBertForSequenceClassification`, "
"`ElectraForSequenceClassification`, `FunnelForSequenceClassification`, "
"`LongformerForSequenceClassification`, "
"`MobileBertForSequenceClassification`, "
"`ReformerForSequenceClassification`, `RobertaForSequenceClassification`, "
"`SqueezeBertForSequenceClassification`, `XLMForSequenceClassification` "
"and `XLNetForSequenceClassification`."
msgstr ""

#: of transformers.PretrainedConfig:142
msgid "Parameters linked to the tokenizer"
msgstr ""

#: of transformers.PretrainedConfig:144
msgid ""
"**tokenizer_class** (:obj:`str`, `optional`) -- The name of the "
"associated tokenizer class to use (if none is set, will use the tokenizer"
" associated to the model by default)."
msgstr ""

#: of transformers.PretrainedConfig:146
msgid ""
"**prefix** (:obj:`str`, `optional`) -- A specific prompt that should be "
"added at the beginning of each text before calling the model."
msgstr ""

#: of transformers.PretrainedConfig:148
msgid ""
"**bos_token_id** (:obj:`int`, `optional`)) -- The id of the `beginning-"
"of-stream` token."
msgstr ""

#: of transformers.PretrainedConfig:149
msgid ""
"**pad_token_id** (:obj:`int`, `optional`)) -- The id of the `padding` "
"token."
msgstr ""

#: of transformers.PretrainedConfig:150
msgid ""
"**eos_token_id** (:obj:`int`, `optional`)) -- The id of the `end-of-"
"stream` token."
msgstr ""

#: of transformers.PretrainedConfig:151
msgid ""
"**decoder_start_token_id** (:obj:`int`, `optional`)) -- If an encoder-"
"decoder model starts decoding with a different token than `bos`, the id "
"of that token."
msgstr ""

#: of transformers.PretrainedConfig:153
msgid ""
"**sep_token_id** (:obj:`int`, `optional`)) -- The id of the `separation` "
"token."
msgstr ""

#: of transformers.PretrainedConfig:155
msgid "PyTorch specific parameters"
msgstr ""

#: of transformers.PretrainedConfig:157
msgid ""
"**torchscript** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- "
"Whether or not the model should be used with Torchscript."
msgstr ""

#: of transformers.PretrainedConfig:159
msgid ""
"**tie_word_embeddings** (:obj:`bool`, `optional`, defaults to "
":obj:`True`) -- Whether the model's input and output word embeddings "
"should be tied. Note that this is only relevant if the model has a output"
" word embedding layer."
msgstr ""

#: of transformers.PretrainedConfig:162
msgid ""
"**torch_dtype** (:obj:`str`, `optional`) -- The :obj:`dtype` of the "
"weights. This attribute can be used to initialize the model to a non-"
"default ``dtype`` (which is normally ``float32``) and thus allow for "
"optimal storage allocation. For example, if the saved model is "
"``float16``, ideally we want to load it back using the minimal amount of "
"memory needed to load ``float16`` weights. Since the config object is "
"stored in plain text, this attribute contains just the floating type "
"string without the ``torch.`` prefix. For example, for ``torch.float16`` "
"``torch_dtype`` is the ``\"float16\"`` string."
msgstr ""

#: of transformers.PretrainedConfig:169
msgid "TensorFlow specific parameters"
msgstr ""

#: of transformers.PretrainedConfig:171
msgid ""
"**use_bfloat16** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- "
"Whether or not the model should use BFloat16 scalars (only used by some "
"TensorFlow models)."
msgstr ""

#: of transformers.PretrainedConfig.from_dict:1
msgid ""
"Instantiates a :class:`~transformers.PretrainedConfig` from a Python "
"dictionary of parameters."
msgstr ""

#: of transformers.PretrainedConfig.from_dict:3
msgid ""
"Dictionary that will be used to instantiate the configuration object. "
"Such a dictionary can be retrieved from a pretrained checkpoint by "
"leveraging the :func:`~transformers.PretrainedConfig.get_config_dict` "
"method."
msgstr ""

#: of transformers.PretrainedConfig.from_dict:7
msgid "Additional parameters from which to initialize the configuration object."
msgstr ""

#: of transformers.PretrainedConfig.from_dict
#: transformers.PretrainedConfig.from_json_file
#: transformers.PretrainedConfig.from_pretrained
#: transformers.PretrainedConfig.get_config_dict
#: transformers.PretrainedConfig.push_to_hub
#: transformers.PretrainedConfig.to_dict
#: transformers.PretrainedConfig.to_diff_dict
#: transformers.PretrainedConfig.to_json_string
msgid "Returns"
msgstr ""

#: of transformers.PretrainedConfig.from_dict:10
msgid "The configuration object instantiated from those parameters."
msgstr ""

#: of transformers.PretrainedConfig.from_dict
#: transformers.PretrainedConfig.from_json_file
#: transformers.PretrainedConfig.from_pretrained
#: transformers.PretrainedConfig.get_config_dict
#: transformers.PretrainedConfig.push_to_hub
#: transformers.PretrainedConfig.to_dict
#: transformers.PretrainedConfig.to_diff_dict
#: transformers.PretrainedConfig.to_json_string
msgid "Return type"
msgstr ""

#: of transformers.PretrainedConfig.from_dict:11
#: transformers.PretrainedConfig.from_json_file:7
#: transformers.PretrainedConfig.from_pretrained:50
msgid ":class:`PretrainedConfig`"
msgstr ""

#: of transformers.PretrainedConfig.from_json_file:1
msgid ""
"Instantiates a :class:`~transformers.PretrainedConfig` from the path to a"
" JSON file of parameters."
msgstr ""

#: of transformers.PretrainedConfig.from_json_file:3
msgid "Path to the JSON file containing the parameters."
msgstr ""

#: of transformers.PretrainedConfig.from_json_file:6
msgid "The configuration object instantiated from that JSON file."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:1
msgid ""
"Instantiate a :class:`~transformers.PretrainedConfig` (or a derived "
"class) from a pretrained model configuration."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:4
msgid ""
"This can be either:  - a string, the `model id` of a pretrained model "
"configuration hosted inside a model repo on   huggingface.co. Valid model"
" ids can be located at the root-level, like ``bert-base-uncased``, or   "
"namespaced under a user or organization name, like ``dbmdz/bert-base-"
"german-cased``. - a path to a `directory` containing a configuration file"
" saved using the   :func:`~transformers.PretrainedConfig.save_pretrained`"
" method, e.g., ``./my_model_directory/``. - a path or url to a saved "
"configuration JSON `file`, e.g.,   "
"``./my_model_directory/configuration.json``."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:4
msgid "This can be either:"
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:6
msgid ""
"a string, the `model id` of a pretrained model configuration hosted "
"inside a model repo on huggingface.co. Valid model ids can be located at "
"the root-level, like ``bert-base-uncased``, or namespaced under a user or"
" organization name, like ``dbmdz/bert-base-german-cased``."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:9
msgid ""
"a path to a `directory` containing a configuration file saved using the "
":func:`~transformers.PretrainedConfig.save_pretrained` method, e.g., "
"``./my_model_directory/``."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:11
msgid ""
"a path or url to a saved configuration JSON `file`, e.g., "
"``./my_model_directory/configuration.json``."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:14
msgid ""
"Path to a directory in which a downloaded pretrained model configuration "
"should be cached if the standard cache should not be used."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:17
msgid ""
"Whether or not to force to (re-)download the configuration files and "
"override the cached versions if they exist."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:20
msgid ""
"Whether or not to delete incompletely received file. Attempts to resume "
"the download if such a file exists."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:23
msgid ""
"A dictionary of proxy servers to use by protocol or endpoint, e.g., "
":obj:`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.` The "
"proxies are used on each request."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:26
msgid ""
"The token to use as HTTP bearer authorization for remote files. If "
":obj:`True`, will use the token generated when running :obj"
":`transformers-cli login` (stored in :obj:`~/.huggingface`)."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:29
msgid ""
"The specific model version to use. It can be a branch name, a tag name, "
"or a commit id, since we use a git-based system for storing models and "
"other artifacts on huggingface.co, so ``revision`` can be any identifier "
"allowed by git."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:33
msgid ""
"If :obj:`False`, then this function returns just the final configuration "
"object.  If :obj:`True`, then this functions returns a "
":obj:`Tuple(config, unused_kwargs)` where `unused_kwargs` is a dictionary"
" consisting of the key/value pairs whose keys are not configuration "
"attributes: i.e., the part of ``kwargs`` which has not been used to "
"update ``config`` and is otherwise ignored."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:33
msgid ""
"If :obj:`False`, then this function returns just the final configuration "
"object."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:35
msgid ""
"If :obj:`True`, then this functions returns a :obj:`Tuple(config, "
"unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the "
"key/value pairs whose keys are not configuration attributes: i.e., the "
"part of ``kwargs`` which has not been used to update ``config`` and is "
"otherwise ignored."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:39
msgid ""
"The values in kwargs of any keys which are configuration attributes will "
"be used to override the loaded values. Behavior concerning key/value "
"pairs whose keys are *not* configuration attributes is controlled by the "
"``return_unused_kwargs`` keyword parameter."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:46
msgid ""
"Passing :obj:`use_auth_token=True` is required when you want to use a "
"private model."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:49
msgid "The configuration object instantiated from this pretrained model."
msgstr ""

#: of transformers.PretrainedConfig.from_pretrained:52
#: transformers.PretrainedConfig.push_to_hub:30
msgid "Examples::"
msgstr ""

#: of transformers.PretrainedConfig.get_config_dict:1
msgid ""
"From a ``pretrained_model_name_or_path``, resolve to a dictionary of "
"parameters, to be used for instantiating a "
":class:`~transformers.PretrainedConfig` using ``from_dict``."
msgstr ""

#: of transformers.PretrainedConfig.get_config_dict:6
msgid ""
"The identifier of the pre-trained checkpoint from which we want the "
"dictionary of parameters."
msgstr ""

#: of transformers.PretrainedConfig.get_config_dict:9
msgid ""
"The dictionary(ies) that will be used to instantiate the configuration "
"object."
msgstr ""

#: of transformers.PretrainedConfig.get_config_dict:10
msgid ":obj:`Tuple[Dict, Dict]`"
msgstr ""

#: of transformers.PretrainedConfig.num_labels:1
msgid "The number of labels for classification models."
msgstr ""

#: of transformers.PretrainedConfig.num_labels
#: transformers.PretrainedConfig.use_return_dict
msgid "type"
msgstr ""

#: of transformers.PretrainedConfig.num_labels:3
msgid ":obj:`int`"
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:1
msgid ""
"Upload the configuration file to the 🤗 Model Hub while synchronizing a "
"local clone of the repo in :obj:`repo_path_or_name`."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:4
msgid ""
"Can either be a repository name for your config in the Hub or a path to a"
" local folder (in which case the repository will have the name of that "
"local folder). If not specified, will default to the name given by "
":obj:`repo_url` and a local directory with that name will be created."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:8
msgid ""
"Specify this in case you want to push to an existing repository in the "
"hub. If unspecified, a new repository will be created in your namespace "
"(unless you specify an :obj:`organization`) with :obj:`repo_name`."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:12
msgid ""
"Whether or not to clone the distant repo in a temporary directory or in "
":obj:`repo_path_or_name` inside the current working directory. This will "
"slow things down if you are making changes in an existing repo since you "
"will need to clone the repo before every push."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:16
msgid "Message to commit while pushing. Will default to :obj:`\"add config\"`."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:18
msgid ""
"Organization in which you want to push your config (you must be a member "
"of this organization)."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:20
msgid ""
"Whether or not the repository created should be private (requires a "
"paying subscription)."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:22
msgid ""
"The token to use as HTTP bearer authorization for remote files. If "
":obj:`True`, will use the token generated when running :obj"
":`transformers-cli login` (stored in :obj:`~/.huggingface`). Will default"
" to :obj:`True` if :obj:`repo_url` is not specified."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:27
msgid "The url of the commit of your config in the given repository."
msgstr ""

#: of transformers.PretrainedConfig.push_to_hub:28
#: transformers.PretrainedConfig.to_json_string:8
msgid ":obj:`str`"
msgstr ""

#: of transformers.PretrainedConfig.save_pretrained:1
msgid ""
"Save a configuration object to the directory ``save_directory``, so that "
"it can be re-loaded using the "
":func:`~transformers.PretrainedConfig.from_pretrained` class method."
msgstr ""

#: of transformers.PretrainedConfig.save_pretrained:4
msgid ""
"Directory where the configuration JSON file will be saved (will be "
"created if it does not exist)."
msgstr ""

#: of transformers.PretrainedConfig.save_pretrained:6
msgid ""
"Whether or not to push your model to the Hugging Face model hub after "
"saving it.  .. warning::      Using :obj:`push_to_hub=True` will "
"synchronize the repository you are pushing to with     "
":obj:`save_directory`, which requires :obj:`save_directory` to be a local"
" clone of the repo you are     pushing to if it's an existing folder. "
"Pass along :obj:`temp_dir=True` to use a temporary directory     instead."
msgstr ""

#: of transformers.PretrainedConfig.save_pretrained:6
msgid ""
"Whether or not to push your model to the Hugging Face model hub after "
"saving it."
msgstr ""

#: of transformers.PretrainedConfig.save_pretrained:10
msgid ""
"Using :obj:`push_to_hub=True` will synchronize the repository you are "
"pushing to with :obj:`save_directory`, which requires "
":obj:`save_directory` to be a local clone of the repo you are pushing to "
"if it's an existing folder. Pass along :obj:`temp_dir=True` to use a "
"temporary directory instead."
msgstr ""

#: of transformers.PretrainedConfig.save_pretrained:15
msgid ""
"Additional key word arguments passed along to the "
":meth:`~transformers.file_utils.PushToHubMixin.push_to_hub` method."
msgstr ""

#: of transformers.PretrainedConfig.to_dict:1
msgid "Serializes this instance to a Python dictionary."
msgstr ""

#: of transformers.PretrainedConfig.to_dict:3
msgid "Dictionary of all the attributes that make up this configuration instance."
msgstr ""

#: of transformers.PretrainedConfig.to_dict:4
#: transformers.PretrainedConfig.to_diff_dict:5
msgid ":obj:`Dict[str, Any]`"
msgstr ""

#: of transformers.PretrainedConfig.to_diff_dict:1
msgid ""
"Removes all attributes from config which correspond to the default config"
" attributes for better readability and serializes to a Python dictionary."
msgstr ""

#: of transformers.PretrainedConfig.to_diff_dict:4
msgid "Dictionary of all the attributes that make up this configuration instance,"
msgstr ""

#: of transformers.PretrainedConfig.to_json_file:1
msgid "Save this instance to a JSON file."
msgstr ""

#: of transformers.PretrainedConfig.to_json_file:3
msgid ""
"Path to the JSON file in which this configuration instance's parameters "
"will be saved."
msgstr ""

#: of transformers.PretrainedConfig.to_json_file:5
msgid ""
"If set to ``True``, only the difference between the config instance and "
"the default ``PretrainedConfig()`` is serialized to JSON file."
msgstr ""

#: of transformers.PretrainedConfig.to_json_string:1
msgid "Serializes this instance to a JSON string."
msgstr ""

#: of transformers.PretrainedConfig.to_json_string:3
msgid ""
"If set to ``True``, only the difference between the config instance and "
"the default ``PretrainedConfig()`` is serialized to JSON string."
msgstr ""

#: of transformers.PretrainedConfig.to_json_string:7
msgid ""
"String containing all the attributes that make up this configuration "
"instance in JSON format."
msgstr ""

#: of transformers.PretrainedConfig.update:1
msgid "Updates attributes of this class with attributes from ``config_dict``."
msgstr ""

#: of transformers.PretrainedConfig.update:3
msgid "Dictionary of attributes that should be updated for this class."
msgstr ""

#: of transformers.PretrainedConfig.update_from_string:1
msgid "Updates attributes of this class with attributes from ``update_str``."
msgstr ""

#: of transformers.PretrainedConfig.update_from_string:3
msgid ""
"The expected format is ints, floats and strings as is, and for booleans "
"use ``true`` or ``false``. For example: "
"\"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\""
msgstr ""

#: of transformers.PretrainedConfig.update_from_string:6
msgid "The keys to change have to already exist in the config object."
msgstr ""

#: of transformers.PretrainedConfig.update_from_string:8
msgid "String with attributes that should be updated for this class."
msgstr ""

#: of transformers.PretrainedConfig.use_return_dict:1
msgid ""
"Whether or not return :class:`~transformers.file_utils.ModelOutput` "
"instead of tuples."
msgstr ""

#: of transformers.PretrainedConfig.use_return_dict:3
msgid ":obj:`bool`"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/custom_datasets.rst:14
msgid "Fine-tuning with custom datasets"
msgstr ""

#: ../../source/custom_datasets.rst:18
msgid ""
"The datasets used in this tutorial are available and can be more easily "
"accessed using the `ðŸ¤— Datasets library "
"<https://github.com/huggingface/datasets>`_. We do not use this library "
"to access the datasets here since this tutorial meant to illustrate how "
"to work with your own data. A brief of introduction can be found at the "
"end of the tutorial in the section \":ref:`datasetslib`\"."
msgstr ""

#: ../../source/custom_datasets.rst:23
msgid ""
"This tutorial will take you through several examples of using ðŸ¤— "
"Transformers models with your own datasets. The guide shows one of many "
"valid workflows for using these models and is meant to be illustrative "
"rather than definitive. We show examples of reading in several data "
"formats, preprocessing the data for several types of tasks, and then "
"preparing the data into PyTorch/TensorFlow ``Dataset`` objects which can "
"easily be used either with "
":class:`~transformers.Trainer`/:class:`~transformers.TFTrainer` or with "
"native PyTorch/TensorFlow."
msgstr ""

#: ../../source/custom_datasets.rst:29
msgid ""
"We include several examples, each of which demonstrates a different type "
"of common downstream task:"
msgstr ""

#: ../../source/custom_datasets.rst:31
msgid ":ref:`seq_imdb`"
msgstr ""

#: ../../source/custom_datasets.rst:32
msgid ":ref:`tok_ner`"
msgstr ""

#: ../../source/custom_datasets.rst:33
msgid ":ref:`qa_squad`"
msgstr ""

#: ../../source/custom_datasets.rst:34
msgid ":ref:`resources`"
msgstr ""

#: ../../source/custom_datasets.rst:39
msgid "Sequence Classification with IMDb Reviews"
msgstr ""

#: ../../source/custom_datasets.rst:43
msgid ""
"This dataset can be explored in the Hugging Face model hub (`IMDb "
"<https://huggingface.co/datasets/imdb>`_), and can be alternatively "
"downloaded with the ðŸ¤— Datasets library with ``load_dataset(\"imdb\")``."
msgstr ""

#: ../../source/custom_datasets.rst:46
msgid ""
"In this example, we'll show how to download, tokenize, and train a model "
"on the IMDb reviews dataset. This task takes the text of a review and "
"requires the model to predict whether the sentiment of the review is "
"positive or negative. Let's start by downloading the dataset from the "
"`Large Movie Review Dataset "
"<http://ai.stanford.edu/~amaas/data/sentiment/>`_ webpage."
msgstr ""

#: ../../source/custom_datasets.rst:56
msgid ""
"This data is organized into ``pos`` and ``neg`` folders with one text "
"file per example. Let's write a function that can read this in."
msgstr ""

#: ../../source/custom_datasets.rst:77
msgid ""
"We now have a train and test dataset, but let's also also create a "
"validation set which we can use for for evaluation and tuning without "
"tainting our test set results. Sklearn has a convenient utility for "
"creating such splits:"
msgstr ""

#: ../../source/custom_datasets.rst:85
msgid ""
"Alright, we've read in our dataset. Now let's tackle tokenization. We'll "
"eventually train a classifier using pre-trained DistilBert, so let's use "
"the DistilBert tokenizer."
msgstr ""

#: ../../source/custom_datasets.rst:93
msgid ""
"Now we can simply pass our texts to the tokenizer. We'll pass "
"``truncation=True`` and ``padding=True``, which will ensure that all of "
"our sequences are padded to the same length and are truncated to be no "
"longer model's maximum input length. This will allow us to feed batches "
"of sequences into the model at the same time."
msgstr ""

#: ../../source/custom_datasets.rst:103
msgid ""
"Now, let's turn our labels and encodings into a Dataset object. In "
"PyTorch, this is done by subclassing a ``torch.utils.data.Dataset`` "
"object and implementing ``__len__`` and ``__getitem__``. In TensorFlow, "
"we pass our input encodings and labels to the ``from_tensor_slices`` "
"constructor method. We put the data in this format so that the data can "
"be easily batched such that each key in the batch encoding corresponds to"
" a named parameter of the "
":meth:`~transformers.DistilBertForSequenceClassification.forward` method "
"of the model we will train."
msgstr ""

#: ../../source/custom_datasets.rst:146
msgid ""
"Now that our datasets our ready, we can fine-tune a model either with the"
" ðŸ¤— :class:`~transformers.Trainer`/:class:`~transformers.TFTrainer` or "
"with native PyTorch/TensorFlow. See :doc:`training <training>`."
msgstr ""

#: ../../source/custom_datasets.rst:153
msgid "Fine-tuning with Trainer"
msgstr ""

#: ../../source/custom_datasets.rst:155
msgid ""
"The steps above prepared the datasets in the way that the trainer is "
"expected. Now all we need to do is create a model to fine-tune, define "
"the "
":class:`~transformers.TrainingArguments`/:class:`~transformers.TFTrainingArguments`"
" and instantiate a "
":class:`~transformers.Trainer`/:class:`~transformers.TFTrainer`."
msgstr ""

#: ../../source/custom_datasets.rst:214
msgid "Fine-tuning with native PyTorch/TensorFlow"
msgstr ""

#: ../../source/custom_datasets.rst:216
msgid "We can also train use native PyTorch or TensorFlow:"
msgstr ""

#: ../../source/custom_datasets.rst:258
msgid "Token Classification with W-NUT Emerging Entities"
msgstr ""

#: ../../source/custom_datasets.rst:262
msgid ""
"This dataset can be explored in the Hugging Face model hub (`WNUT-17 "
"<https://huggingface.co/datasets/wnut_17>`_), and can be alternatively "
"downloaded with the ðŸ¤— Datasets library with "
"``load_dataset(\"wnut_17\")``."
msgstr ""

#: ../../source/custom_datasets.rst:265
msgid ""
"Next we will look at token classification. Rather than classifying an "
"entire sequence, this task classifies token by token. We'll demonstrate "
"how to do this with `Named Entity Recognition "
"<http://nlpprogress.com/english/named_entity_recognition.html>`_, which "
"involves identifying tokens which correspond to a predefined set of "
"\"entities\". Specifically, we'll use the `W-NUT Emerging and Rare "
"entities <http://noisy-text.github.io/2017/emerging-rare-entities.html>`_"
" corpus. The data is given as a collection of pre-tokenized documents "
"where each token is assigned a tag."
msgstr ""

#: ../../source/custom_datasets.rst:272
msgid "Let's start by downloading the data."
msgstr ""

#: ../../source/custom_datasets.rst:278
msgid ""
"In this case, we'll just download the train set, which is a single text "
"file. Each line of the file contains either (1) a word and tag separated "
"by a tab, or (2) a blank line indicating the end of a document. Let's "
"write a function to read this in. We'll take in the file path and return "
"``token_docs`` which is a list of lists of token strings, and "
"``token_tags`` which is a list of lists of tag strings."
msgstr ""

#: ../../source/custom_datasets.rst:309
msgid ""
"Just to see what this data looks like, let's take a look at a segment of "
"the first document."
msgstr ""

#: ../../source/custom_datasets.rst:317
msgid ""
"``location`` is an entity type, ``B-`` indicates the beginning of an "
"entity, and ``I-`` indicates consecutive positions of the same entity "
"(\"Empire State Building\" is considered one entity). ``O`` indicates the"
" token does not correspond to any entity."
msgstr ""

#: ../../source/custom_datasets.rst:321
msgid "Now that we've read the data in, let's create a train/validation split:"
msgstr ""

#: ../../source/custom_datasets.rst:328
msgid ""
"Next, let's create encodings for our tokens and tags. For the tags, we "
"can start by just create a simple mapping which we'll use in a moment:"
msgstr ""

#: ../../source/custom_datasets.rst:337
msgid ""
"To encode the tokens, we'll use a pre-trained DistilBert tokenizer. We "
"can tell the tokenizer that we're dealing with ready-split tokens rather "
"than full sentence strings by passing ``is_split_into_words=True``. We'll"
" also pass ``padding=True`` and ``truncation=True`` to pad the sequences "
"to be the same length. Lastly, we can tell the model to return "
"information about the tokens which are split by the wordpiece "
"tokenization process, which we will need in a moment."
msgstr ""

#: ../../source/custom_datasets.rst:350
msgid ""
"Great, so now our tokens are nicely encoded in the format that they need "
"to be in to feed them into our DistilBert model below."
msgstr ""

#: ../../source/custom_datasets.rst:353
msgid ""
"Now we arrive at a common obstacle with using pre-trained models for "
"token-level classification: many of the tokens in the W-NUT corpus are "
"not in DistilBert's vocabulary. Bert and many models like it use a method"
" called WordPiece Tokenization, meaning that single words are split into "
"multiple tokens such that each token is likely to be in the vocabulary. "
"For example, DistilBert's tokenizer would split the Twitter handle "
"``@huggingface`` into the tokens ``['@', 'hugging', '##face']``. This is "
"a problem for us because we have exactly one tag per token. If the "
"tokenizer splits a token into multiple sub-tokens, then we will end up "
"with a mismatch between our tokens and our labels."
msgstr ""

#: ../../source/custom_datasets.rst:360
msgid ""
"One way to handle this is to only train on the tag labels for the first "
"subtoken of a split token. We can do this in ðŸ¤— Transformers by setting "
"the labels we wish to ignore to ``-100``. In the example above, if the "
"label for ``@HuggingFace`` is ``3`` (indexing ``B-corporation``), we "
"would set the labels of ``['@', 'hugging', '##face']`` to ``[3, -100, "
"-100]``."
msgstr ""

#: ../../source/custom_datasets.rst:365
msgid ""
"Let's write a function to do this. This is where we will use the "
"``offset_mapping`` from the tokenizer as mentioned above. For each sub-"
"token returned by the tokenizer, the offset mapping gives us a tuple "
"indicating the sub-token's start position and end position relative to "
"the original token it was split from. That means that if the first "
"position in the tuple is anything other than ``0``, we will set its "
"corresponding label to ``-100``. While we're at it, we can also set "
"labels to ``-100`` if the second position of the offset mapping is ``0``,"
" since this means it must be a special token like ``[PAD]`` or ``[CLS]``."
msgstr ""

#: ../../source/custom_datasets.rst:374
msgid ""
"Due to a recently fixed bug, -1 must be used instead of -100 when using "
"TensorFlow in ðŸ¤— Transformers <= 3.02."
msgstr ""

#: ../../source/custom_datasets.rst:397
msgid ""
"The hard part is now done. Just as in the sequence classification example"
" above, we can create a dataset object:"
msgstr ""

#: ../../source/custom_datasets.rst:436
msgid "Now load in a token classification model and specify the number of labels:"
msgstr ""

#: ../../source/custom_datasets.rst:447
msgid ""
"The data and model are both ready to go. You can train the model either "
"with :class:`~transformers.Trainer`/:class:`~transformers.TFTrainer` or "
"with native PyTorch/TensorFlow, exactly as in the sequence classification"
" example above."
msgstr ""

#: ../../source/custom_datasets.rst:451 ../../source/custom_datasets.rst:625
msgid ":ref:`ft_trainer`"
msgstr ""

#: ../../source/custom_datasets.rst:452
msgid ":ref:`ft_native`"
msgstr ""

#: ../../source/custom_datasets.rst:457
msgid "Question Answering with SQuAD 2.0"
msgstr ""

#: ../../source/custom_datasets.rst:461
msgid ""
"This dataset can be explored in the Hugging Face model hub (`SQuAD V2 "
"<https://huggingface.co/datasets/squad_v2>`_), and can be alternatively "
"downloaded with the ðŸ¤— Datasets library with "
"``load_dataset(\"squad_v2\")``."
msgstr ""

#: ../../source/custom_datasets.rst:465
msgid ""
"Question answering comes in many forms. In this example, we'll look at "
"the particular type of extractive QA that involves answering a question "
"about a passage by highlighting the segment of the passage that answers "
"the question. This involves fine-tuning a model which predicts a start "
"position and an end position in the passage. We will use the `Stanford "
"Question Answering Dataset (SQuAD) 2.0 <https://rajpurkar.github.io"
"/SQuAD-explorer/>`_."
msgstr ""

#: ../../source/custom_datasets.rst:470
msgid "We will start by downloading the data:"
msgstr ""

#: ../../source/custom_datasets.rst:478
msgid ""
"Each split is in a structured json file with a number of questions and "
"answers for each passage (or context). We'll take this apart into "
"parallel lists of contexts, questions, and answers (note that the "
"contexts here are repeated since there are multiple questions per "
"context):"
msgstr ""

#: ../../source/custom_datasets.rst:510
msgid ""
"The contexts and questions are just strings. The answers are dicts "
"containing the subsequence of the passage with the correct answer as well"
" as an integer indicating the character at which the answer begins. In "
"order to train a model on this data we need (1) the tokenized "
"context/question pairs, and (2) integers indicating at which *token* "
"positions the answer begins and ends."
msgstr ""

#: ../../source/custom_datasets.rst:515
msgid ""
"First, let's get the *character* position at which the answer ends in the"
" passage (we are given the starting position). Sometimes SQuAD answers "
"are off by one or two characters, so we will also adjust for that."
msgstr ""

#: ../../source/custom_datasets.rst:539
msgid ""
"Now ``train_answers`` and ``val_answers`` include the character end "
"positions and the corrected start positions. Next, let's tokenize our "
"context/question pairs. ðŸ¤— Tokenizers can accept parallel lists of "
"sequences and encode them together as sequence pairs."
msgstr ""

#: ../../source/custom_datasets.rst:551
msgid ""
"Next we need to convert our character start/end positions to token "
"start/end positions. When using ðŸ¤— Fast Tokenizers, we can use the built "
"in :func:`~transformers.BatchEncoding.char_to_token` method."
msgstr ""

#: ../../source/custom_datasets.rst:574
msgid ""
"Our data is ready. Let's just put it in a PyTorch/TensorFlow dataset so "
"that we can easily use it for training. In PyTorch, we define a custom "
"``Dataset`` class. In TensorFlow, we pass a tuple of ``(inputs_dict, "
"labels_dict)`` to the ``from_tensor_slices`` method."
msgstr ""

#: ../../source/custom_datasets.rst:607
msgid "Now we can use a DistilBert model with a QA head for training:"
msgstr ""

#: ../../source/custom_datasets.rst:619
msgid ""
"The data and model are both ready to go. You can train the model with "
":class:`~transformers.Trainer`/:class:`~transformers.TFTrainer` exactly "
"as in the sequence classification example above. If using native PyTorch,"
" replace ``labels`` with ``start_positions`` and ``end_positions`` in the"
" training example. If using Keras's ``fit``, we need to make a minor "
"modification to handle this example since it involves multiple model "
"outputs."
msgstr ""

#: ../../source/custom_datasets.rst:672
msgid "Additional Resources"
msgstr ""

#: ../../source/custom_datasets.rst:674
msgid ""
"`How to train a new language model from scratch using Transformers and "
"Tokenizers <https://huggingface.co/blog/how-to-train>`_. Blog post "
"showing the steps to load in Esperanto data and train a masked language "
"model from scratch."
msgstr ""

#: ../../source/custom_datasets.rst:677
msgid ":doc:`Preprocessing <preprocessing>`. Docs page on data preprocessing."
msgstr ""

#: ../../source/custom_datasets.rst:678
msgid ":doc:`Training <training>`. Docs page on training and fine-tuning."
msgstr ""

#: ../../source/custom_datasets.rst:683
msgid "Using the ðŸ¤— Datasets & Metrics library"
msgstr ""

#: ../../source/custom_datasets.rst:685
msgid ""
"This tutorial demonstrates how to read in datasets from various raw text "
"formats and prepare them for training with ðŸ¤— Transformers so that you can"
" do the same thing with your own custom datasets. However, we recommend "
"users use the `ðŸ¤— Datasets library "
"<https://github.com/huggingface/datasets>`_ for working with the 150+ "
"datasets included in the `hub <https://huggingface.co/datasets>`_, "
"including the three datasets used in this tutorial. As a very brief "
"overview, we will show how to use the Datasets library to download and "
"prepare the IMDb dataset from the first example, :ref:`seq_imdb`."
msgstr ""

#: ../../source/custom_datasets.rst:692
msgid "Start by downloading the dataset:"
msgstr ""

#: ../../source/custom_datasets.rst:699
msgid ""
"Each dataset has multiple columns corresponding to different features. "
"Let's see what our columns are."
msgstr ""

#: ../../source/custom_datasets.rst:706
msgid ""
"Great. Now let's tokenize the text. We can do this using the ``map`` "
"method. We'll also rename the ``label`` column to ``labels`` to match the"
" model's input arguments."
msgstr ""

#: ../../source/custom_datasets.rst:714
msgid ""
"Lastly, we can use the ``set_format`` method to determine which columns "
"and in what data format we want to access dataset elements."
msgstr ""

#: ../../source/custom_datasets.rst:728
msgid ""
"We now have a fully-prepared dataset. Check out `the ðŸ¤— Datasets docs "
"<https://huggingface.co/docs/datasets/processing.html>`_ for a more "
"thorough introduction."
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/bertology.rst:14
msgid "BERTology"
msgstr ""

#: ../../source/bertology.rst:16
msgid ""
"There is a growing field of study concerned with investigating the inner "
"working of large-scale transformers like BERT (that some call "
"\"BERTology\"). Some good examples of this field are:"
msgstr ""

#: ../../source/bertology.rst:20
msgid ""
"BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, "
"Ellie Pavlick: https://arxiv.org/abs/1905.05950"
msgstr ""

#: ../../source/bertology.rst:22
msgid ""
"Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, "
"Graham Neubig: https://arxiv.org/abs/1905.10650"
msgstr ""

#: ../../source/bertology.rst:23
msgid ""
"What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, "
"Urvashi Khandelwal, Omer Levy, Christopher D. Manning: "
"https://arxiv.org/abs/1906.04341"
msgstr ""

#: ../../source/bertology.rst:26
msgid ""
"In order to help this new field develop, we have included a few "
"additional features in the BERT/GPT/GPT-2 models to help people access "
"the inner representations, mainly adapted from the great work of Paul "
"Michel (https://arxiv.org/abs/1905.10650):"
msgstr ""

#: ../../source/bertology.rst:31
msgid "accessing all the hidden-states of BERT/GPT/GPT-2,"
msgstr ""

#: ../../source/bertology.rst:32
msgid "accessing all the attention weights for each head of BERT/GPT/GPT-2,"
msgstr ""

#: ../../source/bertology.rst:33
msgid ""
"retrieving heads output values and gradients to be able to compute head "
"importance score and prune head as explained in "
"https://arxiv.org/abs/1905.10650."
msgstr ""

#: ../../source/bertology.rst:36
msgid ""
"To help you understand and use these features, we have added a specific "
"example script: :prefix_link:`bertology.py "
"<examples/research_projects/bertology/run_bertology.py>` while extract "
"information and prune a model pre-trained on GLUE."
msgstr ""


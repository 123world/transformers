# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/debugging.rst:16
msgid "Debugging"
msgstr ""

#: ../../source/debugging.rst:19
msgid "Underflow and Overflow Detection"
msgstr ""

#: ../../source/debugging.rst:23
msgid "This feature is currently available for PyTorch-only."
msgstr ""

#: ../../source/debugging.rst:27
msgid "For multi-GPU training it requires DDP (``torch.distributed.launch``)."
msgstr ""

#: ../../source/debugging.rst:31
msgid "This feature can be used with any ``nn.Module``-based model."
msgstr ""

#: ../../source/debugging.rst:33
msgid ""
"If you start getting ``loss=NaN`` or the model inhibits some other "
"abnormal behavior due to ``inf`` or ``nan`` in activations or weights one"
" needs to discover where the first underflow or overflow happens and what"
" led to it. Luckily you can accomplish that easily by activating a "
"special module that will do the detection automatically."
msgstr ""

#: ../../source/debugging.rst:37
msgid "If you're using :class:`~transformers.Trainer`, you just need to add:"
msgstr ""

#: ../../source/debugging.rst:43
msgid ""
"to the normal command line arguments, or pass "
"``debug=\"underflow_overflow\"`` when creating the "
":class:`~transformers.TrainingArguments` object."
msgstr ""

#: ../../source/debugging.rst:46
msgid ""
"If you're using your own training loop or another Trainer you can "
"accomplish the same with:"
msgstr ""

#: ../../source/debugging.rst:53
msgid ""
":class:`~transformers.debug_utils.DebugUnderflowOverflow` inserts hooks "
"into the model that immediately after each forward call will test input "
"and output variables and also the corresponding module's weights. As soon"
" as ``inf`` or ``nan`` is detected in at least one element of the "
"activations or weights, the program will assert and print a report like "
"this (this was caught with ``google/mt5-small`` under fp16 mixed "
"precision):"
msgstr ""

#: ../../source/debugging.rst:98
msgid "The example output has been trimmed in the middle for brevity."
msgstr ""

#: ../../source/debugging.rst:100
msgid ""
"The second column shows the value of the absolute largest element, so if "
"you have a closer look at the last few frames, the inputs and outputs "
"were in the range of ``1e4``. So when this training was done under fp16 "
"mixed precision the very last step overflowed (since under ``fp16`` the "
"largest number before ``inf`` is ``64e3``). To avoid overflows under "
"``fp16`` the activations must remain way below ``1e4``, because ``1e4 * "
"1e4 = 1e8`` so any matrix multiplication with large activations is going "
"to lead to a numerical overflow condition."
msgstr ""

#: ../../source/debugging.rst:106
msgid ""
"At the very start of the trace you can discover at which batch number the"
" problem occurred (here ``Detected inf/nan during batch_number=0`` means "
"the problem occurred on the first batch)."
msgstr ""

#: ../../source/debugging.rst:109
msgid ""
"Each reported frame starts by declaring the fully qualified entry for the"
" corresponding module this frame is reporting for. If we look just at "
"this frame:"
msgstr ""

#: ../../source/debugging.rst:119
msgid ""
"Here, ``encoder.block.2.layer.1.layer_norm`` indicates that it was a "
"layer norm for the first layer, of the second block of the encoder. And "
"the specific calls of the ``forward`` is ``T5LayerNorm``."
msgstr ""

#: ../../source/debugging.rst:122
msgid "Let's look at the last few frames of that report:"
msgstr ""

#: ../../source/debugging.rst:149
msgid ""
"The last frame reports for ``Dropout.forward`` function with the first "
"entry for the only input and the second for the only output. You can see "
"that it was called from an attribute ``dropout`` inside "
"``DenseReluDense`` class. We can see that it happened during the first "
"layer, of the 2nd block, during the very first batch. Finally, the "
"absolute largest input elements was ``6.27e+04`` and same for the output "
"was ``inf``."
msgstr ""

#: ../../source/debugging.rst:154
msgid ""
"You can see here, that ``T5DenseGatedGeluDense.forward`` resulted in "
"output activations, whose absolute max value was around 62.7K, which is "
"very close to fp16's top limit of 64K. In the next frame we have "
"``Dropout`` which renormalizes the weights, after it zeroed some of the "
"elements, which pushes the absolute max value to more than 64K, and we "
"get an overlow (``inf``)."
msgstr ""

#: ../../source/debugging.rst:159
msgid ""
"As you can see it's the previous frames that we need to look into when "
"the numbers start going into very large for fp16 numbers."
msgstr ""

#: ../../source/debugging.rst:162
msgid "Let's match the report to the code from ``models/t5/modeling_t5.py``:"
msgstr ""

#: ../../source/debugging.rst:183
msgid ""
"Now it's easy to see the ``dropout`` call, and all the previous calls as "
"well."
msgstr ""

#: ../../source/debugging.rst:185
msgid ""
"Since the detection is happening in a forward hook, these reports are "
"printed immediately after each ``forward`` returns."
msgstr ""

#: ../../source/debugging.rst:188
msgid ""
"Going back to the full report, to act on it and to fix the problem, we "
"need to go a few frames up where the numbers started to go up and most "
"likely switch to the ``fp32`` mode here, so that the numbers don't "
"overflow when multiplied or summed up. Of course, there might be other "
"solutions. For example, we could turn off ``amp`` temporarily if it's "
"enabled, after moving the original ``forward`` into a helper wrapper, "
"like so:"
msgstr ""

#: ../../source/debugging.rst:211
msgid ""
"Since the automatic detector only reports on inputs and outputs of full "
"frames, once you know where to look, you may want to analyse the "
"intermediary stages of any specific ``forward`` function as well. In such"
" a case you can use the ``detect_overflow`` helper function to inject the"
" detector where you want it, for example:"
msgstr ""

#: ../../source/debugging.rst:228
msgid ""
"You can see that we added 2 of these and now we track if ``inf`` or "
"``nan`` for ``forwarded_states`` was detected somewhere in between."
msgstr ""

#: ../../source/debugging.rst:231
msgid ""
"Actually, the detector already reports these because each of the calls in"
" the example above is a `nn.Module``, but let's say if you had some local"
" direct calculations this is how you'd do that."
msgstr ""

#: ../../source/debugging.rst:234
msgid ""
"Additionally, if you're instantiating the debugger in your own code, you "
"can adjust the number of frames printed from its default, e.g.:"
msgstr ""

#: ../../source/debugging.rst:243
msgid "Specific batch absolute mix and max value tracing"
msgstr ""

#: ../../source/debugging.rst:245
msgid ""
"The same debugging class can be used for per-batch tracing with the "
"underflow/overflow detection feature turned off."
msgstr ""

#: ../../source/debugging.rst:247
msgid ""
"Let's say you want to watch the absolute min and max values for all the "
"ingredients of each ``forward`` call of a given batch, and only do that "
"for batches 1 and 3. Then you instantiate this class as:"
msgstr ""

#: ../../source/debugging.rst:254
msgid ""
"And now full batches 1 and 3 will be traced using the same format as the "
"underflow/overflow detector does."
msgstr ""

#: ../../source/debugging.rst:256
msgid "Batches are 0-indexed."
msgstr ""

#: ../../source/debugging.rst:258
msgid ""
"This is helpful if you know that the program starts misbehaving after a "
"certain batch number, so you can fast-forward right to that area. Here is"
" a sample truncated output for such configuration:"
msgstr ""

#: ../../source/debugging.rst:290
msgid ""
"Here you will get a huge number of frames dumped - as many as there were "
"forward calls in your model, so it may or may not what you want, but "
"sometimes it can be easier to use for debugging purposes than a normal "
"debugger. For example, if a problem starts happening at batch number 150."
" So you can dump traces for batches 149 and 150 and compare where numbers"
" started to diverge."
msgstr ""

#: ../../source/debugging.rst:295
msgid ""
"You can also specify the batch number after which to stop the training, "
"with:"
msgstr ""


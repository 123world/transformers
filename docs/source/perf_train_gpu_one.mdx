<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
-->

# Efficient Training on a Single GPU



## Less Memory

The following techniques will help you reduce memory usage.

### fp16 / bf16

Enabling mixed precision will make your training both faster and use less memory. The science of it is explained [here](performance#fp16) and [here]((performance#bf16).

bf16 can only be used with Ampere based NVIDIA gpus (or newer). bf16 makes the training more stable than fp16 since it has almost the same numerical range as fp32.

Activation:

- HF Trainer-based examples: add `--fp16` or `--bf16` to the command line arguments.
- Custom HF Trainer-based program, pass one of:

    ```python
    TrainingArguments(fp16=True)
    # TrainingArguments(bf16=True)
    ```
- `accelerate`:  use: ... (XXX: Sylvain/Leandro?)

- Custom training loop: enable [`torch.cuda.amp`](https://pytorch.org/docs/stable/amp.html). For example for bf16:

    ```python
    from torch.cuda.amp import autocast
    with autocast(dtype=torch.bfloat16):
        loss, outputs = ...
    ```


### Gradient Accumulation

Gradient accumulation allows you to train a much larger batch than can fit into your GPU's memory. It also speeds up the training because it updates the weights less frequently.

The science of gradient accumulation is explained [here](performance#gradient-accumulation).

You will need to experiment to find the best number of steps to accumulate for. In the following examples we will use 4 steps.

Activation:

- HF Trainer-based examples: add `--gradient_accumulation_steps 4` to the command line arguments.
- Custom HF Trainer-based program, pass one of:

    ```python
    TrainingArguments(gradient_accumulation_steps=4)
    ```
- `accelerate`:  use: ... (XXX: Sylvain/Leandro?)

- Custom training loop: This is somewhat complex but you can study how this is implemented in [HF Trainer](
https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) - simply search for `gradient_accumulation_steps` in the code.


### Gradient Checkpointing

Gradient checkpointing saves memory while trading off some 20-30% of throughput.

The science of gradient checkpointing is explained [here](performance#gradient-checkpointing).

Activation:

- HF Trainer-based examples: add `--gradient_checkpointing` to the command line arguments.
- Custom HF Trainer-based program, pass one of:

    ```python
    TrainingArguments(gradient_checkpointing=True)
    ```
- `accelerate`:  use: ... (XXX: Sylvain/Leandro?)

- Custom training loop: To implement see [torch.utils.checkpoint.checkpoint](https://pytorch.org/docs/stable/checkpoint.html).



### Optimizer

Some optimizers require a lot more memory than others.

The science of optimizer's memory usage and which optimizers to choose when are explained [here](performance#optimizer).


Activation:

- HF Trainer-based examples: add `--optim` to the command line arguments, followed by the desired optimizer. e.g. one of `adamw_hf`, `adamw_torch`, `adamw_torch_xla`, `adamw_apex_fused`, `adafactor`
- Custom HF Trainer-based program, pass one of the optimizers listed above like so:

    ```python
    TrainingArguments(optim="adam_torch")
    ```
- `accelerate`:  use: ... (XXX: Sylvain/Leandro?)

- Custom training loop: This is somewhat complex but you can study how this is implemented in [HF Trainer](
https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) - simply search for `optimizer` in the code.


### Deepspeed ZeRO

The in-depth details on how to use Deepspeed can be found [here](main_classes/deepspeed).

First, a quick decision tree:

1. Model fits onto a single GPU and you have enough space to fit a small batch size - you don't need to use Deepspeed as it'll only slow things down in this use case.
2. Model doesn't fit onto a single GPU or you can't fit a small batch - use DeepSpeed ZeRO + CPU Offload and for much larger models NVMe Offload.

Now if the decision tree suggested you use DeepSpeed first you need to [install it](main_classes/deepspeed#installation), then follow one of the following guides to create a configuration file and launch DeepSpeed.

Activation:

- HF Trainer-based examples: see this [guide](main_classes/deepspeed#deployment-with-one-gpu).
- Custom HF Trainer-based program: Same as above, but pass:

    ```python
    TrainingArguments(deepspeed="/path/to/ds_config.json")
    ```
- Deployment in Notebooks: see this [guide](main_classes/deepspeed#deployment-in-notebooks).

- `accelerate`:  use: ... (XXX: Sylvain/Leandro?)

- Custom training loop: This is somewhat complex but you can study how this is implemented in [HF Trainer](
https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) - simply search for `deepspeed` in the code.






## Faster Speed

There are times where you have plenty of GPU memory and you want the training to go faster either because you're in a rush or because you're paying per hour for the hardware usage.

The following techniques will make the training faster.


### Gradient Accumulation

Gradient accumulation allows you to speed up training because it updates the weights less frequently.

The science of gradient accumulation is explained [here](performance#gradient-accumulation).

To activate please see the "Gradient Accumulation" section in the "Less Memory" section of this document (XXX: how to link?)


### Batch sizes

A large batch size allows for a much better GPU utilization and thus often may lead to significant speed ups in performance.

The science of batch sizes is explained [here](performance#batch-sizes).

Activation:

- HF Trainer-based examples: set `--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE` for training and `--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE` for evaluation.

- Custom HF Trainer-based program, pass one of the optimizers listed above like so:

    ```python
    TrainingArguments(per_device_train_batch_size=4, per_device_eval_batch_size=4)
    ```
- `accelerate`:  use: ... (XXX: Sylvain/Leandro?)

- Custom training loop: This is somewhat complex but you can study how this is implemented in [HF Trainer](
https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) - simply search for `batch_size` in the code.



### Optimizer

A choice of optimizer can impact the throughput. For example, using the fused AdamW optimizer from  [NVIDIA/apex](https://github.com/NVIDIA/apex) will be faster than the same optimizer from `torch`.

The science of optimizer's speed and which optimizers to choose when are explained [here](performance#optimizer).

To activate please see the Optimizer section in the "Less Memory" section of this document (XXX: how to link?)

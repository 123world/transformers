<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
-->

# Efficient Training on a Single GPU



## Less Memory


### fp16 / bf16

Enabling mixed precision will make your training both faster and use less memory. For full details see ...

bf16 can only be used with Ampere based NVIDIA gpus (or newer). bf16 makes the training more stable than fp16 since it has almost the same numerical range as fp32.

To activate these in HF Trainer-based examples simply add `--fp16` or `--bf16` to the command line arguments

To activate these in your custom HF Trainer-based program, pass one of:

```
TrainingArguments(fp16=True)
TrainingArguments(bf16=True)
```

To activate these in `accelerate` use: ... (XXX?)

to use these in your own custom training loop, you need to enable `torch.cuda.amp` for more details see ...


### Gradient Accumulation


### Gradient Checkpointing


### Optimizer


### Deepspeed ZeRO




## Faster Speed

### Gradient Accumulation

### Batch sizes




## Scalability Strategy

* Model fits onto a single GPU:

    1. Normal use

* Model doesn't fit onto a single GPU:

    1. ZeRO + Offload CPU and optionally NVMe
    2. as above plus Memory Centric Tiling (see below for details) if the largest layer can't fit into a single GPU

* Largest Layer not fitting into a single GPU:

1. ZeRO - Enable [Memory Centric Tiling](https://deepspeed.readthedocs.io/en/latest/zero3.html#memory-centric-tiling) (MCT). It allows you to run arbitrarily large layers by automatically splitting them and executing them sequentially. MCT reduces the number of parameters that are live on a GPU, but it does not affect the activation memory. As this need is very rare as of this writing a manual override of `torch.nn.Linear` needs to be done by the user.
